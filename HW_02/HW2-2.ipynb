{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1sCWXCiBDs6UhYtxXTfWfIb30nC7kHTgc","timestamp":1612276273852},{"file_id":"1FIsDx7KTE5tiF-Xag22pl4VLQZc6G8Yw","timestamp":1612057948373}],"toc_visible":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Language Modeling","metadata":{"collapsed":true,"id":"15QfB7RAuXAc","jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"In this Exercise, we will be using Pytorch Lightning to implement our neural LM. Your job will be just to write the forward method of the model.\n\n","metadata":{"id":"gucid6KNuXAe"}},{"cell_type":"markdown","source":"## setup","metadata":{"id":"yL_M2zf4myYa"}},{"cell_type":"code","source":"# #download corpus\n!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n!unzip BEST2010.zip","metadata":{"id":"MRRrn78ZjL54","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:09.113517Z","iopub.execute_input":"2025-01-16T15:52:09.113799Z","iopub.status.idle":"2025-01-16T15:52:10.246743Z","shell.execute_reply.started":"2025-01-16T15:52:09.113768Z","shell.execute_reply":"2025-01-16T15:52:10.245960Z"}},"outputs":[{"name":"stdout","text":"--2025-01-16 15:52:09--  https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\nResolving github.com (github.com)... 140.82.116.4\nConnecting to github.com (github.com)|140.82.116.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip [following]\n--2025-01-16 15:52:09--  https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7423530 (7.1M) [application/zip]\nSaving to: ‘BEST2010.zip’\n\nBEST2010.zip        100%[===================>]   7.08M  --.-KB/s    in 0.05s   \n\n2025-01-16 15:52:09 (136 MB/s) - ‘BEST2010.zip’ saved [7423530/7423530]\n\nArchive:  BEST2010.zip\n   creating: BEST2010/\n  inflating: BEST2010/article.txt    \n  inflating: BEST2010/encyclopedia.txt  \n  inflating: BEST2010/news.txt       \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install lightning","metadata":{"id":"SGmYebp38OUl","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:10.247757Z","iopub.execute_input":"2025-01-16T15:52:10.248056Z","iopub.status.idle":"2025-01-16T15:52:15.399986Z","shell.execute_reply.started":"2025-01-16T15:52:10.248022Z","shell.execute_reply":"2025-01-16T15:52:15.398826Z"}},"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\nRequirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.1+cu121)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.0)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.5)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.4.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (71.0.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\nDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.5.0.post0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## code","metadata":{"id":"IR4HK5jQm17K"}},{"cell_type":"code","source":"total_word_count = 0\nbest2010 = []\nwith open('BEST2010/news.txt','r',encoding='utf-8') as f:\n  for i,line in enumerate(f):\n    line=line.strip()[:-1] #remove the trailing |\n    total_word_count += len(line.split(\"|\"))\n    best2010.append(line)\n\ntrain = best2010[:int(len(best2010)*0.7)]\ntest = best2010[int(len(best2010)*0.7):]\n#Training data\ntrain_word_count =0\nfor line in train:\n    for word in line.split('|'):\n        train_word_count+=1\nprint ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\nprint ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))","metadata":{"id":"oPE1RqKOrWJ0","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:15.401083Z","iopub.execute_input":"2025-01-16T15:52:15.401334Z","iopub.status.idle":"2025-01-16T15:52:15.775897Z","shell.execute_reply.started":"2025-01-16T15:52:15.401312Z","shell.execute_reply":"2025-01-16T15:52:15.775161Z"}},"outputs":[{"name":"stdout","text":"Total sentences in BEST2010 news training dataset :\t21678\nTotal word counts in BEST2010 news training dataset :\t1042797\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Here we are going to use a library from huggingface called `tokenizers`. This will help us create a vocabulary and handle the encoding and decoding, i.e., convert text to its corresponding ID (which will be learned by the tokenizer).","metadata":{"id":"SQBjqe5arHGX"}},{"cell_type":"code","source":"from tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.pre_tokenizers import CharDelimiterSplit\nfrom tokenizers.trainers import WordLevelTrainer\n\n#Basically, we just use the new tokenizer as our vocab building tool.\n#In practice, you will have to use a compatible tokenizer like newmm to tokenize the corpus first then do this step\ntokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\ntokenizer.pre_tokenizer = CharDelimiterSplit(delimiter=\"|\") #now the tokenizer will split \"|\" for us\ntrainer = WordLevelTrainer(min_frequency=3,  #we can set a frequency threshold for taking a word into our vocab. for this example, words with freq < 3 will be excluded from the vocab.\n                           special_tokens=[\"[UNK]\", \"<s>\", \"</s>\"]) #these are our special tokens: for unknown, begin-of-sentence, and end-of-sentence, respectively.\ntokenizer.train_from_iterator(train, trainer=trainer)","metadata":{"id":"elwE0gh2rE3C","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:15.776705Z","iopub.execute_input":"2025-01-16T15:52:15.777055Z","iopub.status.idle":"2025-01-16T15:52:16.273376Z","shell.execute_reply.started":"2025-01-16T15:52:15.777026Z","shell.execute_reply":"2025-01-16T15:52:16.272665Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"len(tokenizer.get_vocab()) #same as nltk","metadata":{"id":"TrKtjv4PJpg2","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:16.274212Z","iopub.execute_input":"2025-01-16T15:52:16.274521Z","iopub.status.idle":"2025-01-16T15:52:16.285985Z","shell.execute_reply.started":"2025-01-16T15:52:16.274488Z","shell.execute_reply":"2025-01-16T15:52:16.285181Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"9062"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").tokens #tokens we get after tokenizing this sentence. unknown words will be tokenized as [UNK]","metadata":{"id":"WqM_jrZwrJpB","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:16.288205Z","iopub.execute_input":"2025-01-16T15:52:16.288395Z","iopub.status.idle":"2025-01-16T15:52:16.300549Z","shell.execute_reply.started":"2025-01-16T15:52:16.288378Z","shell.execute_reply":"2025-01-16T15:52:16.299917Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['กฎหมาย', 'กับ', 'การ', 'เบียดบัง', 'คน', 'จน', '[UNK]']"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").ids #this is what we will feed to the LM","metadata":{"id":"1r1pJ1B_sp9j","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:16.301817Z","iopub.execute_input":"2025-01-16T15:52:16.302110Z","iopub.status.idle":"2025-01-16T15:52:16.314203Z","shell.execute_reply.started":"2025-01-16T15:52:16.302090Z","shell.execute_reply":"2025-01-16T15:52:16.313565Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[242, 28, 5, 8883, 22, 190, 0]"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import itertools\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport lightning as L\nfrom tqdm import tqdm","metadata":{"id":"Fkx6CSoXWXmG","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:16.315022Z","iopub.execute_input":"2025-01-16T15:52:16.315259Z","iopub.status.idle":"2025-01-16T15:52:22.262552Z","shell.execute_reply.started":"2025-01-16T15:52:16.315240Z","shell.execute_reply":"2025-01-16T15:52:22.261682Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"L.seed_everything(42, workers=True)","metadata":{"id":"3XHJsP8_898x","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:22.263428Z","iopub.execute_input":"2025-01-16T15:52:22.263913Z","iopub.status.idle":"2025-01-16T15:52:22.275679Z","shell.execute_reply.started":"2025-01-16T15:52:22.263880Z","shell.execute_reply":"2025-01-16T15:52:22.274857Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"class TextDataset(Dataset):\n  def __init__(self, data, seq_len = 128):\n    #  data is currently a list of sentences\n    #  [sent1,\n    #   sent2,\n    #   ...,\n    #  ]\n\n    data = [d+'|</s>' for d in data] #append an </s> token to each sentence\n    encodings = tokenizer.encode_batch(data) #encode (turn token into token_id) data\n    token_ids = [enc.ids for enc in encodings] #get the token ids for each sentence\n    flatten_token_ids = list(itertools.chain(*token_ids)) #turn a list of token_ids into one long token_ids\n    ## now data looks like this [sent1_ids </s> sent2_ids </s> ...]\n    encoded = torch.LongTensor(flatten_token_ids)\n\n    #remove some left over tokens so that we can form batches of seq_len (128 in this case). Optionally, we can use padding tokens instead.\n    left_over = len(encoded) % seq_len\n    encoded = encoded[:len(encoded)-left_over]\n    self.encoded = encoded.view(-1, seq_len) #reshape data so it becomes a 2-D matrix of shape (len(encoded)//128, 128), i.e. each row contains data of len==128\n    ## now data looks like this\n    ## [ [1,2,3, ... , 128] (this is just an example, not actual input_ids)\n    ##   [1,2,3, ... , 128]\n    ##   [1,2,3, ... , 128]\n    ## ]\n\n  def __getitem__(self, idx):\n    return self.encoded[idx]\n\n  def __len__(self):\n    return len(self.encoded)","metadata":{"id":"-r_kyrrrDHZq","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:22.276531Z","iopub.execute_input":"2025-01-16T15:52:22.276795Z","iopub.status.idle":"2025-01-16T15:52:22.284972Z","shell.execute_reply.started":"2025-01-16T15:52:22.276745Z","shell.execute_reply":"2025-01-16T15:52:22.284127Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_batch_size = 64\ntest_batch_size = 128\ntrain_dataset = TextDataset(train)\ntrain_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True) #DataLoader will take care of the random sampling and batching of data\n\ntest_dataset = TextDataset(test)\ntest_loader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)","metadata":{"id":"YmW-K0XBZ4Dq","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:22.285808Z","iopub.execute_input":"2025-01-16T15:52:22.286079Z","iopub.status.idle":"2025-01-16T15:52:23.925914Z","shell.execute_reply.started":"2025-01-16T15:52:22.286053Z","shell.execute_reply":"2025-01-16T15:52:23.925249Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Model : Implement the forward function here","metadata":{"id":"ElhZcB94MUtC"}},{"cell_type":"code","source":"class LSTM(L.LightningModule):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim) #this will turn the token ids into vectors\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n                    dropout=dropout_rate, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim, vocab_size) #turn the vectors back into token ids\n        self.learning_rate = learning_rate\n        self.criterion = criterion\n\n    def forward(self, src):\n        embedded = self.embedding(src)\n        embedded = self.dropout(embedded)\n        lstm_out, _ = self.lstm(embedded)\n        lstm_out = self.dropout(lstm_out)\n        output = self.fc(lstm_out)\n\n        return output\n\n    def training_step(self, batch, batch_idx):\n\n        src = batch[:, :-1]\n        target = batch[:, 1:]\n        prediction = self(src) # run the sequence through the model (the forward method)\n        prediction = prediction.reshape(-1, vocab_size)\n        target = target.reshape(-1)\n        loss = self.criterion(prediction, target)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n\n        src = batch[:, :-1]  #[batch_size (64) , seq_len-1 (127)] except last words\n        target = batch[:, 1:] #[batch_size (64) , seq_len-1 (127)] except first words\n        with torch.no_grad(): #disable gradient calculation for faster inference\n          prediction = self(src) #[batch_size (64), seq_len-1 (127) , vocab size (9000)]\n        prediction = prediction.reshape(-1, vocab_size) #[batch_size*(seq_len-1) (64*127=8128) , vocab]\n        target = target.reshape(-1) #[batch_size (64), seq_len-1 (127)] -> [batch_size*(seq_len-1) (8128)]\n        loss = self.criterion(prediction, target)\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.learning_rate)","metadata":{"id":"nKNJAolug-1I","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:23.926702Z","iopub.execute_input":"2025-01-16T15:52:23.926976Z","iopub.status.idle":"2025-01-16T15:52:23.934200Z","shell.execute_reply.started":"2025-01-16T15:52:23.926954Z","shell.execute_reply":"2025-01-16T15:52:23.933363Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"vocab_size = tokenizer.get_vocab_size()\nembedding_dim = 200\nhidden_dim = 512\nnum_layers = 3\ndropout_rate = 0.2\nlr = 1e-3","metadata":{"id":"jBnYCh-miOEr","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:23.935019Z","iopub.execute_input":"2025-01-16T15:52:23.935250Z","iopub.status.idle":"2025-01-16T15:52:23.953745Z","shell.execute_reply.started":"2025-01-16T15:52:23.935231Z","shell.execute_reply":"2025-01-16T15:52:23.953163Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)","metadata":{"id":"HHWXaPsvigPq","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:23.954488Z","iopub.execute_input":"2025-01-16T15:52:23.954770Z","iopub.status.idle":"2025-01-16T15:52:24.070366Z","shell.execute_reply.started":"2025-01-16T15:52:23.954743Z","shell.execute_reply":"2025-01-16T15:52:24.069536Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from lightning.pytorch.loggers import CSVLogger\ncsv_logger = CSVLogger(\"log\")","metadata":{"id":"_yNEZ4jwXumR","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:24.071219Z","iopub.execute_input":"2025-01-16T15:52:24.071437Z","iopub.status.idle":"2025-01-16T15:52:24.075201Z","shell.execute_reply.started":"2025-01-16T15:52:24.071419Z","shell.execute_reply":"2025-01-16T15:52:24.074349Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Training","metadata":{"id":"eZwqhWicMdH0"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=20,\n    logger=csv_logger,\n    deterministic=True\n)","metadata":{"id":"kr0zdeMAjD1U","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:24.076023Z","iopub.execute_input":"2025-01-16T15:52:24.076320Z","iopub.status.idle":"2025-01-16T15:52:24.155116Z","shell.execute_reply.started":"2025-01-16T15:52:24.076276Z","shell.execute_reply":"2025-01-16T15:52:24.154536Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"trainer.fit(model, train_dataloaders=train_loader) # takes about 8 mins","metadata":{"id":"A9qcwNA0mN6J","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:52:24.155806Z","iopub.execute_input":"2025-01-16T15:52:24.156041Z","iopub.status.idle":"2025-01-16T15:57:22.075985Z","shell.execute_reply.started":"2025-01-16T15:52:24.156022Z","shell.execute_reply":"2025-01-16T15:57:22.075101Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 1.8 M  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 4.6 M  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n12.1 M    Trainable params\n0         Non-trainable params\n12.1 M    Total params\n48.504    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f33b3cb8288645ddac50297ec0904da5"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Testing","metadata":{"id":"uUfWF_V6Me9H"}},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=test_loader)","metadata":{"id":"WXVj9ewNqweZ","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:57:22.076861Z","iopub.execute_input":"2025-01-16T15:57:22.077083Z","iopub.status.idle":"2025-01-16T15:57:24.575475Z","shell.execute_reply.started":"2025-01-16T15:57:22.077064Z","shell.execute_reply":"2025-01-16T15:57:24.574884Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"068a5298cad7496a9c4cd015340047c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   4.1011762619018555    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    4.1011762619018555     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import numpy as np","metadata":{"id":"4pVjEyYDtnc-","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:57:24.576154Z","iopub.execute_input":"2025-01-16T15:57:24.576347Z","iopub.status.idle":"2025-01-16T15:57:24.579707Z","shell.execute_reply.started":"2025-01-16T15:57:24.576330Z","shell.execute_reply":"2025-01-16T15:57:24.579030Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"print(f\"Perplexity : {np.exp(test_result[0]['test_loss'])}\")","metadata":{"id":"uuIPToGQs-ZG","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:57:24.582314Z","iopub.execute_input":"2025-01-16T15:57:24.582512Z","iopub.status.idle":"2025-01-16T15:57:24.600936Z","shell.execute_reply.started":"2025-01-16T15:57:24.582496Z","shell.execute_reply":"2025-01-16T15:57:24.600121Z"}},"outputs":[{"name":"stdout","text":"Perplexity : 60.41130533835299\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model.eval() #disable dropout","metadata":{"id":"pAZwiRqsnOPe","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:57:24.601793Z","iopub.execute_input":"2025-01-16T15:57:24.602082Z","iopub.status.idle":"2025-01-16T15:57:24.616479Z","shell.execute_reply.started":"2025-01-16T15:57:24.602057Z","shell.execute_reply":"2025-01-16T15:57:24.615818Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"LSTM(\n  (embedding): Embedding(9062, 200)\n  (lstm): LSTM(200, 512, num_layers=3, batch_first=True, dropout=0.2)\n  (dropout): Dropout(p=0.2, inplace=False)\n  (fc): Linear(in_features=512, out_features=9062, bias=True)\n  (criterion): CrossEntropyLoss()\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"unk_token_id = tokenizer.encode(\"[UNK]\").ids\neos_token_id = tokenizer.encode(\"</s>\").ids","metadata":{"id":"VFtebDAmVh_T","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:57:24.617272Z","iopub.execute_input":"2025-01-16T15:57:24.617531Z","iopub.status.idle":"2025-01-16T15:57:24.629298Z","shell.execute_reply.started":"2025-01-16T15:57:24.617503Z","shell.execute_reply":"2025-01-16T15:57:24.628601Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def generate_seq(context, max_new_token = 10):\n  encoded = tokenizer.encode(context).ids\n  with torch.no_grad():\n      for i in range(max_new_token):\n          src = torch.LongTensor([encoded]).to(model.device)\n          prediction = model(src)\n          probs = torch.softmax(prediction[:, -1] / 1, dim=-1)\n          prediction = torch.multinomial(probs, num_samples=1).item()\n\n          while prediction == unk_token_id:\n              prediction = torch.multinomial(probs, num_samples=1).item()\n\n          if prediction == eos_token_id:\n              break\n\n          encoded.append(prediction)\n\n  return tokenizer.decode(encoded)","metadata":{"id":"hj-V4OsDqpBO","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:57:24.630146Z","iopub.execute_input":"2025-01-16T15:57:24.630394Z","iopub.status.idle":"2025-01-16T15:57:24.643733Z","shell.execute_reply.started":"2025-01-16T15:57:24.630364Z","shell.execute_reply":"2025-01-16T15:57:24.642919Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"context = \"<s>|วัน|จันทร์\"\ngenerate_seq(context, 50)","metadata":{"id":"u20r9w8zvJi4","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T16:03:23.253913Z","iopub.execute_input":"2025-01-16T16:03:23.254307Z","iopub.status.idle":"2025-01-16T16:03:24.147095Z","shell.execute_reply.started":"2025-01-16T16:03:23.254278Z","shell.execute_reply":"2025-01-16T16:03:24.146233Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"'วัน จันทร์ ที่ 952335   เมื่อ เวลา   11 . 00   น.   เจ้าหน้าที่ ตำรวจ   จ.นราธิวาส   โรง เรียน ร่วม กัน เตือน ใหม่ ที่ กำหนด   กล่าว ให้ รับ สารภาพ ว่า   กำลัง พบ เหตุการณ์ ว่า เกิด ชีวิต   3   ของ กลุ่ม ของ ข่าว การ ก่อ ออก'"},"metadata":{}}],"execution_count":57},{"cell_type":"markdown","source":"## Questions: Answer the following in MyCourseville\n\n1. What is the perplexity of the neural LM you trained?\n2. Paste your favorite sentence generated with the LM.","metadata":{"id":"1fr536NVvGX3"}}]}