{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Language Modeling using Ngram","metadata":{"collapsed":true,"id":"15QfB7RAuXAc","jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"In this Exercise, we are going to create a bigram language model and its variation. We will build one model for each of the following type and calculate their perplexity:\n- Unigram Model\n- Bigram Model\n- Bigram Model with Laplace smoothing\n- Bigram Model with Interpolation\n- Bigram Model with Kneser-ney Interpolation\n\nWe will also use NLTK which is a natural language processing library for python to make our lives easier.\n\n","metadata":{"id":"gucid6KNuXAe"}},{"cell_type":"code","source":"# #download corpus\n!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n!unzip BEST2010.zip","metadata":{"id":"MRRrn78ZjL54","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:24:48.420725Z","iopub.execute_input":"2025-01-16T15:24:48.421049Z","iopub.status.idle":"2025-01-16T15:24:50.308846Z","shell.execute_reply.started":"2025-01-16T15:24:48.421022Z","shell.execute_reply":"2025-01-16T15:24:50.307480Z"}},"outputs":[{"name":"stdout","text":"--2025-01-16 15:24:48--  https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\nResolving github.com (github.com)... 4.237.22.38\nConnecting to github.com (github.com)|4.237.22.38|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip [following]\n--2025-01-16 15:24:48--  https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7423530 (7.1M) [application/zip]\nSaving to: ‘BEST2010.zip’\n\nBEST2010.zip        100%[===================>]   7.08M  --.-KB/s    in 0.04s   \n\n2025-01-16 15:24:49 (202 MB/s) - ‘BEST2010.zip’ saved [7423530/7423530]\n\nArchive:  BEST2010.zip\n   creating: BEST2010/\n  inflating: BEST2010/article.txt    \n  inflating: BEST2010/encyclopedia.txt  \n  inflating: BEST2010/news.txt       \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!wget https://www.dropbox.com/s/jajdlqnp5h0ywvo/tokenized_wiki_sample.csv","metadata":{"id":"qeyvLSptdKXj","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:24:50.310937Z","iopub.execute_input":"2025-01-16T15:24:50.311388Z","iopub.status.idle":"2025-01-16T15:24:54.453179Z","shell.execute_reply.started":"2025-01-16T15:24:50.311343Z","shell.execute_reply":"2025-01-16T15:24:54.452007Z"}},"outputs":[{"name":"stdout","text":"--2025-01-16 15:24:50--  https://www.dropbox.com/s/jajdlqnp5h0ywvo/tokenized_wiki_sample.csv\nResolving www.dropbox.com (www.dropbox.com)... 162.125.83.18, 2620:100:6033:18::a27d:5312\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.83.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/88uzig0mno1b57d6bhwht/tokenized_wiki_sample.csv?rlkey=oya9jw1rljj31jc49fvoaty01 [following]\n--2025-01-16 15:24:50--  https://www.dropbox.com/scl/fi/88uzig0mno1b57d6bhwht/tokenized_wiki_sample.csv?rlkey=oya9jw1rljj31jc49fvoaty01\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc56b90ee3c8c7c0764750d1d7c8.dl.dropboxusercontent.com/cd/0/inline/CiRSlqf6B9Da_iFFGi-S39GrYCInpwT-VPOhSNeAT0A8lMLBRiudLYPmZdi5JabQGWQ1PCVb3K1KjC_OJgyTYBW69yVtfOVQvNgIEZuN-eD0d9Uts-jxt3zWDv2wuiZDby8LxfT5Oti3cj3fQDXDo2ln/file# [following]\n--2025-01-16 15:24:51--  https://uc56b90ee3c8c7c0764750d1d7c8.dl.dropboxusercontent.com/cd/0/inline/CiRSlqf6B9Da_iFFGi-S39GrYCInpwT-VPOhSNeAT0A8lMLBRiudLYPmZdi5JabQGWQ1PCVb3K1KjC_OJgyTYBW69yVtfOVQvNgIEZuN-eD0d9Uts-jxt3zWDv2wuiZDby8LxfT5Oti3cj3fQDXDo2ln/file\nResolving uc56b90ee3c8c7c0764750d1d7c8.dl.dropboxusercontent.com (uc56b90ee3c8c7c0764750d1d7c8.dl.dropboxusercontent.com)... 162.125.83.15, 2620:100:6033:15::a27d:530f\nConnecting to uc56b90ee3c8c7c0764750d1d7c8.dl.dropboxusercontent.com (uc56b90ee3c8c7c0764750d1d7c8.dl.dropboxusercontent.com)|162.125.83.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4399477 (4.2M) [text/plain]\nSaving to: ‘tokenized_wiki_sample.csv’\n\ntokenized_wiki_samp 100%[===================>]   4.20M  3.09MB/s    in 1.4s    \n\n2025-01-16 15:24:54 (3.09 MB/s) - ‘tokenized_wiki_sample.csv’ saved [4399477/4399477]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#First we import necessary library such as math, nltk, bigram, and collections.\nimport math\nimport nltk\nimport io\nimport random\nfrom random import shuffle\nfrom nltk import bigrams, trigrams\nfrom collections import Counter, defaultdict\nrandom.seed(999)","metadata":{"id":"GjJDeG03uXAf","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:24:54.455875Z","iopub.execute_input":"2025-01-16T15:24:54.456179Z","iopub.status.idle":"2025-01-16T15:24:55.970269Z","shell.execute_reply.started":"2025-01-16T15:24:54.456153Z","shell.execute_reply":"2025-01-16T15:24:55.969166Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"BEST2010 is a free Thai NLP dataset by NECTEC usually used as a standard benchmark for various NLP tasks including language modeling. It is separated into 4 domains including article, encyclopedia, news, and novel. The data is already  tokenized using '|' as a separator.\n\nFor example,\n\nตาม|ที่|นางประนอม ทองจันทร์| |กับ| |ด.ช.กิตติพงษ์ แหลมผักแว่น| |และ| |ด.ญ.กาญจนา กรองแก้ว| |ป่วย|สงสัย|ติด|เชื้อ|ไข้|ขณะ|นี้|ยัง|ไม่|ดี|ขึ้น|","metadata":{"id":"HugXBHNEuXAh"}},{"cell_type":"code","source":"total_word_count = 0\nbest2010 = []\nwith open('BEST2010/news.txt','r',encoding='utf-8') as f:\n  for i,line in enumerate(f):\n    line=line.strip()[:-1] #remove the trailing |\n    total_word_count += len(line.split(\"|\"))\n    best2010.append(line)","metadata":{"id":"iu-AJSZZuXAi","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:24:55.971833Z","iopub.execute_input":"2025-01-16T15:24:55.972311Z","iopub.status.idle":"2025-01-16T15:24:56.226179Z","shell.execute_reply.started":"2025-01-16T15:24:55.972254Z","shell.execute_reply":"2025-01-16T15:24:56.225000Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#For simplicity, we assumes that each line is a sentence.\nprint (f'Total sentences in BEST2010 news dataset :\\t{len(best2010)}')\nprint (f'Total word counts in BEST2010 news dataset :\\t{total_word_count}')","metadata":{"id":"3WfpGgbruXAj","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:24:56.227354Z","iopub.execute_input":"2025-01-16T15:24:56.227747Z","iopub.status.idle":"2025-01-16T15:24:56.232886Z","shell.execute_reply.started":"2025-01-16T15:24:56.227708Z","shell.execute_reply":"2025-01-16T15:24:56.231901Z"}},"outputs":[{"name":"stdout","text":"Total sentences in BEST2010 news dataset :\t30969\nTotal word counts in BEST2010 news dataset :\t1660190\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"We separate the input into 2 sets, train and test data with 70:30 ratio","metadata":{"id":"_JD9iXF1uXAm"}},{"cell_type":"code","source":"sentences = best2010\n# The data is separated to train and test set with 70:30 ratio.\ntrain = sentences[:int(len(sentences)*0.7)]\ntest = sentences[int(len(sentences)*0.7):]\n\n#Training data\ntrain_word_count =0\nfor line in train:\n    for word in line.split('|'):\n        train_word_count+=1\nprint ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\nprint ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))","metadata":{"id":"_WGcQq_juXAm","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:24:56.234054Z","iopub.execute_input":"2025-01-16T15:24:56.234478Z","iopub.status.idle":"2025-01-16T15:24:56.419385Z","shell.execute_reply.started":"2025-01-16T15:24:56.234438Z","shell.execute_reply":"2025-01-16T15:24:56.418224Z"}},"outputs":[{"name":"stdout","text":"Total sentences in BEST2010 news training dataset :\t21678\nTotal word counts in BEST2010 news training dataset :\t1042797\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Here we load the data from Wikipedia which is also already tokenized. It will be used for answering questions in MyCourseville.","metadata":{"id":"17x6tW-3ae7Z"}},{"cell_type":"code","source":"import pandas as pd\nwiki_data = pd.read_csv(\"tokenized_wiki_sample.csv\")","metadata":{"id":"0fAl6dTg_9HG","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:24:56.420669Z","iopub.execute_input":"2025-01-16T15:24:56.421099Z","iopub.status.idle":"2025-01-16T15:24:56.875871Z","shell.execute_reply.started":"2025-01-16T15:24:56.421058Z","shell.execute_reply":"2025-01-16T15:24:56.874651Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Data Preprocessing\n\nBefore training any language models, the first step we always do is process the data into the format suited for the LM.\n\nFor this exercise, we will use NLTK to help process our data.","metadata":{"id":"H1W5bm-hbQXa"}},{"cell_type":"code","source":"!pip install -U nltk","metadata":{"id":"4OIqxJB7P29D","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:24:56.876981Z","iopub.execute_input":"2025-01-16T15:24:56.877325Z","iopub.status.idle":"2025-01-16T15:25:04.287894Z","shell.execute_reply.started":"2025-01-16T15:24:56.877270Z","shell.execute_reply":"2025-01-16T15:25:04.286178Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.9.1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from nltk.lm.preprocessing import pad_both_ends, flatten\nfrom nltk.lm.vocabulary import Vocabulary\nfrom nltk import ngrams","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:04.291923Z","iopub.execute_input":"2025-01-16T15:25:04.292255Z","iopub.status.idle":"2025-01-16T15:25:04.304580Z","shell.execute_reply.started":"2025-01-16T15:25:04.292227Z","shell.execute_reply":"2025-01-16T15:25:04.303102Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"We begin by \"tokenizing\" our training set. Note that the data is already tokenized so we can just split it.","metadata":{"id":"Oy0ZN2_0bzRr"}},{"cell_type":"code","source":"tokenized_train = [[\"<s>\"] + t.split(\"|\") + [\"</s>\"] for t in train] # \"tokenize\" each sentence","metadata":{"id":"WQM0PXnXbzCN","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:04.307014Z","iopub.execute_input":"2025-01-16T15:25:04.307365Z","iopub.status.idle":"2025-01-16T15:25:04.635344Z","shell.execute_reply.started":"2025-01-16T15:25:04.307329Z","shell.execute_reply":"2025-01-16T15:25:04.634411Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Next we create a vocabulary with the ```Vocabulary``` class from NLTK. It accepts a list of tokens so we flatten our sentences into one long sentence first.\n\n\n\n\n\n","metadata":{"id":"TM2ylNRNcrg9"}},{"cell_type":"code","source":"flat_tokens = list(flatten(tokenized_train)) #join all sentences into one long sentence\nvocab = Vocabulary(flat_tokens, unk_cutoff=3) #Words with frequency **below** 3 (not exactly 3) will not be considered in our vocab and will be converted to <UNK>.","metadata":{"id":"Tbp-VmkHcq4d","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:04.636383Z","iopub.execute_input":"2025-01-16T15:25:04.636769Z","iopub.status.idle":"2025-01-16T15:25:04.853499Z","shell.execute_reply.started":"2025-01-16T15:25:04.636737Z","shell.execute_reply":"2025-01-16T15:25:04.852443Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Then we replace low frequency words and pad each sentence with \\<s\\> in the front and \\</s\\> in the back of each sentence.\n\nNow *each* sentence is going to look something like this:\n\\[\"\\<s\\>\", \"hello\", \"my\", \"name\", \"is\", \"\\<UNK\\>\", \"\\</s\\>\" \\]","metadata":{"id":"oFnBHe6ScAaV"}},{"cell_type":"code","source":"tokenized_train = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_train]\npadded_tokenized_train = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_train]","metadata":{"id":"9q6QakuibxqN","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:04.854557Z","iopub.execute_input":"2025-01-16T15:25:04.854929Z","iopub.status.idle":"2025-01-16T15:25:05.580948Z","shell.execute_reply.started":"2025-01-16T15:25:04.854893Z","shell.execute_reply":"2025-01-16T15:25:05.579735Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Finally, we do the same for the test set and the wiki dataset.","metadata":{"id":"Dn6GxaSFeSpD"}},{"cell_type":"code","source":"tokenized_test = [t.split(\"|\") for t in test]\ntokenized_test = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_test]\npadded_tokenized_test = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_test]\n\ntokenized_wiki_test = [t.split(\"|\") for t in wiki_data['tokenized'].tolist()]\ntokenized_wiki_test = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_wiki_test]\npadded_tokenized_wiki_test = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_wiki_test]","metadata":{"id":"D4N6qKrPadIj","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:05.582089Z","iopub.execute_input":"2025-01-16T15:25:05.582431Z","iopub.status.idle":"2025-01-16T15:25:06.262623Z","shell.execute_reply.started":"2025-01-16T15:25:05.582402Z","shell.execute_reply":"2025-01-16T15:25:06.261555Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Unigram","metadata":{"id":"pHtCMFMluXAo"}},{"cell_type":"markdown","source":"In this section, we will demonstrate how to build a unigram language model <br>\n**Important note:** <br>\n**\\<s\\>** = sentence start symbol <br>\n**\\</s\\>** = sentence end symbol","metadata":{"id":"2V1WQTGzuXAp"}},{"cell_type":"markdown","source":"# VERY IMPORTANT:\n- In this notebook, we will *not* default the unknown token probability to ```1/len(vocab)``` but instead will treat it as a normal word and let the model learn its probability so that we can compare our results to NLTK.\n- **Also make sure that the code in this notebook can be executed without any problem. If we find that you used NLTK to answer questions in MyCourseVille and did not finish the assignment, you will receive a grade of 0 for this assignment.**","metadata":{"id":"Xd7qOd7KAYWM"}},{"cell_type":"code","source":"class UnigramModel():\n  def __init__(self, data, vocab):\n    self.unigram_count = defaultdict(lambda: 0.0)\n    self.word_count = 0\n    self.vocab = vocab\n    for sentence in data:\n        for w in sentence: #[(word1, ), (word2, ), (word3, )...]\n          w = w[0]\n          if w in self.vocab:\n            self.unigram_count[w] +=1\n          else:\n            self.unigram_count[\"<UNK>\"] += 1\n          self.word_count+=1\n\n  def __getitem__(self, w):\n    w = w[0]  #[(word1, ), (word2, ), (word3, )...]\n    if w in self.vocab:\n      return self.unigram_count[w]/(self.word_count)\n    else:\n      return self.unigram_count[\"<UNK>\"]/(self.word_count)","metadata":{"id":"CTV-i9kdse58","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:06.263624Z","iopub.execute_input":"2025-01-16T15:25:06.263923Z","iopub.status.idle":"2025-01-16T15:25:06.279166Z","shell.execute_reply.started":"2025-01-16T15:25:06.263897Z","shell.execute_reply":"2025-01-16T15:25:06.277692Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_unigrams = [list(ngrams(sent, n=1)) for sent in padded_tokenized_train] #creating the unigrams by setting n=1\nmodel = UnigramModel(train_unigrams, vocab)","metadata":{"id":"FnWJJ8Hqs8Qs","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:06.280087Z","iopub.execute_input":"2025-01-16T15:25:06.280508Z","iopub.status.idle":"2025-01-16T15:25:07.827822Z","shell.execute_reply.started":"2025-01-16T15:25:06.280474Z","shell.execute_reply":"2025-01-16T15:25:07.826575Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def getLnValue(x):\n      return math.log(x)","metadata":{"id":"6coGxSY7uXAt","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:07.828756Z","iopub.execute_input":"2025-01-16T15:25:07.829037Z","iopub.status.idle":"2025-01-16T15:25:07.833529Z","shell.execute_reply.started":"2025-01-16T15:25:07.829013Z","shell.execute_reply":"2025-01-16T15:25:07.832440Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#problability of 'นายก'\nprint(getLnValue(model['นายก']))\n\n#for example, problability of 'นายกรัฐมนตรี' which is an unknown word is equal to\nprint(getLnValue(model['นายกรัฐมนตรี']))\n\n#problability of 'นายก' 'ได้' 'ให้' 'สัมภาษณ์' 'กับ' 'สื่อ'\nprob = getLnValue(model['นายก'])+getLnValue(model['ได้'])+ getLnValue(model['ให้'])+getLnValue(model['สัมภาษณ์'])+getLnValue(model['กับ'])+getLnValue(model['สื่อ'])+getLnValue(model['</s>'])\nprint ('Problability of a sentence', math.exp(prob))","metadata":{"id":"cFy8yhZjuXAv","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:07.834747Z","iopub.execute_input":"2025-01-16T15:25:07.835077Z","iopub.status.idle":"2025-01-16T15:25:07.853750Z","shell.execute_reply.started":"2025-01-16T15:25:07.835050Z","shell.execute_reply":"2025-01-16T15:25:07.852514Z"}},"outputs":[{"name":"stdout","text":"-3.991273499731109\n-3.991273499731109\nProblability of a sentence 1.408776035744038e-16\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Perplexity\n\nIn order to compare language model we need to calculate perplexity. In this task you should write a perplexity calculation code for the unigram model. The result perplexity should be around 406.89 and\n376.86 on train and test data.","metadata":{"id":"D8EfqnDsuXAw"}},{"cell_type":"markdown","source":"## TODO #1 Calculate perplexity","metadata":{"id":"hZHQ-6tVuXAx"}},{"cell_type":"code","source":"def getLnValue(x):\n    return math.log(x)\n\ndef calculate_sentence_ln_prob(sentence, model):\n    sum_prob = 0\n    for w in sentence:\n        sum_prob += getLnValue(model[w])\n    return sum_prob\n\ndef perplexity(test,model):\n    sum_ln = 0\n    n = 0\n    for sentence in test:\n        sum_ln += calculate_sentence_ln_prob(sentence, model)\n        n += len(sentence)\n    return math.exp(-1/n * sum_ln)","metadata":{"id":"kh0DwzoouXAx","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:07.854820Z","iopub.execute_input":"2025-01-16T15:25:07.855216Z","iopub.status.idle":"2025-01-16T15:25:07.873723Z","shell.execute_reply.started":"2025-01-16T15:25:07.855179Z","shell.execute_reply":"2025-01-16T15:25:07.872676Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"test_unigrams = [list(ngrams(sent, n=1)) for sent in padded_tokenized_test]","metadata":{"id":"X-t_8mEzRxT-","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:07.874852Z","iopub.execute_input":"2025-01-16T15:25:07.875216Z","iopub.status.idle":"2025-01-16T15:25:08.079098Z","shell.execute_reply.started":"2025-01-16T15:25:07.875186Z","shell.execute_reply":"2025-01-16T15:25:08.078005Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"print(perplexity(train_unigrams,model))\nprint(perplexity(test_unigrams,model))","metadata":{"id":"PztVYprdtBja","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:08.080140Z","iopub.execute_input":"2025-01-16T15:25:08.080448Z","iopub.status.idle":"2025-01-16T15:25:09.830174Z","shell.execute_reply.started":"2025-01-16T15:25:08.080420Z","shell.execute_reply":"2025-01-16T15:25:09.829096Z"}},"outputs":[{"name":"stdout","text":"406.8950820766048\n376.86063648570286\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Q1 MCV\nCalculate the perplexity of the model on the wiki test set and answer in MyCourseVille","metadata":{"id":"PHnBXtt3b-OY"}},{"cell_type":"code","source":"wiki_test_unigrams = [list(ngrams(sent, n=1)) for sent in padded_tokenized_wiki_test]","metadata":{"id":"JRd6hF_WSBl_","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:09.831200Z","iopub.execute_input":"2025-01-16T15:25:09.831604Z","iopub.status.idle":"2025-01-16T15:25:09.898876Z","shell.execute_reply.started":"2025-01-16T15:25:09.831562Z","shell.execute_reply":"2025-01-16T15:25:09.897892Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_unigrams))], model))","metadata":{"id":"I_LiSohADNLC","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:09.899951Z","iopub.execute_input":"2025-01-16T15:25:09.900395Z","iopub.status.idle":"2025-01-16T15:25:10.136272Z","shell.execute_reply.started":"2025-01-16T15:25:09.900354Z","shell.execute_reply":"2025-01-16T15:25:10.135120Z"}},"outputs":[{"name":"stdout","text":"498.2505681123939\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Bigram","metadata":{"id":"lK0gaMf0uXA2"}},{"cell_type":"markdown","source":"Next, you will create a better language model than a unigram (which is not much to compare with). But first, it is very tedious to count every pair of words that occur in our corpus by ourselves. Lucky for us, nltk provides us a simple library which will simplify the process.","metadata":{"id":"dmTkAY_QuXA3"}},{"cell_type":"code","source":"#example of nltk usage for bigram\nsentence = 'I always search google for an answer .'\npadded_sentence = list(pad_both_ends(sentence.split(), n=2))\n\nprint('This is how nltk generate bigram.')\nfor w1,w2 in bigrams(padded_sentence):\n    print(w1,w2)\nprint('\\n<s> and </s> are used as a start and end of sentence symbol. respectively.')","metadata":{"id":"Lv6r2LJ1uXA4","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:10.137423Z","iopub.execute_input":"2025-01-16T15:25:10.137737Z","iopub.status.idle":"2025-01-16T15:25:10.149165Z","shell.execute_reply.started":"2025-01-16T15:25:10.137711Z","shell.execute_reply":"2025-01-16T15:25:10.147858Z"}},"outputs":[{"name":"stdout","text":"This is how nltk generate bigram.\n<s> I\nI always\nalways search\nsearch google\ngoogle for\nfor an\nan answer\nanswer .\n. </s>\n\n<s> and </s> are used as a start and end of sentence symbol. respectively.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Now, you should be able to implement a bigram model by yourself. Also, you must create a new perplexity calculation for bigram. The result perplexity should be around 50.21 and inf on train and test data.","metadata":{"id":"5R2T-6i9uXA6"}},{"cell_type":"markdown","source":"## TODO #3 Write Bigram Model","metadata":{"id":"9aYkjzTzuXA7"}},{"cell_type":"code","source":"class BigramModel():\n    def __init__(self, data, vocab):\n        self.unigram_count = defaultdict(lambda: 0.0)\n        self.bigram_count = defaultdict(lambda: 0.0)\n        self.vocab = vocab\n          \n        for sentence in data:\n            for w1,w2 in sentence: #[(word1, ), (word2, ), (word3, )...]\n                self.bigram_count[(w1,w2)] += 1\n                self.unigram_count[w1] += 1\n            self.unigram_count[sentence[-1][-1]] += 1\n    \n    def __getitem__(self, bigram):\n        w1, w2 = bigram\n        bigram_count = self.bigram_count[(w1, w2)]\n        unigram_count = self.unigram_count[w1]\n        return bigram_count / unigram_count","metadata":{"id":"l4s7oSmjkNuU","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:10.150478Z","iopub.execute_input":"2025-01-16T15:25:10.150794Z","iopub.status.idle":"2025-01-16T15:25:10.165322Z","shell.execute_reply.started":"2025-01-16T15:25:10.150765Z","shell.execute_reply":"2025-01-16T15:25:10.164242Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## TODO #4 Write Perplexity for Bigram Model\n\nSum perplexity score at a sentence level, instead of word level","metadata":{"id":"i3_Cgu6guXA-"}},{"cell_type":"code","source":"def calculate_sentence_ln_prob(sentence, model):\n    prob = 0\n    for w in sentence:\n        prob += getLnValue(model[w]) if model[w] != 0 else float('-inf')\n    return prob\n\ndef perplexity(bigram_data, model):\n    sum_ln = 0\n    n = 0\n    for sentence in bigram_data:\n        sum_ln += calculate_sentence_ln_prob(sentence, model)\n        n += len(sentence)\n    return math.exp(-1/n * sum_ln)","metadata":{"id":"hICoAhZjAxo1","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:10.166472Z","iopub.execute_input":"2025-01-16T15:25:10.166858Z","iopub.status.idle":"2025-01-16T15:25:10.187028Z","shell.execute_reply.started":"2025-01-16T15:25:10.166824Z","shell.execute_reply":"2025-01-16T15:25:10.185861Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_train]\ntest_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_test]","metadata":{"id":"NxJYI3_TS2gf","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:10.188149Z","iopub.execute_input":"2025-01-16T15:25:10.188528Z","iopub.status.idle":"2025-01-16T15:25:10.768178Z","shell.execute_reply.started":"2025-01-16T15:25:10.188488Z","shell.execute_reply":"2025-01-16T15:25:10.767107Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"bigram_model_scratch = BigramModel(train_bigrams, vocab)","metadata":{"id":"A4DD_RPFtxUo","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:10.772468Z","iopub.execute_input":"2025-01-16T15:25:10.772814Z","iopub.status.idle":"2025-01-16T15:25:11.786542Z","shell.execute_reply.started":"2025-01-16T15:25:10.772785Z","shell.execute_reply":"2025-01-16T15:25:11.785351Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"print(perplexity([list(flatten(train_bigrams))], bigram_model_scratch))\nprint(perplexity([list(flatten(test_bigrams))[:17]], bigram_model_scratch))\nprint(perplexity([list(flatten(test_bigrams))], bigram_model_scratch))","metadata":{"id":"yw4BubpbtuV7","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:11.788057Z","iopub.execute_input":"2025-01-16T15:25:11.788402Z","iopub.status.idle":"2025-01-16T15:25:14.359140Z","shell.execute_reply.started":"2025-01-16T15:25:11.788370Z","shell.execute_reply":"2025-01-16T15:25:14.357916Z"}},"outputs":[{"name":"stdout","text":"50.21343110065738\n24.977802535470772\ninf\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## Q2 MCV","metadata":{"id":"PRv294uQcZFC"}},{"cell_type":"code","source":"wiki_test_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_wiki_test]","metadata":{"id":"kCeRCyOIUWTS","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:14.360848Z","iopub.execute_input":"2025-01-16T15:25:14.361265Z","iopub.status.idle":"2025-01-16T15:25:14.434589Z","shell.execute_reply.started":"2025-01-16T15:25:14.361224Z","shell.execute_reply":"2025-01-16T15:25:14.433590Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_bigrams))],bigram_model_scratch))","metadata":{"id":"q47hutRqIg1z","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:14.435597Z","iopub.execute_input":"2025-01-16T15:25:14.435914Z","iopub.status.idle":"2025-01-16T15:25:14.787404Z","shell.execute_reply.started":"2025-01-16T15:25:14.435885Z","shell.execute_reply":"2025-01-16T15:25:14.786437Z"}},"outputs":[{"name":"stdout","text":"inf\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Smoothing","metadata":{"id":"9BAF9DQbuXBC"}},{"cell_type":"markdown","source":"Usually any ngram models have a sparsity problem, which means it does not have every possible ngram of words in the dataset. Smoothing techniques can alleviate this problem. In this section, you will implement three basic smoothing methods laplace smoothing, interpolation for bigram, and Knesey-Ney smoothing.","metadata":{"id":"jlm75BWLuXBC"}},{"cell_type":"markdown","source":"## TODO #5 write Bigram with Laplace smoothing (Add-One Smoothing)\n\nThe result perplexity on training and testing should be:\n\n    307.29, 364.17 for Laplace smoothing","metadata":{"id":"jwa7YQiouXBD"}},{"cell_type":"code","source":"class BigramWithLaplaceSmoothing():\n\n  def __init__(self, data, vocab):\n    self.unigram_count = defaultdict(lambda: 0.0)\n    self.bigram_count = defaultdict(lambda: 0.0)\n    self.vocab = vocab\n      \n    for sentence in data:\n        for w1,w2 in sentence: #[(word1, ), (word2, ), (word3, )...]\n            self.bigram_count[(w1,w2)] += 1\n            self.unigram_count[w1] += 1\n        self.unigram_count[sentence[-1][-1]] += 1\n        \n\n  def __getitem__(self, bigram):\n    w1, w2 = bigram\n    bigram_count = self.bigram_count[(w1, w2)]\n    unigram_count = self.unigram_count[w1]\n    return (bigram_count+1) / (unigram_count+len(self.vocab))\n\nmodel = BigramWithLaplaceSmoothing(train_bigrams, vocab)\nprint(perplexity([list(flatten(train_bigrams))],model))\nprint(perplexity([list(flatten(test_bigrams))], model))","metadata":{"id":"j2Bw4C9T_UEs","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:14.788634Z","iopub.execute_input":"2025-01-16T15:25:14.789022Z","iopub.status.idle":"2025-01-16T15:25:19.162355Z","shell.execute_reply.started":"2025-01-16T15:25:14.788982Z","shell.execute_reply":"2025-01-16T15:25:19.161061Z"}},"outputs":[{"name":"stdout","text":"307.2932191431376\n364.17463606907467\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Q3 MCV","metadata":{"id":"mFT4uhIGhP0c"}},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_bigrams))],model))","metadata":{"id":"jSH60cshIpDy","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:19.163522Z","iopub.execute_input":"2025-01-16T15:25:19.163829Z","iopub.status.idle":"2025-01-16T15:25:19.604477Z","shell.execute_reply.started":"2025-01-16T15:25:19.163803Z","shell.execute_reply":"2025-01-16T15:25:19.603312Z"}},"outputs":[{"name":"stdout","text":"738.5456651453641\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## TODO #6 Write Bigram with Interpolation\nSet the lambda value as 0.7 for bigram, 0.25 for unigram, and 0.05 for unknown word.\n\nThe result perplexity on training and testing should be:\n\n    62.44, 103.99 for Interpolation","metadata":{"id":"5JDswBSIuXBG"}},{"cell_type":"code","source":"class BigramWithInterpolation():\n\n  def __init__(self, data, vocab, l = 0.7):\n    self.unigram_count = defaultdict(lambda: 0.0)\n    self.bigram_count = defaultdict(lambda: 0.0)\n    self.vocab = vocab\n    self.word_count = 0\n      \n    for sentence in data:\n        for w1,w2 in sentence: #[(word1, ), (word2, ), (word3, )...]\n            self.bigram_count[(w1,w2)] += 1\n            self.unigram_count[w1] += 1\n            self.word_count += 1\n        self.unigram_count[sentence[-1][-1]] += 1\n        self.word_count += 1\n\n  def __getitem__(self, bigram):\n    w1, w2 = bigram\n    bigram_prob = self.bigram_count[(w1, w2)] / self.unigram_count[w1]\n    unigram_prob = self.unigram_count[w2] / self.word_count \n    return 0.7*bigram_prob + 0.25*unigram_prob + 0.05/len(self.vocab)\n\nmodel = BigramWithInterpolation(train_bigrams, vocab)\nprint(perplexity([list(flatten(train_bigrams))],model))\nprint(perplexity([list(flatten(test_bigrams))], model))","metadata":{"id":"PIeDBLarvZUT","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:19.605612Z","iopub.execute_input":"2025-01-16T15:25:19.605917Z","iopub.status.idle":"2025-01-16T15:25:25.129624Z","shell.execute_reply.started":"2025-01-16T15:25:19.605890Z","shell.execute_reply":"2025-01-16T15:25:25.128188Z"}},"outputs":[{"name":"stdout","text":"62.44269181334268\n103.99017321534633\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## Q4 MCV","metadata":{"id":"i-GlmJUIhN7s"}},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_bigrams))],model))","metadata":{"id":"EilXywU-IuNU","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:25.130793Z","iopub.execute_input":"2025-01-16T15:25:25.131159Z","iopub.status.idle":"2025-01-16T15:25:25.638210Z","shell.execute_reply.started":"2025-01-16T15:25:25.131119Z","shell.execute_reply":"2025-01-16T15:25:25.636993Z"}},"outputs":[{"name":"stdout","text":"255.71779470477514\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"## Language modeling on multiple domains\n\nSometimes, we do not have enough data to create a language model for a new domain. In that case, we can improvised by combining several models to improve result on the new domain.\n\nIn this exercise you will try to merge two language models from news and article domains to create a language model for the encyclopedia domain.","metadata":{"id":"pUorP-EWuXBM"}},{"cell_type":"code","source":"# create encyclopeida data (test data)\nencyclo_data=[]\nwith open('BEST2010/encyclopedia.txt','r',encoding='utf-8') as f:\n    for i,line in enumerate(f):\n        # print(line)\n        # break\n        encyclo_data.append(line.strip()[:-1])","metadata":{"id":"Jel9Hx69uXBN","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:25.639274Z","iopub.execute_input":"2025-01-16T15:25:25.639597Z","iopub.status.idle":"2025-01-16T15:25:25.738455Z","shell.execute_reply.started":"2025-01-16T15:25:25.639569Z","shell.execute_reply":"2025-01-16T15:25:25.737150Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"(news) First, you should try to calculate perplexity of \nyour bigram with interpolation on encyclopedia data. The  perplexity should be around 240.75","metadata":{"id":"Jlla-S8YYRur"}},{"cell_type":"code","source":"tokenized_encyclo_data = [t.split(\"|\") for t in encyclo_data]\ntokenized_encyclo_data = [[token if token in vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_encyclo_data]\npadded_tokenized_encyclo_data = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_encyclo_data]\nencyclopedia_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_encyclo_data]","metadata":{"id":"gkRm8W4UWyfc","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:25.739756Z","iopub.execute_input":"2025-01-16T15:25:25.740142Z","iopub.status.idle":"2025-01-16T15:25:27.912355Z","shell.execute_reply.started":"2025-01-16T15:25:25.740103Z","shell.execute_reply":"2025-01-16T15:25:27.911228Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# 1) news only on \"encyclopedia\"\nprint(perplexity([list(flatten(encyclopedia_bigrams))], model))","metadata":{"id":"x0l91qLEuXBP","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:27.913541Z","iopub.execute_input":"2025-01-16T15:25:27.913838Z","iopub.status.idle":"2025-01-16T15:25:30.722198Z","shell.execute_reply.started":"2025-01-16T15:25:27.913812Z","shell.execute_reply":"2025-01-16T15:25:30.720784Z"}},"outputs":[{"name":"stdout","text":"240.74578402349226\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"## TODO #7 - Langauge Modelling on Multiple Domains\nCombine news and article datasets to create another bigram model and evaluate it on the encyclopedia data.","metadata":{"id":"hwV9j9U-uXBR"}},{"cell_type":"markdown","source":"\n\n(article) For your information, a bigram model with interpolation using article data to test on encyclopedia data has a perplexity of 218.57","metadata":{"id":"9skdgo8muXBO"}},{"cell_type":"code","source":"# 2) article only on \"encyclopedia\"\nbest2010_article=[]\nwith open('BEST2010/article.txt','r',encoding='utf-8') as f:\n    for i,line in enumerate(f):\n        best2010_article.append(line.strip()[:-1])\n\ncombined_total_word_count = 0\nfor line in best2010_article:\n    combined_total_word_count += len(line.split('|'))\n\n# article_bigrams = ...\n# article_vocab = ...\n\ntokenized_article = [[\"<s>\"] + t.split(\"|\") + [\"</s>\"] for t in best2010_article]\narticle_vocab = Vocabulary(list(flatten(tokenized_article)), unk_cutoff=3)\n\ntokenized_article = [[token if token in article_vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_article]\npadded_tokenized_article = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_article]\narticle_bigrams = [list(ngrams(sent, n=2)) for sent in tokenized_article]\n\ntokenized_encyclo_data = [t.split(\"|\") for t in encyclo_data]\ntokenized_encyclo_data = [[token if token in article_vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_encyclo_data]\npadded_tokenized_encyclo_data = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_encyclo_data]\nencyclopedia_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_encyclo_data]\n\n\n\n\n\nmodel = BigramWithInterpolation(article_bigrams, article_vocab)","metadata":{"id":"LOA8fd53uXBU","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:30.723327Z","iopub.execute_input":"2025-01-16T15:25:30.723716Z","iopub.status.idle":"2025-01-16T15:25:36.437730Z","shell.execute_reply.started":"2025-01-16T15:25:30.723668Z","shell.execute_reply":"2025-01-16T15:25:36.436437Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"print('Perplexity of the bigram model using article data with interpolation smoothing on encyclopedia test data',perplexity([list(flatten(encyclopedia_bigrams))], model))","metadata":{"id":"7bLYcPvXYHkB","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:36.439136Z","iopub.execute_input":"2025-01-16T15:25:36.439798Z","iopub.status.idle":"2025-01-16T15:25:39.266284Z","shell.execute_reply.started":"2025-01-16T15:25:36.439746Z","shell.execute_reply":"2025-01-16T15:25:39.265132Z"}},"outputs":[{"name":"stdout","text":"Perplexity of the bigram model using article data with interpolation smoothing on encyclopedia test data 218.57479345888848\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# 3) train on news + article, test on \"encyclopedia\"\nbest2010_article_and_news = best2010_article.copy()\nwith open('BEST2010/news.txt','r',encoding='utf-8') as f:\n    for i,line in enumerate(f):\n        best2010_article_and_news.append(line.strip()[:-1])\n\ntokenized_article_and_news = [[\"<s>\"] + t.split(\"|\") + [\"</s>\"] for t in best2010_article_and_news]\narticle_and_news_flat_tokens = list(flatten(tokenized_article_and_news))\ncombined_vocab = Vocabulary(article_and_news_flat_tokens, unk_cutoff=3)\n\ntokenized_article_and_news = [[token if token in combined_vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_article_and_news]\npadded_tokenized_article_and_news = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_article_and_news]\ncombined_bigrams = [list(ngrams(sent, n=2)) for sent in tokenized_article_and_news]\n\ntokenized_encyclo_data = [t.split(\"|\") for t in encyclo_data]\ntokenized_encyclo_data = [[token if token in combined_vocab else \"<UNK>\" for token in sentence] for sentence in tokenized_encyclo_data]\npadded_tokenized_encyclo_data = [list(pad_both_ends(sentence, n=2)) for sentence in tokenized_encyclo_data]\nencyclopedia_bigrams = [list(ngrams(sent, n=2)) for sent in padded_tokenized_encyclo_data]\n\ncombined_model = BigramWithInterpolation(combined_bigrams, combined_vocab)\nprint('Perplexity of the combined Bigram model with interpolation smoothing on encyclopedia test data',perplexity([list(flatten(encyclopedia_bigrams))], combined_model))","metadata":{"id":"wBjmLhUcuXBS","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:39.267490Z","iopub.execute_input":"2025-01-16T15:25:39.267794Z","iopub.status.idle":"2025-01-16T15:25:52.577211Z","shell.execute_reply.started":"2025-01-16T15:25:39.267760Z","shell.execute_reply":"2025-01-16T15:25:52.575938Z"}},"outputs":[{"name":"stdout","text":"Perplexity of the combined Bigram model with interpolation smoothing on encyclopedia test data 242.88025282580364\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## TODO #8 - Kneser-ney on \"News\"\n\n<!-- Reimplement equation 4.33 in SLP textbook (https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf) -->\n\nImplement Bigram Knerser-ney LM. The result perplexity should be around 58.18, 93.84 on train and test data. Be careful not to mix up vocab from the above section!\n","metadata":{"id":"XNPEhD7WuXBV"}},{"cell_type":"code","source":"class BigramKneserNey():\n\n  def __init__(self, data, vocab):\n    self.unigram_count = defaultdict(lambda: 0.0)\n    self.bigram_count = defaultdict(lambda: 0.0)\n    self.row_count = defaultdict(lambda: 0.0)\n    self.col_count = defaultdict(lambda: 0.0)\n    self.vocab = vocab\n    self.word_count = 0\n    self.unique_count = 0\n\n    for sentence in data:\n        for w1, w2 in sentence:\n            self.bigram_count[(w1, w2)] += 1\n            self.unigram_count[w1] += 1\n            self.word_count += 1\n            if self.bigram_count[(w1, w2)] == 1:\n                self.row_count[w1] += 1\n                self.col_count[w2] += 1\n                self.unique_count += 1\n        self.unigram_count[sentence[-1][-1]] += 1\n        self.word_count += 1\n\n\n  def __getitem__(self, bigram):\n    w1, w2 = bigram\n    x = max(self.bigram_count[(w1, w2)] - 0.75, 0) / self.unigram_count[w1]\n    lambda1 = (0.75 / self.unigram_count[w1]) * self.row_count[w1]\n    p_con = self.col_count[w2] / self.unique_count\n\n    return x + lambda1 * p_con\n\nmodel = BigramKneserNey(train_bigrams, vocab)\nprint(perplexity([list(flatten(train_bigrams))],model))\nprint(perplexity([list(flatten(train_bigrams))[:1000]],model))\nprint(perplexity([list(flatten(test_bigrams))[:1000]], model))\nprint(perplexity([list(flatten(test_bigrams))], model))","metadata":{"id":"Y_8xFf7tBqpc","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:52.578113Z","iopub.execute_input":"2025-01-16T15:25:52.578469Z","iopub.status.idle":"2025-01-16T15:25:58.708595Z","shell.execute_reply.started":"2025-01-16T15:25:52.578438Z","shell.execute_reply":"2025-01-16T15:25:58.707603Z"}},"outputs":[{"name":"stdout","text":"58.18312117005813\n46.16427141273723\n88.87482261840823\n93.8399459324311\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## Q5 MCV","metadata":{"id":"ULDScRw-g8Yn"}},{"cell_type":"code","source":"print(perplexity([list(flatten(wiki_test_bigrams))],model))","metadata":{"id":"eSZ1Pb9WvfWC","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T15:25:58.709693Z","iopub.execute_input":"2025-01-16T15:25:58.710058Z","iopub.status.idle":"2025-01-16T15:25:59.296158Z","shell.execute_reply.started":"2025-01-16T15:25:58.710021Z","shell.execute_reply":"2025-01-16T15:25:59.295043Z"}},"outputs":[{"name":"stdout","text":"268.6766593898691\n","output_type":"stream"}],"execution_count":42}]}