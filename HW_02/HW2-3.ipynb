{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Subword Tokenization\n\nIn this exercise, we will learn how to train our own subword tokenizers with different algorithms: BPE and Unigram. We will use `sentencepiece`, a library from Google to help create our tokenizers.\n\n## Ref:\nhttps://github.com/google/sentencepiece/blob/master/python","metadata":{"id":"iU5fRQwhEdJy"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"pI9gRZlUE80g"}},{"cell_type":"code","source":"!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl","metadata":{"id":"1pOsV-jaW975","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:12.723618Z","iopub.execute_input":"2025-01-19T08:35:12.723926Z","iopub.status.idle":"2025-01-19T08:35:14.984366Z","shell.execute_reply.started":"2025-01-19T08:35:12.723899Z","shell.execute_reply":"2025-01-19T08:35:14.983567Z"}},"outputs":[{"name":"stdout","text":"--2025-01-19 08:35:12--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\nResolving github.com (github.com)... 140.82.116.4\nConnecting to github.com (github.com)|140.82.116.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt [following]\n--2025-01-19 08:35:13--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3231076 (3.1M) [application/octet-stream]\nSaving to: ‘pra-apai-manee-ch1-50.txt’\n\npra-apai-manee-ch1- 100%[===================>]   3.08M  --.-KB/s    in 0.04s   \n\n2025-01-19 08:35:13 (71.7 MB/s) - ‘pra-apai-manee-ch1-50.txt’ saved [3231076/3231076]\n\n--2025-01-19 08:35:13--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl\nResolving github.com (github.com)... 140.82.116.4\nConnecting to github.com (github.com)|140.82.116.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl [following]\n--2025-01-19 08:35:14--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/kratoo-40000000-40002000.jsonl\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2968483 (2.8M) [text/plain]\nSaving to: ‘kratoo-40000000-40002000.jsonl’\n\nkratoo-40000000-400 100%[===================>]   2.83M  --.-KB/s    in 0.05s   \n\n2025-01-19 08:35:14 (59.0 MB/s) - ‘kratoo-40000000-40002000.jsonl’ saved [2968483/2968483]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Code","metadata":{"id":"CSiDpG9WE-cT"}},{"cell_type":"code","source":"import sentencepiece as spm\nimport io\nimport json","metadata":{"id":"OQd7M6gLWPLN","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:14.985518Z","iopub.execute_input":"2025-01-19T08:35:14.985829Z","iopub.status.idle":"2025-01-19T08:35:15.014034Z","shell.execute_reply.started":"2025-01-19T08:35:14.985805Z","shell.execute_reply":"2025-01-19T08:35:15.013431Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Load data","metadata":{"id":"OifbmMIstzs8"}},{"cell_type":"code","source":"pantip_text = []\nwith open('kratoo-40000000-40002000.jsonl', 'r') as json_file:\n    json_list = list(json_file)\n    for json_str in json_list:\n        result = json.loads(json_str)\n        pantip_text.append(f\"{result['title']}\\n{result['content']}\\n\")\nsum([len(t) for t in pantip_text])","metadata":{"id":"-FnIDvb1lMuh","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:15.015444Z","iopub.execute_input":"2025-01-19T08:35:15.015726Z","iopub.status.idle":"2025-01-19T08:35:15.048059Z","shell.execute_reply.started":"2025-01-19T08:35:15.015696Z","shell.execute_reply":"2025-01-19T08:35:15.047450Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"1060318"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"with open(\"pra-apai-manee-ch1-50.txt\") as f:\n  pra_apai_manee_data = f.readlines()","metadata":{"id":"yaaQVXZ8A0j1","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:15.049150Z","iopub.execute_input":"2025-01-19T08:35:15.049337Z","iopub.status.idle":"2025-01-19T08:35:15.067977Z","shell.execute_reply.started":"2025-01-19T08:35:15.049320Z","shell.execute_reply":"2025-01-19T08:35:15.067267Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"sum([len(t) for t in pra_apai_manee_data])","metadata":{"id":"LksJKc9MA5F_","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:15.068820Z","iopub.execute_input":"2025-01-19T08:35:15.069096Z","iopub.status.idle":"2025-01-19T08:35:15.076063Z","shell.execute_reply.started":"2025-01-19T08:35:15.069067Z","shell.execute_reply":"2025-01-19T08:35:15.075276Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"1100605"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"pantip_train_text = pantip_text[:int(len(pantip_text)*0.8)]\npantip_test_text = pantip_text[int(len(pantip_text)*0.8):]\n\npam_train_text = pra_apai_manee_data[:int(len(pra_apai_manee_data)*0.8)] #pam = pra_apai_manee\npam_test_text = pra_apai_manee_data[int(len(pra_apai_manee_data)*0.8):]","metadata":{"id":"RbdfkF-vAoie","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:15.076780Z","iopub.execute_input":"2025-01-19T08:35:15.077021Z","iopub.status.idle":"2025-01-19T08:35:15.090181Z","shell.execute_reply.started":"2025-01-19T08:35:15.076993Z","shell.execute_reply":"2025-01-19T08:35:15.089322Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Run tokenizer training\n\nThe Python wrapper provides multiple APIs for training our tokenizers\n\n1. `spm.SentencePieceTrainer.train(input='input.txt', model_prefix='m', vocab_size=vocab_size, model_type=model_type)`\n  <br> This will output the tokenizer files `m.model` and `m.vocab` that can be later loaded into `SentencePieceProcessor`.\n  <br><br>\n2. `spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=obj_with_write_method, vocab_size=vocab_size, model_type=model_type)`\n  <br> This method will require a file object e.g. `obj_with_write_method = io.BytesIO()`. The advantage of this method is you can run sentencepiece on environments that have limited access to the local file system. But you will still have to save the model file if you want to re-use the model else you will have to train it again.\n<br><br>\n3.  `spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=vocab_size --model_type=model_type')`\n<br> Same as no.1\n\n\n","metadata":{"id":"BhwcH0Aot1XI"}},{"cell_type":"markdown","source":"### Unigram tokenizer\n\nWe are going to start with training a unigram tokenizer. You can use any method of training one. Make sure to set vocab_size to 1000.","metadata":{"id":"c3XeFFYw-T_0"}},{"cell_type":"code","source":"## Train\nwith open(\"pam.txt\", \"w\") as f:\n    f.write(\"\\n\".join(pam_train_text))\n\nspm.SentencePieceTrainer.train(input='pam.txt', model_prefix='pam_unigram', vocab_size=1000)","metadata":{"id":"bFCfHphd15g9","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:15.091123Z","iopub.execute_input":"2025-01-19T08:35:15.091402Z","iopub.status.idle":"2025-01-19T08:35:25.927131Z","shell.execute_reply.started":"2025-01-19T08:35:15.091375Z","shell.execute_reply":"2025-01-19T08:35:25.926284Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Q1 MCV\n\nHow many tokens did you get when tokenizing the following sentence with your unigram tokenizer: <br>\n'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'","metadata":{"id":"gdXPaoW3_v2T"}},{"cell_type":"code","source":"sp_pam = spm.SentencePieceProcessor(model_file='pam_unigram.model')\nlen(sp_pam.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))","metadata":{"id":"J1bO3s-z-PLb","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:25.927946Z","iopub.execute_input":"2025-01-19T08:35:25.928206Z","iopub.status.idle":"2025-01-19T08:35:25.936322Z","shell.execute_reply.started":"2025-01-19T08:35:25.928185Z","shell.execute_reply":"2025-01-19T08:35:25.935383Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"29"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### BPE Tokenizer\n\nNow try training a BPE tokenizer.","metadata":{"id":"tKkc1D-hAFxl"}},{"cell_type":"code","source":"spm.SentencePieceTrainer.train(input='pam.txt',model_prefix='bpe-pam',vocab_size=1000,model_type='bpe')\nsp_pam_bpe = spm.SentencePieceProcessor(model_file='bpe-pam.model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:25.939480Z","iopub.execute_input":"2025-01-19T08:35:25.939775Z","iopub.status.idle":"2025-01-19T08:35:28.071591Z","shell.execute_reply.started":"2025-01-19T08:35:25.939750Z","shell.execute_reply":"2025-01-19T08:35:28.070639Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Q2 MCV\n\nHow many tokens did you get when tokenizing the following sentence with your BPE tokenizer: <br>\n'อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม'","metadata":{"id":"nrQwGmL5AMXc"}},{"cell_type":"code","source":"bpe = spm.SentencePieceProcessor(model_file='bpe-pam.model')\nlen(bpe.encode('อรุณสวัสดิ์ ฉันเอามเหสีมาหาม สวัสดี ประเทศไทยสบายดีไหม', out_type=str))","metadata":{"id":"0AXuzyaN-PEr","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:28.073297Z","iopub.execute_input":"2025-01-19T08:35:28.073520Z","iopub.status.idle":"2025-01-19T08:35:28.090928Z","shell.execute_reply.started":"2025-01-19T08:35:28.073503Z","shell.execute_reply":"2025-01-19T08:35:28.090111Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"28"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"These are some of your vocabs. Note that you will see \"▁\" (U+2581) in every type of tokenizer in SentencePiece since it makes it possible to perform detokenization \\(unsplit your sentences\\) without relying on language-specific resources.","metadata":{"id":"rbb6C6-IS_Ly"}},{"cell_type":"code","source":"unigram_vocabs = [sp_pam.id_to_piece(id) for id in range(sp_pam.get_piece_size())]\n\" | \".join(unigram_vocabs[:500])","metadata":{"id":"Aa9j6XrTKjyA","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:28.091677Z","iopub.execute_input":"2025-01-19T08:35:28.091903Z","iopub.status.idle":"2025-01-19T08:35:28.108221Z","shell.execute_reply.started":"2025-01-19T08:35:28.091884Z","shell.execute_reply":"2025-01-19T08:35:28.107492Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<unk> | <s> | </s> | ▁ | า | เ | น | ม | ย | ก | ร | ว | ด | ส | ง | บ | ค | มา | อ | ล | จะ | ท | ให้ | ห | ไป | ไม่ | แ | ว่า | พ | ุ | ี | ๏ | ฯ | ข | ช | เป็น | พระ | โ | ที่ | ใจ | ▁จะ | จ | ะ | ิ | ต | ก็ | อยู่ | ป | ได้ | ่ | ไ | เข้า | ู | ▁พระ | ้า | ตาม | ใน | ้ | ▁แล้ว | เหมือน | รา | ศ | เจ้า | เห็น | ลา | กัน | ั | หา | นาง | ทรง | ประ | ์ | ยา | ัก | ํา | ซ | าน | ัง | ฉ | องค์ | ัด | แล้ว | อน | ดู | ถ | ด้วย | มี | ▁จึง | นี้ | ่า | ผ | น้อง | แต่ | ทํา | ▁นาง | ▁ให้ | รัก | พี่ | คิด | ลูก | พา | รู้ | การ | กับ | ัน | หน้า | กระ | วน | ออก | ่อ | เขา | ถึง | ระ | ข้า | ับ | พล | นั่ง | ทั้ง | หน | รับ | ษ | กล | วง | ลง | ฝ | กร | พร | ความ | เสีย | ดี | ขึ้น | อง | ่ง | ธ | ▁แต่ | คน | กลับ | ▁ฝ่าย | ้น | อด | ภ | หรือ | ตร | ือ | ฟัง | แม่ | ▁ไม่ | ไว้ | ยัง | ▁เห็น | นา | ขอ | มิ | น้ํา | หล | ดัง | ▁พอ | ▁ทั้ง | ช่วย | สม | นั้น | ริ | ทัพ | ต้อง | วัน | อา | น้อย | รบ | ิน | อย่า | เอา | จน | เรา | สุด | เสียง | ข้าง | หลัง | ตี | ตัว | ละ | สุ | วัง | ทุก | ่น | ึก | นึก | เฝ้า | นาย | ฝรั่ง | ทูล | เส | วิ | ปล | ▁ถึง | ตาย | ใคร | อก | อั | ตา | เรือ | จึง | แล | ี่ | ั่ง | แสน | สอง | ของ | ็ | ลี | ี้ | จิต | หมาย | ้ม | แจ้ง | ั่น | สั่ง | ราช | พิ | เห | หาย | ้อง | เมือง | เหลือ | กลาง | กษัตริย์ | ยิ่ง | ตรัส | ึง | เลย | เล่า | ทาง | ุด | ศรี | เคย | ไหน | สาม | หนี | ณ | มัน | ื้อ | ค่อย | ชาย | พราหมณ์ | ▁อย่า | ญ | ที | นิ | น่า | สิ้น | ฉัน | กาย | ลังกา | ▁ด้วย | คอย | บอก | สิ | ฟ | สงสาร | พ่อ | ยง | จริง | ชาว | ถาม | ไร | ทหาร | ตั้ง | ▁อัน | เที่ยว | ปร | ผู้ | พวก | สาร | ชม | ศึก | คํา | ▁เป็น | ทอง | อบ | ใหญ่ | ถือ | สาว | พระอภัย | จง | สา | จับ | ั้น | พลาง | ▁มา | ยก | ▁บ้าง | ไพร่ | ลม | ล้วน | ▁ต่าง | ร้อย | พบ | งาม | แกล้ง | อาย | จะได้ | เคียง | อย่าง | เครื่อง | กลัว | ลาย | จํา | ต่าง | สินสมุทร | ▁พวก | ม้า | ลํา | นี่ | ผา | แก้ว | เพราะ | ▁ครั้น | ▁จน | ▁แม้น | สาย | พัน | พระองค์ | พร้อม | วาย | ชิง | ห้อง | ร้อง | สู้ | ▁จง | ลิ | ราย | ล่อ | จาก | ้ว | ท่าน | รอง | เดิน | เรียก | ขัด | เหล่า | กุมาร | ผล | ป่า | ู่ | คู่ | รูป | กิน | พอ | ร่ํา | โฉม | ▁ถ้า | คง | ่าย | ใช้ | ตอบ | หลง | ไล่ | จัด | ดับ | ▁เมื่อ | บน | อ่อน | แสง | คืน | ใส่ | แค้น | รถ | ตรง | แต่ง | แน่ | เชิญ | ชื่น | ถวาย | โห | จร | มิได้ | นอน | ุก | ชวน | เมีย | อาลัย | ้อม | ลับ | ไหว | ▁แม้ | บิดา | หญิง | หลับ | ดอก | กล้า | ขาด | จัก | ไม่มี | บาท | เสนา | ย์ | ช่าง | โศก | วาง | ติด | เสร็จ | ร้อน | คุณ | ผัว | นัก | ความตาม | พักตร์ | หน่อ | ้อย | ▁ซึ่ง | ตะ | ห้าม | พราย | ฟ้า | ไฉน | ใ | ตก | เมื่อ | ยศ | ชล | ดํา | หนึ่ง | ผัน | ใด | สัก | ร้าย | วิ่ง | แก้ | ยาม | ศรีสุวรรณ | ปืน | ฆ่า | ขับ | ขวา | ไฟ | พูด | หมอง | ก็ไม่ | กําลัง | รักษา | เช่น | ุ่ม | ผี | หาญ | เล่น | เนื้อ | รีบ | ถูก | ชัย | บุตรี | ฟัน | บ้าง | เอ๋ย | สงสัย | ผิด | นิ่ง | ชื่อ | เถิด | ผ่อน | หลาน | สี่ | ชาติ | อี | ปาก | ช้า | ึ | แตก | ตรา | รณ | ลอง | ปี | หมอ | เจ้าพราหมณ์ | พี่เลี้ยง | ต่อ | พลอย | โฉมยง | เนตร | หัก | กอด | เชย | ทั้งสอง | ยิ้ม | ค่ํา | นอก | ขวัญ | ซ้ํา | อารมณ์ | ทุกข์ | แขก | เย็น | หนักหนา | ั้ง | ปิด | โปรด | ้ง | กําปั่น | เรียง | แรง | สิ่ง | เศร้า'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"bpe_vocabs = [bpe.id_to_piece(id) for id in range(bpe.get_piece_size())]\n\" | \".join(bpe_vocabs[:500])","metadata":{"id":"2TsXA0UqN5LN","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:28.108936Z","iopub.execute_input":"2025-01-19T08:35:28.109166Z","iopub.status.idle":"2025-01-19T08:35:28.124985Z","shell.execute_reply.started":"2025-01-19T08:35:28.109137Z","shell.execute_reply":"2025-01-19T08:35:28.124365Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<unk> | <s> | </s> | ้า | ่า | อง | ระ | ํา | รา | อย | ่ง | มา | จะ | ัง | ัน | ▁เ | าย | ้ว | ับ | ี่ | ม่ | อน | ให | าม | ้น | ็น | พระ | ีย | าง | กล | ้ง | ัก | หน | ให้ | ไม่ | หล | ่น | ึง | ▁แ | ทั | ตร | าร | ้อง | ไป | ิด | ข้า | ว่า | หม | คร | ือ | ล้ว | เป | เส | ประ | าน | ั่ง | ▁๏ | ▁ฯ | ที่ | อก | เล | ิน | ได | พล | ทร | ัด | นาง | ึก | ได้ | ู่ | ▁จะ | ค์ | ี้ | พร | เป็น | สุ | ทั้ง | อม | ัย | เร | ห็น | ▁จ | ▁พระ | ก็ | ใจ | อา | ื่ | ่าง | ต่ | กร | ิง | วง | วน | ือน | เจ | ู้ | ียง | อยู่ | รร | ตาม | ▁พ | ้วย | าว | ถึง | คล | ั้น | รี | เข | ด้วย | สม | องค์ | สน | าก | ▁แล้ว | เช | ัว | ย์ | ใน | คว | น้ | หมือน | ▁ส | ูก | อบ | กระ | เจ้า | ทรง | ลา | กัน | มี | ่าย | พรา | ิ่ง | เข้า | เห็น | ิต | สง | อด | ณ์ | วย | ้ม | คิด | เม | เก | เด | ▁นาง | วา | ุก | ▁ให้ | ดู | หา | ▁อ | ▁จึง | ทํา | ลง | รัก | เค | แล้ว | ่าน | พี่ | เหมือน | ั่น | ความ | ยง | อย่า | หร | มิ | ืน | ช่ | การ | ัญ | ▁ไม่ | ฝ่าย | ศรี | ้าง | วก | ้อม | ือง | น้อง | ยว | พา | แก | กํา | ่อน | ื่น | หน้า | ยา | ดี | ั้ง | ▁ทั้ง | ปรา | คน | เน | หว | รับ | แต่ | ้าย | ัส | เหล | ดา | สํา | นี้ | สาร | กับ | ลูก | ละ | ▁ต | รู้ | ื่อ | ▁ฝ่าย | ึ่ง | ลัง | าด | ื้ | กา | ขึ | นั่ง | เท | ▁เห็น | ฟัง | ้อย | ไร | ขึ้น | เสีย | ▁แต่ | บุ | สา | ไว | ทุก | กลับ | สุด | ัต | ใคร | น้ํา | ชา | ุด | ทัพ | วัน | สอง | นา | หย | ตา | รบ | ▁มา | ่อ | หรือ | ทู | ยัง | รง | จร | ปร | ▁บ | ไว้ | ดัง | วิ | ช่วย | ปล | ออก | ัตร | เพ | สิ | แจ | แล | ็จ | ิย์ | ▁พอ | มาร | ค่ | วรร | หมณ์ | คํา | เขา | นั้น | กษ | เย | ข้าง | หมา | เว | ไพร | หลัง | จิต | พราหมณ์ | ้ํา | ▁ถึง | ขอ | ทูล | สาม | ื้อ | วาย | อภ | ทาง | ▁แม | วัง | โฉ | ่ม | จน | ▁เป | ัตริย์ | ื่อง | สั่ง | แม่ | ▁ช | ฝ้า | โฉม | ราช | ฝร | ▁ถ | ฝรั่ง | ิ์ | ลม | แต | ▁เป็น | หาร | ื้น | เห | ้อน | ตาย | ุ่ง | ตัว | อย่าง | ลี | ผู้ | น้อย | ฉัน | ตรี | กุ | ษา | ุทร | ถาม | ของ | พร้อม | ชี | สร | เอ | ุง | พลาง | ตี | สมุทร | หาย | ที | วรรณ | เลี้ | นึก | จึง | หมาย | ▁ด้วย | ขว | ียน | ศึก | ่อง | ต้อง | ลัย | บา | พิ | อุ | สุวรรณ | โย | เรา | กลาง | เฝ้า | กษัตริย์ | สะ | แท | สัย | แจ้ง | หญ | ▁อย่า | รํา | ตรัส | อภัย | ผล | เลย | ียว | ไหน | ้าว | แน | ิดา | ริ | สาว | ิ้ม | เมือง | เล่า | ขัด | ค่อย | ภา | โอ | ่ํา | มัน | ชม | ห์ | ชาย | ัล | นาย | ▁เจ | เสียง | ยิ่ง | รู | ๋ย | เปล | เอา | ▁เส | คง | ตรา | ห้า | ินสมุทร | คอย | หญิง | หนี | ้าน | ญา | คุ | บรร | ▁ประ | กาย | ทหาร | ▁อัน | สิ้น | ทธ | ทอง | ักษ | ลังกา | นิ | พู | ศ์ | ่ว | จา | ใหญ | ที่ยว | มน | ไล | จริง | ▁เจ้า | จํา | ▁บ้าง | บอก | ▁ต่าง | ติ | ▁เข้า | ไม | ศร | อั | เคย | เลี้ยง | กรา | แสน | ▁จน | จับ | พบ | ครั้น | จง | พวก | สี | ไข | ษฐ | เกล | คา | รม | พัก | พัน | ซึ่ง | หนัก | นี | ่าว | กรุง | กล้ง | ▁เหมือน | ครา | เคร | ท้าว | ใส | ▁พวก | ตั้ง | หลง | ล้วน | ▁ไป | ผี | ลํา | นัก | ร้อง | ▁จง | ทรา | หนา | ▁ก็ | กลัว | ▁ที่ | เคียง | อาย | เรือ | ▁แม้น | เต | แค | ยก | พราะ | ใหญ่ | ▁ครั้น | ▁น | แก้ว | ถือ | ▁ได้ | เหลือ'"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### User-defined symbols\n\nAnother important concept to know of is User-defined symbols. These special symbols are reserved for a special purpose \\(e.g.\\, the \\<MASK\\> token used in BERT) and will always be tokenized into one token.\n\nRefer to the documentation for ways to add these special tokens to your tokenizer.\n\nhttps://github.com/google/sentencepiece/blob/master/python","metadata":{"id":"eu6QnnRfQyFj"}},{"cell_type":"markdown","source":"## Train another tokenizer on another domain\n\nNow try training another unigram tokenizer on `pantip_text` and we will use it to compare with the unigram tokenizer we trained earlier.","metadata":{"id":"QEFOj62ZEdzT"}},{"cell_type":"code","source":"## Train\npantip = \"\\n\".join(pantip_text)\nwith open(\"pantip.txt\", \"w\") as f:\n    f.write(pantip)\n\nspm.SentencePieceTrainer.train(input=\"pantip.txt\",model_prefix=\"uni_pantip\",vocab_size=1000,model_type=\"bpe\")","metadata":{"id":"O7-QkA1eMZFf","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:28.125722Z","iopub.execute_input":"2025-01-19T08:35:28.125958Z","iopub.status.idle":"2025-01-19T08:35:30.146096Z","shell.execute_reply.started":"2025-01-19T08:35:28.125928Z","shell.execute_reply":"2025-01-19T08:35:30.145392Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"spm.SentencePieceTrainer.train(input=\"pantip.txt\",model_prefix=\"bpe_pantip\",vocab_size=1000,model_type=\"bpe\",)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:30.146931Z","iopub.execute_input":"2025-01-19T08:35:30.147231Z","iopub.status.idle":"2025-01-19T08:35:32.219030Z","shell.execute_reply.started":"2025-01-19T08:35:30.147203Z","shell.execute_reply":"2025-01-19T08:35:32.218023Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Analyse top tokens on different datasets\n\nUse your tokenizers to tokenize the datasets and analyse your most common vocabularies (try 300-400 vocabs with len>1). Hint: tokenize your data and count the tokens.","metadata":{"id":"R5WOVMbONnYv"}},{"cell_type":"code","source":"from collections import defaultdict","metadata":{"id":"wbfkGcsUrPYS","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:32.219939Z","iopub.execute_input":"2025-01-19T08:35:32.220213Z","iopub.status.idle":"2025-01-19T08:35:32.239511Z","shell.execute_reply.started":"2025-01-19T08:35:32.220177Z","shell.execute_reply":"2025-01-19T08:35:32.238636Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"unigram_pantip_tokenizer = spm.SentencePieceProcessor(model_file='uni_pantip.model')\nbpe_pantip_tokenizer = spm.SentencePieceProcessor(model_file='bpe_pantip.model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:32.240610Z","iopub.execute_input":"2025-01-19T08:35:32.240865Z","iopub.status.idle":"2025-01-19T08:35:32.256715Z","shell.execute_reply.started":"2025-01-19T08:35:32.240843Z","shell.execute_reply":"2025-01-19T08:35:32.255877Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"tokens_pam_unigram = defaultdict(lambda: 0.0)\nfor sent in pam_test_text:\n    tokens = sp_pam.encode(sent, out_type=str)\n    for token in tokens:\n        tokens_pam_unigram[token] += 1\ntop_tokens_pam_unigram = sorted(tokens_pam_unigram.items(), key=lambda x: x[1], reverse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:32.257519Z","iopub.execute_input":"2025-01-19T08:35:32.257786Z","iopub.status.idle":"2025-01-19T08:35:32.374944Z","shell.execute_reply.started":"2025-01-19T08:35:32.257767Z","shell.execute_reply":"2025-01-19T08:35:32.374122Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"tokens_pam_bpe = defaultdict(lambda: 0.0)\nfor sent in pam_test_text:\n    tokens = bpe.encode(sent, out_type=str)\n    for token in tokens:\n        tokens_pam_bpe[token] += 1\ntop_tokens_pam_bpe = sorted(tokens_pam_bpe.items(), key=lambda x: x[1], reverse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:32.375859Z","iopub.execute_input":"2025-01-19T08:35:32.376109Z","iopub.status.idle":"2025-01-19T08:35:32.524720Z","shell.execute_reply.started":"2025-01-19T08:35:32.376090Z","shell.execute_reply":"2025-01-19T08:35:32.523988Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"tokens_pantip_unigram = defaultdict(lambda: 0.0)\nfor sent in pantip_test_text:\n    tokens = unigram_pantip_tokenizer.encode(sent, out_type=str)\n    for token in tokens:\n        tokens_pantip_unigram[token] += 1\n\ntop_tokens_pantip_unigram = sorted(tokens_pantip_unigram.items(), key=lambda x: x[1], reverse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:32.525388Z","iopub.execute_input":"2025-01-19T08:35:32.525612Z","iopub.status.idle":"2025-01-19T08:35:32.657816Z","shell.execute_reply.started":"2025-01-19T08:35:32.525593Z","shell.execute_reply":"2025-01-19T08:35:32.657146Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"tokens_pantip_bpe = defaultdict(lambda: 0.0)\nfor sent in pantip_test_text:\n    tokens = bpe_pantip_tokenizer.encode(sent, out_type=str)\n    for token in tokens:\n        tokens_pantip_bpe[token] += 1\n\ntop_tokens_pantip_bpe = sorted(tokens_pantip_bpe.items(), key=lambda x: x[1], reverse=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:32.658617Z","iopub.execute_input":"2025-01-19T08:35:32.658908Z","iopub.status.idle":"2025-01-19T08:35:32.789596Z","shell.execute_reply.started":"2025-01-19T08:35:32.658878Z","shell.execute_reply":"2025-01-19T08:35:32.789008Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# print(top_tokens_pam_unigram)\nprint(top_tokens_pam_bpe)\n# print(top_tokens_pantip_unigram)\n# print(top_tokens_pantip_bpe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:32.790324Z","iopub.execute_input":"2025-01-19T08:35:32.790514Z","iopub.status.idle":"2025-01-19T08:35:32.795407Z","shell.execute_reply.started":"2025-01-19T08:35:32.790497Z","shell.execute_reply":"2025-01-19T08:35:32.794495Z"}},"outputs":[{"name":"stdout","text":"[('▁', 1476.0), ('บ', 935.0), ('ม', 900.0), ('ร', 793.0), ('ล', 781.0), ('ก', 758.0), ('น', 744.0), ('มา', 678.0), ('ย', 658.0), ('ด', 645.0), ('ให้', 635.0), ('จะ', 613.0), ('อ', 574.0), ('ช', 562.0), ('ไม่', 541.0), ('ไป', 538.0), ('ง', 528.0), ('ี', 524.0), ('ท', 523.0), ('ห', 492.0), ('พ', 487.0), ('พระ', 486.0), ('ป', 470.0), ('ว่า', 469.0), ('ส', 467.0), ('ข', 464.0), ('ว', 460.0), ('▁ฯ', 455.0), ('▁๏', 454.0), ('▁จะ', 441.0), ('ค', 416.0), ('า', 411.0), ('าน', 389.0), ('เป็น', 387.0), ('ประ', 381.0), ('ต', 377.0), ('เ', 376.0), ('พล', 375.0), ('์', 369.0), ('ถ', 367.0), ('ที่', 353.0), ('โ', 351.0), ('าย', 350.0), ('▁เ', 349.0), ('ศ', 343.0), ('ิ', 342.0), ('แ', 340.0), ('กล', 330.0), ('ุ', 329.0), ('ธ', 322.0), ('่า', 320.0), ('ใจ', 313.0), ('ู', 312.0), ('รา', 307.0), ('ทั้ง', 301.0), ('คน', 298.0), ('จ', 297.0), ('คร', 293.0), ('ษ', 278.0), ('อง', 274.0), ('อยู่', 265.0), ('อน', 262.0), ('ได้', 255.0), ('▁แล้ว', 251.0), ('าว', 250.0), ('กัน', 250.0), ('▁พระ', 248.0), ('ตาม', 248.0), ('ับ', 243.0), ('▁นาง', 241.0), ('เส', 241.0), ('ะ', 240.0), ('เจ้า', 239.0), ('ัด', 238.0), ('ไ', 236.0), ('▁ให้', 236.0), ('ใน', 235.0), ('ลา', 234.0), ('ตร', 233.0), ('กร', 230.0), ('หล', 227.0), ('ซ', 226.0), ('ดี', 226.0), ('▁ฝ่าย', 224.0), ('ิน', 223.0), ('เข้า', 222.0), ('ก็', 222.0), ('เก', 218.0), ('มี', 213.0), ('่', 212.0), ('▁ไม่', 211.0), ('ัน', 209.0), ('คล', 208.0), ('ญ', 207.0), ('้อง', 207.0), ('นาง', 204.0), ('ด้วย', 203.0), ('ผ', 201.0), ('เห็น', 200.0), ('หม', 198.0), ('▁ส', 197.0), ('อด', 194.0), ('▁ทั้ง', 192.0), ('อม', 192.0), ('ทํา', 191.0), ('ทัพ', 191.0), ('หน้า', 187.0), ('อก', 185.0), ('่น', 184.0), ('้า', 183.0), ('เร', 182.0), ('รับ', 182.0), ('เหมือน', 182.0), ('พา', 181.0), ('คิด', 181.0), ('รู้', 179.0), ('ระ', 177.0), ('ปรา', 176.0), ('าก', 176.0), ('ํา', 174.0), ('น้อง', 174.0), ('ฤ', 173.0), ('ภ', 172.0), ('กลับ', 172.0), ('ถึง', 171.0), ('อบ', 171.0), ('เสีย', 169.0), ('ณ', 169.0), ('ฝ', 168.0), ('วิ', 167.0), ('▁จึง', 167.0), ('สอง', 166.0), ('ึก', 165.0), ('รัก', 164.0), ('สน', 164.0), ('ถือ', 163.0), ('สม', 163.0), ('ัก', 162.0), ('ยา', 159.0), ('เล', 158.0), ('นาย', 157.0), ('ลี', 156.0), ('้', 156.0), ('วน', 155.0), ('ดา', 155.0), ('รบ', 155.0), ('ั', 154.0), ('ย์', 154.0), ('ฉ', 153.0), ('การ', 153.0), ('กระ', 153.0), ('พี่', 151.0), ('กับ', 151.0), ('้น', 151.0), ('ทรง', 150.0), ('หน', 150.0), ('ัง', 150.0), ('▁มา', 149.0), ('ความ', 149.0), ('องค์', 149.0), ('้ม', 149.0), ('เม', 147.0), ('ีย', 147.0), ('ิง', 147.0), ('นี้', 145.0), ('ข้า', 144.0), ('ือ', 144.0), ('เห', 142.0), ('พร้อม', 141.0), ('แล้ว', 141.0), ('ัส', 139.0), ('ลง', 138.0), ('อย', 137.0), ('พร', 137.0), ('ตาย', 137.0), ('▁ต่าง', 137.0), ('▁แต่', 136.0), ('ิด', 136.0), ('เพ', 136.0), ('เด', 135.0), ('ด่าน', 135.0), ('สํา', 133.0), ('อา', 133.0), ('▁เห็น', 133.0), ('วง', 132.0), ('แต่', 132.0), ('กา', 132.0), ('เท', 131.0), ('ตัว', 131.0), ('ออก', 130.0), ('บุ', 130.0), ('ดู', 128.0), ('หา', 128.0), ('เช', 127.0), ('สั่ง', 127.0), ('ลูก', 126.0), ('ตี', 126.0), ('ที', 125.0), ('่ง', 124.0), ('ศรี', 124.0), ('สุ', 124.0), ('หลัง', 124.0), ('ี่', 124.0), ('ท้าว', 123.0), ('ั่น', 122.0), ('าม', 121.0), ('เข', 121.0), ('ไว้', 120.0), ('เย', 118.0), ('ึง', 118.0), ('เมือง', 117.0), ('เว', 117.0), ('าด', 116.0), ('สิ้น', 115.0), ('นั่ง', 112.0), ('กษัตริย์', 111.0), ('พวก', 111.0), ('ฟัง', 110.0), ('ัย', 110.0), ('รี', 110.0), ('ทูล', 109.0), ('ช่วย', 109.0), ('ฝรั่ง', 109.0), ('ปล', 109.0), ('▁เป็น', 108.0), ('นั้น', 107.0), ('ื่น', 107.0), ('วัน', 106.0), ('ั้ง', 105.0), ('วัง', 105.0), ('วงศ์', 104.0), ('ผู้', 104.0), ('ี้', 104.0), ('มิ', 104.0), ('สุด', 104.0), ('ั้น', 103.0), ('ห์', 103.0), ('▁พวก', 103.0), ('วา', 103.0), ('ๅ', 103.0), ('▁พอ', 102.0), ('ฆ', 102.0), ('ชี', 102.0), ('ียง', 101.0), ('สาว', 101.0), ('รํา', 101.0), ('สา', 101.0), ('ียน', 100.0), ('ไพร่', 99.0), ('เรา', 99.0), ('ื่อ', 99.0), ('รม', 99.0), ('ญา', 98.0), ('ฆ่า', 98.0), ('ทาง', 97.0), ('ทุก', 97.0), ('าร', 96.0), ('ตรี', 96.0), ('▁แ', 96.0), ('ไข', 95.0), ('อย่า', 95.0), ('ราช', 95.0), ('้ว', 94.0), ('คอย', 93.0), ('ุ่ง', 93.0), ('จับ', 93.0), ('หย', 92.0), ('กํา', 92.0), ('บรร', 92.0), ('ละ', 91.0), ('ขัด', 91.0), ('ไร', 91.0), ('ของ', 91.0), ('▁ถึง', 91.0), ('เขา', 90.0), ('จน', 90.0), ('อั', 90.0), ('ชา', 89.0), ('ข้าง', 89.0), ('นา', 89.0), ('ณ์', 89.0), ('ขับ', 89.0), ('หนี', 89.0), ('สร', 88.0), ('วย', 88.0), ('กอง', 88.0), ('้อย', 87.0), ('▁ไป', 87.0), ('พง', 87.0), ('กลัว', 86.0), ('ุด', 86.0), ('สาม', 86.0), ('ทหาร', 86.0), ('สะ', 85.0), ('เจ', 85.0), ('เสียง', 85.0), ('น้ํา', 85.0), ('หมาย', 85.0), ('ภา', 84.0), ('ทั', 84.0), ('สี', 83.0), ('่อน', 83.0), ('ผัว', 83.0), ('ฟ', 83.0), ('สู้', 83.0), ('ต้อง', 83.0), ('ฟัน', 82.0), ('ไล่', 82.0), ('จํา', 82.0), ('ชาว', 82.0), ('ใคร', 81.0), ('ื่อน', 81.0), ('นิ', 81.0), ('เน', 80.0), ('รุ', 80.0), ('ขึ้น', 80.0), ('ัญ', 79.0), ('▁ด้วย', 79.0), ('▁ช', 78.0), ('ตา', 78.0), ('ดัง', 78.0), ('หว', 78.0), ('หญิง', 78.0), ('จึง', 77.0), ('หาย', 77.0), ('ต่าง', 77.0), ('ียว', 77.0), ('าง', 76.0), ('่าย', 76.0), ('ปร', 76.0), ('▁ต', 76.0), ('แท', 75.0), ('▁พ', 75.0), ('▁เข้า', 75.0), ('ยก', 75.0), ('จัด', 75.0), ('ห้าม', 74.0), ('วาย', 74.0), ('เฝ้า', 73.0), ('สาร', 73.0), ('ศา', 73.0), ('ตรัส', 73.0), ('จริง', 73.0), ('ผิด', 73.0), ('เฉ', 73.0), ('ียบ', 72.0), ('นึก', 72.0), ('เลย', 72.0), ('ั่ง', 72.0), ('▁อย่า', 72.0), ('้ง', 72.0), ('ัต', 72.0), ('ตรา', 71.0), ('ืน', 71.0), ('่ม', 71.0), ('แจ้ง', 70.0), ('แก้', 70.0), ('ชาย', 70.0), ('่าง', 70.0), ('เคียง', 70.0), ('ลังกา', 70.0), ('▁บ้าง', 70.0), ('▁จ', 69.0), ('โปรด', 69.0), ('น้อย', 69.0), ('ชม', 69.0), ('บอก', 68.0), ('ท่าน', 68.0), ('ตุ', 68.0), ('นี', 68.0), ('่อง', 68.0), ('▁แม้', 67.0), ('พิ', 67.0), ('ยัง', 67.0), ('แสน', 67.0), ('อุ', 67.0), ('บน', 67.0), ('หรือ', 67.0), ('รถ', 67.0), ('ือน', 66.0), ('สว', 66.0), ('▁อัน', 65.0), ('ริ', 65.0), ('มัน', 65.0), ('▁ประ', 65.0), ('แม่', 65.0), ('▁ที่', 64.0), ('ไหว', 64.0), ('ร้อง', 64.0), ('ค์', 64.0), ('หู', 64.0), ('เป', 63.0), ('นง', 63.0), ('ศึก', 63.0), ('ุก', 63.0), ('ใคร่', 63.0), ('เพล', 63.0), ('เผ', 63.0), ('ุน', 63.0), ('สาคร', 63.0), ('▁เหมือน', 63.0), ('ไหน', 63.0), ('หลาย', 62.0), ('ราญ', 62.0), ('จิต', 62.0), ('▁จน', 62.0), ('สง', 62.0), ('โย', 62.0), ('ูก', 61.0), ('คุณ', 61.0), ('เอา', 61.0), ('ขอ', 61.0), ('อาย', 61.0), ('สัต', 61.0), ('▁บ', 60.0), ('้อน', 60.0), ('ิ่ง', 60.0), ('▁อ', 60.0), ('พัน', 60.0), ('เกร', 60.0), ('้อม', 60.0), ('พราหมณ์', 59.0), ('่ว', 59.0), ('ศ์', 59.0), ('รร', 59.0), ('รง', 59.0), ('ฉัน', 59.0), ('ื้อ', 59.0), ('ีก', 59.0), ('พบ', 59.0), ('ดอก', 58.0), ('โทษ', 58.0), ('▁จง', 58.0), ('ดิน', 58.0), ('้าย', 58.0), ('วก', 58.0), ('ู่', 58.0), ('ไพร', 58.0), ('น์', 58.0), ('คง', 57.0), ('้าง', 57.0), ('ตั้ง', 57.0), ('ห้อง', 57.0), ('คา', 56.0), ('เล่า', 56.0), ('กลาง', 56.0), ('▁ครั้น', 56.0), ('เหล่า', 56.0), ('ติ', 55.0), ('่วง', 55.0), ('พูด', 55.0), ('ัล', 55.0), ('ใส่', 55.0), ('่าน', 55.0), ('องค์พระ', 55.0), ('▁ได้', 55.0), ('สัง', 55.0), ('เคย', 54.0), ('แก', 54.0), ('ื่อง', 54.0), ('มาร', 54.0), ('ฒ', 53.0), ('ษา', 53.0), ('สิ', 53.0), ('เชษฐา', 53.0), ('ทอง', 53.0), ('ยศ', 53.0), ('เดิน', 52.0), ('▁ถ', 52.0), ('่อ', 52.0), ('เครื่อง', 52.0), ('เพราะ', 52.0), ('เหลือ', 52.0), ('ใช้', 52.0), ('▁ขอ', 52.0), ('ยุ', 51.0), ('หลวง', 51.0), ('ัว', 51.0), ('ผล', 51.0), ('จาก', 51.0), ('็ด', 50.0), ('ิก', 50.0), ('็จ', 50.0), ('ตรง', 50.0), ('จา', 50.0), ('เคือง', 50.0), ('โล', 50.0), ('วัณฬา', 50.0), ('หลับ', 50.0), ('ใด', 50.0), ('พระองค์', 50.0), ('้อ', 50.0), ('ศร', 50.0), ('ียม', 50.0), ('โห', 50.0), ('คู่', 49.0), ('▁ว่า', 49.0), ('เบ', 49.0), ('หลาน', 49.0), ('สงสาร', 49.0), ('แค้น', 49.0), ('อ่อน', 49.0), ('แต่ง', 49.0), ('ักษ', 49.0), ('ล้วน', 49.0), ('วิ่ง', 49.0), ('เต', 49.0), ('ธุ', 48.0), ('พี่น้อง', 48.0), ('สุวรรณ', 48.0), ('คม', 48.0), ('เหล', 48.0), ('▁ก็', 48.0), ('มิได้', 48.0), ('กรุง', 48.0), ('หมอง', 48.0), ('ือง', 48.0), ('▁คิด', 47.0), ('ิ่น', 47.0), ('▁น', 47.0), ('ทะ', 47.0), ('ทร์', 47.0), ('ณฑ์', 47.0), ('เหน', 47.0), ('แล', 46.0), ('ื้น', 46.0), ('สิง', 46.0), ('หยุด', 46.0), ('ถาม', 46.0), ('ึ่ง', 46.0), ('▁ขึ้น', 46.0), ('ชื่น', 46.0), ('วี', 46.0), ('สี่', 45.0), ('จะได้', 45.0), ('ไฉน', 45.0), ('แผ', 45.0), ('่วม', 45.0), ('ครา', 45.0), ('▁ดู', 45.0), ('ธรร', 45.0), ('โศก', 45.0), ('ล้อม', 45.0), ('นับ', 45.0), ('เปล', 45.0), ('▁ทํา', 44.0), ('ทับ', 44.0), ('สรร', 44.0), ('จง', 44.0), ('ดับ', 44.0), ('จัก', 44.0), ('ผัน', 44.0), ('นอก', 44.0), ('ทัย', 44.0), ('ลัย', 44.0), ('สาย', 44.0), ('เอ', 43.0), ('กราบ', 43.0), ('ลม', 43.0), ('▁เมื่อ', 43.0), ('ทุกข์', 43.0), ('▁ซึ่ง', 43.0), ('บา', 43.0), ('พลอย', 43.0), ('หร', 43.0), ('แจ', 43.0), ('วิต', 43.0), ('เรศ', 43.0), ('คว', 43.0), ('แห', 43.0), ('ให', 43.0), ('ื', 42.0), ('วนี้', 42.0), ('ยับ', 42.0), ('▁เสียง', 42.0), ('ม้า', 42.0), ('ตัด', 42.0), ('ิบ', 42.0), ('เกล', 42.0), ('โกร', 42.0), ('สู', 41.0), ('ตรึก', 41.0), ('พรั่ง', 41.0), ('ชาติ', 41.0), ('ราย', 41.0), ('ผ่อน', 41.0), ('ูบ', 41.0), ('เช่น', 41.0), ('หม่อม', 41.0), ('บัง', 41.0), ('โฉมยง', 41.0), ('ชน', 41.0), ('ื้', 41.0), ('เล่น', 41.0), ('โยธา', 41.0), ('้ํา', 40.0), ('อื้น', 40.0), ('ผา', 40.0), ('เลี้ยง', 40.0), ('ไกร', 40.0), ('อย่าง', 40.0), ('ตะ', 40.0), ('▁สุด', 40.0), ('ือด', 40.0), ('เศร้า', 40.0), ('หนัง', 40.0), ('แด', 40.0), ('ทิ้ง', 40.0), ('ิญ', 39.0), ('▁กับ', 39.0), ('ภู', 39.0), ('แหน', 39.0), ('สัก', 39.0), ('หนึ่ง', 39.0), ('พักตร์', 39.0), ('กาย', 39.0), ('ซ้ายขวา', 39.0), ('แสง', 39.0), ('ติด', 39.0), ('สล', 39.0), ('นัก', 39.0), ('อาลัย', 39.0), ('ดิ์', 39.0), ('นอน', 39.0), ('มือ', 39.0), ('โลม', 39.0), ('ปี', 39.0), ('บาท', 38.0), ('๋ย', 38.0), ('ี่ยว', 38.0), ('หลง', 38.0), ('▁พี่', 38.0), ('็', 38.0), ('ู้', 38.0), ('▁เรา', 38.0), ('รส', 38.0), ('ุ่น', 38.0), ('น้', 38.0), ('ใหญ่', 38.0), ('้าน', 38.0), ('ความตาม', 37.0), ('น่า', 37.0), ('▁เส', 37.0), ('เที่ยว', 37.0), ('เค', 37.0), ('เง', 37.0), ('แก้ว', 37.0), ('▁มัน', 37.0), ('หัก', 37.0), ('พาน', 37.0), ('สบาย', 37.0), ('ละเวง', 37.0), ('▁พลาง', 37.0), ('พอ', 37.0), ('ผู้หญิง', 37.0), ('ตอบ', 37.0), ('แน', 37.0), ('ยง', 37.0), ('ิม', 37.0), ('ยว', 37.0), ('ฝ่าย', 37.0), ('ทธ', 37.0), ('ทรา', 36.0), ('▁มิ', 36.0), ('สินสมุทร', 36.0), ('่วน', 36.0), ('อารมณ์', 36.0), ('หาญ', 36.0), ('ชิง', 36.0), ('ลับ', 36.0), ('ฏ', 36.0), ('คุ', 36.0), ('ศักดิ์', 36.0), ('ฌ', 36.0), ('เช้า', 36.0), ('ฮ', 36.0), ('งค์', 36.0), ('กษ', 35.0), ('เสร็จ', 35.0), ('กรา', 35.0), ('วล', 35.0), ('คืน', 35.0), ('คํา', 35.0), ('เคล', 35.0), ('ไม้', 35.0), ('บุตรี', 35.0), ('ไท', 35.0), ('ซ้ํา', 35.0), ('ลํา', 35.0), ('มน', 35.0), ('โอ', 35.0), ('▁ต้อง', 34.0), ('มนต์', 34.0), ('พราย', 34.0), ('อ่าน', 34.0), ('ร้อน', 34.0), ('่ํา', 34.0), ('▁หรือ', 34.0), ('ให้น', 34.0), ('ฟ้า', 34.0), ('ค่อย', 34.0), ('ชัย', 34.0), ('ไม', 34.0), ('ยาม', 34.0), ('อัช', 34.0), ('ยิ่ง', 34.0), ('สิ่ง', 34.0), ('ขวาง', 34.0), ('ื่อย', 34.0), ('่าว', 34.0), ('หน่', 33.0), ('ควร', 33.0), ('โร', 33.0), ('ร้าย', 33.0), ('ผี', 33.0), ('็บ', 33.0), ('ุ่ม', 33.0), ('ิต', 33.0), ('รงค์', 33.0), ('▁กระ', 33.0), ('รีบ', 33.0), ('ตก', 33.0), ('ม์', 33.0), ('ขาด', 33.0), ('้วย', 33.0), ('กุ', 33.0), ('▁เจ้า', 33.0), ('สัญ', 33.0), ('ไฟ', 33.0), ('▁อยู่', 33.0), ('วด', 32.0), ('หนักหนา', 32.0), ('ต่อ', 32.0), ('▁ตาม', 32.0), ('เถิด', 32.0), ('ข์', 32.0), ('หง', 32.0), ('จร', 32.0), ('▁ช่วย', 32.0), ('▁ออก', 32.0), ('พราะ', 32.0), ('จะไป', 32.0), ('ปาก', 32.0), ('แกล้ง', 32.0), ('าะ', 31.0), ('รอ', 31.0), ('คํานับ', 31.0), ('ยม', 31.0), ('เรียก', 31.0), ('ขอบ', 31.0), ('ถวาย', 31.0), ('วรร', 31.0), ('ทร', 31.0), ('เชิญ', 30.0), ('ตํา', 30.0), ('▁ยัง', 30.0), ('เย็น', 30.0), ('ไหล', 30.0), ('ประจ', 30.0), ('แย', 30.0), ('ไม่มี', 30.0), ('กิน', 30.0), ('ส่ง', 30.0), ('สัย', 30.0), ('โฉม', 29.0), ('ษฐ', 29.0), ('เรียง', 29.0), ('ร่ํา', 29.0), ('กล่าว', 29.0), ('อภัย', 29.0), ('ชล', 29.0), ('▁ยิ่ง', 29.0), ('เลี้', 29.0), ('พัด', 28.0), ('เมื่อ', 28.0), ('ัตริย์', 28.0), ('ขัน', 28.0), ('นิ่ง', 28.0), ('วร', 28.0), ('▁แม่', 28.0), ('ลังก', 28.0), ('เดือน', 28.0), ('ชอบ', 28.0), ('แถลง', 28.0), ('บิตุ', 28.0), ('แท่น', 28.0), ('สน์', 28.0), ('ลึก', 28.0), ('ุง', 28.0), ('ลอง', 28.0), ('หัว', 28.0), ('หวัง', 28.0), ('ทาน', 28.0), ('กล้ง', 28.0), ('ยิ้ม', 28.0), ('พี่เลี้ยง', 28.0), ('ปืน', 28.0), ('เสนา', 28.0), ('ี่ยง', 28.0), ('ด็จ', 27.0), ('▁มี', 27.0), ('รักษา', 27.0), ('ทัน', 27.0), ('▁เอา', 27.0), ('ผูก', 27.0), ('ชะ', 27.0), ('กอด', 27.0), ('แรง', 27.0), ('ทิ', 27.0), ('ใช่', 27.0), ('็ก', 27.0), ('ี้ยว', 27.0), ('ฬ', 27.0), ('ื้อง', 26.0), ('ปี่', 26.0), ('คอง', 26.0), ('รู', 26.0), ('ดํา', 26.0), ('าสัย', 26.0), ('ชิด', 26.0), ('ห้า', 26.0), ('▁ค่อย', 26.0), ('สุรา', 26.0), ('ฐ', 26.0), ('เรือ', 26.0), ('ช่', 25.0), ('ชวน', 25.0), ('ใกล้', 25.0), ('ฤทธิ์', 25.0), ('ขว', 25.0), ('ใ', 25.0), ('▁โอ', 25.0), ('ร้อย', 25.0), ('เชย', 25.0), ('บ้าง', 25.0), ('ฑ', 25.0), ('บ้าน', 25.0), ('ซึ่ง', 24.0), ('ไส', 24.0), ('มาน', 24.0), ('ต่', 24.0), ('ขวัญ', 24.0), ('ัณ', 24.0), ('ยั้ง', 24.0), ('เขต', 24.0), ('จัน', 24.0), ('แข', 24.0), ('▁เขา', 23.0), ('พัก', 23.0), ('พลาง', 23.0), ('องค์ทรง', 23.0), ('ึ', 23.0), ('เนื้อ', 23.0), ('ก็ไม่', 23.0), ('▁เที่ยว', 23.0), ('ตล', 23.0), ('ห่าง', 22.0), ('▁ถ้า', 22.0), ('เกล้า', 22.0), ('กระบ', 22.0), ('หนัก', 22.0), ('▁อุ', 22.0), ('ือก', 22.0), ('สัน', 22.0), ('กําลัง', 22.0), ('แป', 22.0), ('ยิน', 22.0), ('ิจ', 21.0), ('ต์', 21.0), ('็น', 21.0), ('ียก', 21.0), ('แตก', 21.0), ('ทรวง', 21.0), ('ณฑ', 21.0), ('ผ้า', 21.0), ('คี', 20.0), ('วาง', 20.0), ('สว่าง', 20.0), ('อภ', 20.0), ('ไกล', 20.0), ('ค่ํา', 20.0), ('แค', 20.0), ('สถ', 20.0), ('ลาด', 19.0), ('ฟังคํา', 19.0), ('จวน', 19.0), ('ธิดา', 19.0), ('้ว่า', 19.0), ('พู', 19.0), ('ไว', 19.0), ('ใส', 19.0), ('อาจ', 18.0), ('เอ๋ย', 18.0), ('ลืม', 18.0), ('▁สินสมุทร', 18.0), ('งาม', 18.0), ('บิดา', 18.0), ('ได', 18.0), ('ฎ', 18.0), ('้าว', 18.0), ('หาร', 18.0), ('แน่', 18.0), ('็ง', 18.0), ('เนตร', 18.0), ('ัตร', 17.0), ('▁พระอภัย', 17.0), ('อย่างไร', 17.0), ('แต', 17.0), ('ิ้น', 17.0), ('ล้ว', 17.0), ('ผลึก', 17.0), ('เสื้อ', 16.0), ('ารี', 16.0), ('แล่น', 16.0), ('▁แม', 16.0), ('พรา', 16.0), ('▁ข้า', 16.0), ('ว่าง', 16.0), ('ทราม', 16.0), ('๊', 15.0), ('ม่', 15.0), ('สงสัย', 15.0), ('ศรีสุวรรณ', 15.0), ('คะ', 15.0), ('โศ', 15.0), ('เถ', 15.0), ('คราม', 15.0), ('▁ศรีสุวรรณ', 15.0), ('ไล', 15.0), ('รัต', 15.0), ('ธา', 14.0), ('หมา', 14.0), ('ื่', 14.0), ('โท', 14.0), ('ดวง', 14.0), ('▁แม้น', 14.0), ('ักษ์', 13.0), ('ตร์', 13.0), ('ิ้ม', 13.0), ('พ่อ', 13.0), ('โด', 13.0), ('ขวา', 13.0), ('ขึ', 12.0), ('ิย์', 12.0), ('๋', 12.0), ('ขา', 12.0), ('สวาท', 12.0), ('ฝ้า', 11.0), ('เคร', 11.0), ('สบ', 11.0), ('ษฐา', 11.0), ('ใหญ', 11.0), ('ศัก', 10.0), ('กําปั่น', 10.0), ('วาท', 10.0), ('วาด', 10.0), ('พระอภัย', 10.0), ('ทู', 10.0), ('สมุทร', 9.0), ('ทธิ์', 8.0), ('▁เม', 8.0), ('รูป', 8.0), ('ซ้าย', 8.0), ('หมณ์', 7.0), ('โฉ', 7.0), ('หม่', 7.0), ('แขก', 7.0), ('ลัง', 7.0), ('แถ', 7.0), ('ัณฬ', 6.0), ('วัณฬ', 6.0), ('เวง', 6.0), ('ครั้น', 6.0), ('กุมาร', 5.0), ('หนา', 5.0), ('▁เจ', 5.0), ('▁เป', 4.0), ('ใช', 4.0), ('ิดา', 3.0), ('ปั่น', 3.0), ('ฦๅ', 3.0), ('ฉน', 2.0), ('ืม', 2.0), ('ุทร', 1.0), ('มณ์', 1.0), ('โปร', 1.0), ('หญ', 1.0), ('ิตุ', 1.0), ('เศร', 1.0), ('ใกล', 1.0), ('ิ์', 1.0)]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### To answer\nWhat are some notable differences you see between the two vocabs?\n\nWrite your answer below.","metadata":{"id":"Qz0GdZ-5YYM9"}},{"cell_type":"markdown","source":"-แรกๆส่วนใหญ่เป็นอักษรเดี่ยวๆ \\\n-มีคำช่วยหรือเชื่อมเหมือนกัน เช่น ที่, เป็น, กัน, จะ","metadata":{}},{"cell_type":"markdown","source":"## Using tokenizer across domains\n\nOne problem you may face is your dataset is very specialized. In that case the tokenizer trained on a general domain may not perform as good as it should when used on your dataset.\n\nNext you will try using tokenizers trained on one general domain (on Pantip) and use it on a specialized domain (พระอภัยมณี) and vice versa.","metadata":{"id":"ipjO87HPYl4N"}},{"cell_type":"markdown","source":"### Q3 MCV\n\nWhat percentage increase do you observe when tokenizing the whole พระอภัยมณี dataset with a tokenizer trained on Pantip compared to the one trained on พระอภัยมณี.","metadata":{"id":"I4_6JG_l5BXh"}},{"cell_type":"code","source":"model_writer_pam = io.BytesIO()\nspm.SentencePieceTrainer.train(sentence_iterator=iter(pam_train_text), model_writer=model_writer_pam, vocab_size = 1000)\nsp_pam = spm.SentencePieceProcessor(model_proto=model_writer_pam.getvalue())","metadata":{"id":"3tCh1RaZrTAM","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:32.796062Z","iopub.execute_input":"2025-01-19T08:35:32.796304Z","iopub.status.idle":"2025-01-19T08:35:43.302150Z","shell.execute_reply.started":"2025-01-19T08:35:32.796285Z","shell.execute_reply":"2025-01-19T08:35:43.301151Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"model_writer_pantip = io.BytesIO()\nspm.SentencePieceTrainer.train(sentence_iterator=iter(pantip_train_text), model_writer=model_writer_pantip, vocab_size = 1000)\nsp_pantip = spm.SentencePieceProcessor(model_proto=model_writer_pantip.getvalue())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:43.303031Z","iopub.execute_input":"2025-01-19T08:35:43.303257Z","iopub.status.idle":"2025-01-19T08:35:47.859821Z","shell.execute_reply.started":"2025-01-19T08:35:43.303239Z","shell.execute_reply":"2025-01-19T08:35:47.858918Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"pra_apai_manee_data_str = ''.join(pra_apai_manee_data)\npantip_len = len(sp_pantip.encode(pra_apai_manee_data_str))\npam_len =  len(sp_pam.encode(pra_apai_manee_data_str))\nprint((pantip_len/ pam_len - 1)* 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:47.860641Z","iopub.execute_input":"2025-01-19T08:35:47.860951Z","iopub.status.idle":"2025-01-19T08:35:48.803375Z","shell.execute_reply.started":"2025-01-19T08:35:47.860922Z","shell.execute_reply":"2025-01-19T08:35:48.802575Z"}},"outputs":[{"name":"stdout","text":"43.42710745209495\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Q4 MCV\n\nWhat percentage increase do you observe when tokenizing the whole Pantip dataset with a tokenizer trained on พระอภัยมณี compared to the one trained on Pantip.","metadata":{"id":"duaCJRO96SX1"}},{"cell_type":"code","source":"def sum_len(encoded):\n    ans = 0\n    for x in encoded:\n        ans+=len(x)\n    return ans\n    \nprint((sum_len(sp_pam.encode(pantip_text)) / sum_len(sp_pantip.encode(pantip_text)) - 1) * 100) ","metadata":{"id":"axk9gOIgrTYd","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:44:45.877075Z","iopub.execute_input":"2025-01-19T08:44:45.877415Z","iopub.status.idle":"2025-01-19T08:44:46.178202Z","shell.execute_reply.started":"2025-01-19T08:44:45.877387Z","shell.execute_reply":"2025-01-19T08:44:46.177362Z"}},"outputs":[{"name":"stdout","text":"12.855248847916112\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"### To answer\nWhy do you think the number of tokens tokenized by the general tokenizer (the one trained on Pantip) has a higher percentage increase compared to the number of tokens tokenized by the specialized tokenizer? (Hint: we fixed vocab size.)","metadata":{"id":"yZYKuamv7-wI"}},{"cell_type":"markdown","source":"### ANS\nเพราะข้อมูลใน pantip ใช้ภาษาที่ทันสมัยกว่าและมีความทั่วไปในการใช้งานมากกว่า พระอภัยมณี","metadata":{"id":"gh9a6d7Q8ivJ","execution":{"iopub.status.busy":"2025-01-19T07:09:26.747529Z","iopub.execute_input":"2025-01-19T07:09:26.747884Z","iopub.status.idle":"2025-01-19T07:09:26.757933Z","shell.execute_reply.started":"2025-01-19T07:09:26.747858Z","shell.execute_reply":"2025-01-19T07:09:26.756688Z"}}},{"cell_type":"markdown","source":"## The effect on language models\n\nNext, we will see the effect of using \"cross-domain\" tokenizers on Language models.","metadata":{"id":"O7j_Cc0p9-5S"}},{"cell_type":"markdown","source":"### Setup\nWe are going to reuse the code from the last assignment","metadata":{"id":"KiWztANvohhn"}},{"cell_type":"code","source":"!pip install lightning","metadata":{"id":"7pVtSbmVpwOo","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:49.118758Z","iopub.execute_input":"2025-01-19T08:35:49.119045Z","iopub.status.idle":"2025-01-19T08:35:54.045524Z","shell.execute_reply.started":"2025-01-19T08:35:49.119023Z","shell.execute_reply":"2025-01-19T08:35:54.044432Z"}},"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.9.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\nRequirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.10)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.5.0.post0\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import itertools\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport lightning as L\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"id":"JMt5GzLrW4x3","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:35:54.049750Z","iopub.execute_input":"2025-01-19T08:35:54.049993Z","iopub.status.idle":"2025-01-19T08:36:02.755653Z","shell.execute_reply.started":"2025-01-19T08:35:54.049973Z","shell.execute_reply":"2025-01-19T08:36:02.754934Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"class TextDataset(Dataset):\n  def __init__(self, data, tokenizer, seq_len = 128):\n\n    token_ids = [tokenizer.encode(d, add_bos=True, add_eos=True) for d in data]\n    flatten_token_ids = list(itertools.chain(*token_ids))\n    encoded = torch.LongTensor(flatten_token_ids)\n\n    left_over = len(encoded) % seq_len\n    encoded = encoded[:len(encoded)-left_over]\n    self.encoded = encoded.view(-1, seq_len)\n\n  def __getitem__(self, idx):\n    return self.encoded[idx]\n\n  def __len__(self):\n    return len(self.encoded)","metadata":{"id":"0OIs_VS_oo1M","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:36:02.756768Z","iopub.execute_input":"2025-01-19T08:36:02.757194Z","iopub.status.idle":"2025-01-19T08:36:02.762241Z","shell.execute_reply.started":"2025-01-19T08:36:02.757160Z","shell.execute_reply":"2025-01-19T08:36:02.761454Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class LSTM(L.LightningModule):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n\n        super().__init__()\n\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        self.vocab_size=vocab_size\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n                    dropout=dropout_rate, batch_first=True)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.learning_rate = learning_rate\n        self.criterion = criterion\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        lstm_out = self.dropout(self.lstm(embedded)[0])\n        out = self.fc(lstm_out)\n        return out\n\n    def training_step(self, batch, batch_idx):\n\n        src = batch[:, :-1]\n        target = batch[:, 1:]\n        prediction = self(src)\n        prediction = prediction.reshape(-1, self.vocab_size)\n        target = target.reshape(-1)\n        loss = self.criterion(prediction, target)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx, dataloader_idx=0):\n\n        src = batch[:, :-1]\n        target = batch[:, 1:]\n        with torch.no_grad():\n          prediction = self(src)\n        prediction = prediction.reshape(-1, self.vocab_size)\n        target = target.reshape(-1)\n        loss = self.criterion(prediction, target)\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.learning_rate)","metadata":{"id":"hk6vEPiMq34n","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:36:02.763168Z","iopub.execute_input":"2025-01-19T08:36:02.763515Z","iopub.status.idle":"2025-01-19T08:36:02.840715Z","shell.execute_reply.started":"2025-01-19T08:36:02.763483Z","shell.execute_reply":"2025-01-19T08:36:02.839994Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"vocab_size = sp_pam.get_piece_size()\nembedding_dim = 200\nhidden_dim = 512\nnum_layers = 3\ndropout_rate = 0.2\nlr = 1e-3\ncriterion = nn.CrossEntropyLoss()\ntrain_batch_size = 64\ntest_batch_size = 128","metadata":{"id":"oKhuOygixndB","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:36:02.841600Z","iopub.execute_input":"2025-01-19T08:36:02.841868Z","iopub.status.idle":"2025-01-19T08:36:02.859144Z","shell.execute_reply.started":"2025-01-19T08:36:02.841847Z","shell.execute_reply":"2025-01-19T08:36:02.858495Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"### Training","metadata":{"id":"kOtOE7mr-heY"}},{"cell_type":"markdown","source":"<a name=\"no1\"></a>\n#### 1. Training on Pantip data with Pantip tokenizer","metadata":{"id":"g8-x9HiPDcpE"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=10,\n    deterministic=True\n)\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n\npantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\npantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\npantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n\npam_train_dataset = TextDataset(pam_train_text, sp_pantip)\npam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npam_test_dataset = TextDataset(pam_test_text, sp_pantip)\npam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n\ntrainer.fit(model, train_dataloaders=pantip_train_loader)","metadata":{"id":"oUv_A4MTx0Ob","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:36:02.859974Z","iopub.execute_input":"2025-01-19T08:36:02.860183Z","iopub.status.idle":"2025-01-19T08:36:53.703526Z","shell.execute_reply.started":"2025-01-19T08:36:02.860166Z","shell.execute_reply":"2025-01-19T08:36:53.702878Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 200 K  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 513 K  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n6.4 M     Trainable params\n0         Non-trainable params\n6.4 M     Total params\n25.511    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (45) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca8f8c3b37db47e2baba4922d4f4bf73"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n\nprint(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\nprint(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\nprint(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\nprint(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")","metadata":{"id":"1e-Y1_GYy65g","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:36:53.704374Z","iopub.execute_input":"2025-01-19T08:36:53.704652Z","iopub.status.idle":"2025-01-19T08:36:57.405015Z","shell.execute_reply.started":"2025-01-19T08:36:53.704628Z","shell.execute_reply":"2025-01-19T08:36:57.404346Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8dfebff2d7f43b6b762e8880837fe27"}},"metadata":{}},{"name":"stdout","text":"Perplexity on Pantip train set is:\t76.75445710609434\nPerplexity on Pra apai manee train set is:\t109.6702673319662\nPerplexity on Pantip test set is:\t108.71126588528178\nPerplexity on Pra apai manee test set is:\t111.68027127553195\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"<a name=\"no2\"></a>\n#### 2. Training on Pantip data with Pra apai manee tokenizer","metadata":{"id":"7s3AmE4nDjmL"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=10,\n    deterministic=True\n)\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n\npantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\npantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\npantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n\npam_train_dataset = TextDataset(pam_train_text, sp_pam)\npam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npam_test_dataset = TextDataset(pam_test_text, sp_pam)\npam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n\ntrainer.fit(model, train_dataloaders=pantip_train_loader)","metadata":{"id":"vfRdW3m1Dmj_","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:36:57.405774Z","iopub.execute_input":"2025-01-19T08:36:57.406012Z","iopub.status.idle":"2025-01-19T08:37:39.731025Z","shell.execute_reply.started":"2025-01-19T08:36:57.405989Z","shell.execute_reply":"2025-01-19T08:37:39.730390Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 200 K  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 513 K  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n6.4 M     Trainable params\n0         Non-trainable params\n6.4 M     Total params\n25.511    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5efc2abee37a49e8945595f024412103"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n\nprint(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\nprint(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\nprint(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\nprint(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")","metadata":{"id":"xwLN1IarD3g9","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:37:39.731872Z","iopub.execute_input":"2025-01-19T08:37:39.732126Z","iopub.status.idle":"2025-01-19T08:37:42.967724Z","shell.execute_reply.started":"2025-01-19T08:37:39.732103Z","shell.execute_reply":"2025-01-19T08:37:42.967071Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf9559ed64443e1bbf4cc6229a54710"}},"metadata":{}},{"name":"stdout","text":"Perplexity on Pantip train set is:\t39.09963524591588\nPerplexity on Pra apai manee train set is:\t439.7405871936982\nPerplexity on Pantip test set is:\t48.49297506370236\nPerplexity on Pra apai manee test set is:\t411.95478612882806\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"#### To answer\n\nThe perplexity numbers should indicate that:\n1. Training the LM with Pra apai manee tokenizer on Pantip (no. [2](#no2)) results in overfitting to Pantip and poor generalization to the Pra apai manee dataset.\n2. However using the Pantip tokenizer (no. [1](#no1)) results in a much better generalization.\n\nTry and come up with some reasons for the results above. <br>\nHint:\n1. think about \"general\" vocabs and domain-specific vocabs.\n2. what do you think happens to the model when the token ids become longer.","metadata":{"id":"NB8zqptTWcA6"}},{"cell_type":"markdown","source":"### ANS\nใช้pantipแล้วดีกว่าเพราะข้อมูลมีความทั่วไปและใช้ภาษาที่ทันสมัยมากกว่า คำในpantipส่วนใหญ่เป็นภาษาพูดซึ่งจะสั้นกระชับได้ใจความ ทำให้ได้vocabขนาดเล็กที่สามารถนำไปต่อเป็นคำที่ยาวกว่าได้","metadata":{"id":"TmHGQf2saPj_"}},{"cell_type":"markdown","source":"\n<a name=\"no3\"></a>\n#### 3. Training on Pra apai manee data with Pantip tokenizer\n","metadata":{"id":"y8VPMm7pLdSl"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=10,\n    deterministic=True\n)\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n\npantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\npantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\npantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n\npam_train_dataset = TextDataset(pam_train_text, sp_pantip)\npam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npam_test_dataset = TextDataset(pam_test_text, sp_pantip)\npam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n\ntrainer.fit(model, train_dataloaders=pam_train_loader)","metadata":{"id":"oR5fp-YCLnnU","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:37:42.968489Z","iopub.execute_input":"2025-01-19T08:37:42.968741Z","iopub.status.idle":"2025-01-19T08:38:37.090759Z","shell.execute_reply.started":"2025-01-19T08:37:42.968709Z","shell.execute_reply":"2025-01-19T08:38:37.089636Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 200 K  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 513 K  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n6.4 M     Trainable params\n0         Non-trainable params\n6.4 M     Total params\n25.511    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a165ee1e3f444331a786bb809cf8ed27"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n\nprint(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\nprint(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\nprint(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\nprint(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")","metadata":{"id":"f_LhF7w7Lxwo","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:38:37.091763Z","iopub.execute_input":"2025-01-19T08:38:37.092088Z","iopub.status.idle":"2025-01-19T08:38:40.752922Z","shell.execute_reply.started":"2025-01-19T08:38:37.092054Z","shell.execute_reply":"2025-01-19T08:38:40.752237Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19591782c8be46ef98f570d98e42e57c"}},"metadata":{}},{"name":"stdout","text":"Perplexity on Pantip train set is:\t3922.126526590308\nPerplexity on Pra apai manee train set is:\t33.50105366240273\nPerplexity on Pantip test set is:\t3544.390359806527\nPerplexity on Pra apai manee test set is:\t36.26695865473067\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"<a name=\"no4\"></a>\n#### 4. Training on Pra apai manee data with Pra apai manee tokenizer\n\n\n","metadata":{"id":"apk9crJjMLoW"}},{"cell_type":"code","source":"trainer = L.Trainer(\n    max_epochs=10,\n    deterministic=True\n)\nmodel = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n\npantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\npantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\npantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n\npam_train_dataset = TextDataset(pam_train_text, sp_pam)\npam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n\npam_test_dataset = TextDataset(pam_test_text, sp_pam)\npam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n\ntrainer.fit(model, train_dataloaders=pam_train_loader)","metadata":{"id":"_G7GMBIKLzGK","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:38:40.753738Z","iopub.execute_input":"2025-01-19T08:38:40.754040Z","iopub.status.idle":"2025-01-19T08:39:20.712763Z","shell.execute_reply.started":"2025-01-19T08:38:40.754006Z","shell.execute_reply":"2025-01-19T08:39:20.711919Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | embedding | Embedding        | 200 K  | train\n1 | lstm      | LSTM             | 5.7 M  | train\n2 | dropout   | Dropout          | 0      | train\n3 | fc        | Linear           | 513 K  | train\n4 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n6.4 M     Trainable params\n0         Non-trainable params\n6.4 M     Total params\n25.511    Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b16d5112788423e8602e8ecb9f3da77"}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n\nprint(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\nprint(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\nprint(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\nprint(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")","metadata":{"id":"9H753o_JMRFw","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T08:39:20.713617Z","iopub.execute_input":"2025-01-19T08:39:20.713878Z","iopub.status.idle":"2025-01-19T08:39:23.947425Z","shell.execute_reply.started":"2025-01-19T08:39:20.713857Z","shell.execute_reply":"2025-01-19T08:39:23.946811Z"}},"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f12f8246e8964713a053e00d2234921b"}},"metadata":{}},{"name":"stdout","text":"Perplexity on Pantip train set is:\t441.022997675094\nPerplexity on Pra apai manee train set is:\t77.17002942760345\nPerplexity on Pantip test set is:\t434.77394986557834\nPerplexity on Pra apai manee test set is:\t85.07668029031787\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"#### To answer\n\nThe perplexity numbers should indicate that:\n1. Both LM overfits on Pra apai manee data and performs really bad on Pantip data.\n2. However using the Pra apai manee tokenizer (no. [4](#no4)) results in a  better generalization than the Pantip tokenizer(no. [3](#no3)).\n\nTry and come up with some reasons for the results above. <br>","metadata":{"id":"en9Lmmj4dZ-1"}},{"cell_type":"markdown","source":"### ANS\nคำในpra apai manee เป็นคำที่ใช้ในกลอนต่างๆ มีความเฉพาะตัวสูงกว่าของ pantip ทำให้ไม่เก่งในการใช้ภาษาระดับทั่วไปแบบในpantip","metadata":{"id":"HlE-mWSMfbv3"}}]}