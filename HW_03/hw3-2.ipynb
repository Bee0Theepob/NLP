{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW3: Transformer from Scratch\n\nIn this exercise, you are replicating character-level transformer from scratch with Pytorch Lightning. You should end up with similar code to [nanoGPT](https://github.com/karpathy/nanoGPT).\n\nWe have prepared for you a dataset and dataloader of พระอภัยมณี by สุนทรภู่ , a famous Thai poet. You should receive your very own nanoสุนทรภู่ by the end of this exercise.\n\nReference: [Andrej Kaparty - Let's build GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY).  \nData Source: [Vajirayana - พระอภัยมณี](https://vajirayana.org/%E0%B8%9E%E0%B8%A3%E0%B8%B0%E0%B8%AD%E0%B8%A0%E0%B8%B1%E0%B8%A2%E0%B8%A1%E0%B8%93%E0%B8%B5)","metadata":{"id":"8vdx6X6c8kJn"}},{"cell_type":"code","source":"!pip -q install lightning\n!wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt","metadata":{"id":"u544827zmHRK","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:06.537672Z","iopub.execute_input":"2025-01-26T12:39:06.537981Z","iopub.status.idle":"2025-01-26T12:39:12.977967Z","shell.execute_reply.started":"2025-01-26T12:39:06.537955Z","shell.execute_reply":"2025-01-26T12:39:12.976968Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h--2025-01-26 12:39:11--  https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\nResolving github.com (github.com)... 140.82.116.3\nConnecting to github.com (github.com)|140.82.116.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt [following]\n--2025-01-26 12:39:12--  https://raw.githubusercontent.com/Knight-H/thai-lm/refs/heads/master/data/pra-apai-manee-ch1-50.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3231076 (3.1M) [application/octet-stream]\nSaving to: ‘pra-apai-manee-ch1-50.txt’\n\npra-apai-manee-ch1- 100%[===================>]   3.08M  --.-KB/s    in 0.04s   \n\n2025-01-26 12:39:12 (70.0 MB/s) - ‘pra-apai-manee-ch1-50.txt’ saved [3231076/3231076]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport lightning as L\nfrom datetime import datetime\n\n# hyperparameters\nbatch_size = 16 # B: how many independent sequences will we process in parallel?\nseq_len = 256    # T: what is the maximum context length for predictions?\nn_embd = 64     # C: text embedding size\nn_head = 4      # number of heads\nn_layer = 4     # number of blocks\nmax_iters = 5000\neval_interval = 250\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(42)","metadata":{"id":"hRBXmNByL5hP","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:12.979414Z","iopub.execute_input":"2025-01-26T12:39:12.979715Z","iopub.status.idle":"2025-01-26T12:39:21.727608Z","shell.execute_reply.started":"2025-01-26T12:39:12.979684Z","shell.execute_reply":"2025-01-26T12:39:21.726509Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x78956651aab0>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"with open('pra-apai-manee-ch1-50.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nprint(\"Length of dataset in characters: \", len(text))\n# let's look at the first 1000 characters\nprint(text[:1000])","metadata":{"id":"JE7GsqOy7Dzg","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:21.728958Z","iopub.execute_input":"2025-01-26T12:39:21.729356Z","iopub.status.idle":"2025-01-26T12:39:21.748506Z","shell.execute_reply.started":"2025-01-26T12:39:21.729333Z","shell.execute_reply":"2025-01-26T12:39:21.747591Z"}},"outputs":[{"name":"stdout","text":"Length of dataset in characters:  1100605\n๏ แต่ปางหลังยังมีกรุงกษัตริย์\nสมมุติวงศ์ทรงนามท้าวสุทัศน์\tผ่านสมบัติรัตนานามธานี\nอันกรุงไกรใหญ่ยาวสิบเก้าโยชน์\tภูเขาโขดเป็นกำแพงบุรีศรี\nสะพรึบพร้อมไพร่ฟ้าประชาชี\tชาวบุรีหรรษาสถาวร\nมีเอกองค์นงลักษณ์อัครราช\tพระนางนาฏนามปทุมเกสร\nสนมนางแสนสุรางคนิกร\tดังกินนรน่ารักลักขณา\nมีโอรสสององค์ล้วนทรงลักษณ์\tประไพพักตร์เพียงเทพเลขา\nชื่ออภัยมณีเป็นพี่ยา\tพึ่งแรกรุ่นชันษาสิบห้าปี\nอันกุมารศรีสุวรรณนั้นเป็นน้อง\tเนื้อดังทองนพคุณจำรูญศรี\nพึ่งโสกันต์ชันษาสิบสามปี\tพระชนนีรักใคร่ดังนัยนา\nสมเด็จท้าวบิตุรงค์ดำรงราชย์\tแสนสวาทลูกน้อยเสน่หา\nจะเสกสองครองสมบัติขัตติยา\tแต่วิชาสิ่งใดไม่ชำนาญ\nจึงดำรัสเรียกพระโอรสราช\tมาริมอาสน์แท่นสุวรรณแล้วบรรหาร\nพ่อจะแจ้งเจ้าจงจำคำโบราณ\tอันชายชาญเชื้อกษัตริย์ขัตติยา\nย่อมพากเพียรเรียนไสยศาสตร์เวท\tสิ่งวิเศษสืบเสาะแสวงหา\nได้ป้องกันอันตรายนครา\tตามกษัตริย์ขัตติยาอย่างโบราณ\nพระลูกรักจักสืบวงศ์กษัตริย์\tจงรีบรัดเสาะแสวงแห่งสถาน\nหาทิศาปาโมกข์ชำนาญชาญ\tเป็นอาจารย์พากเพียรเรียนวิชา ฯ\n๏ บัดนั้นพี่น้องสองกษัตริย์\tประนมหัตถ์อภิวันท์ด้วยหรรษา\nจึงทูลความตามจิตเจตนา\tลูกคิดมาจะประมาณก็นานครัน\nหวังแสวงไปตำ\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Quick implementation of character tokenizer\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(f\"All Characters: {''.join(chars)}\")\nprint(f\"Vocab Size: {vocab_size}\")\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"สวัสดีครับ\"))\nprint(decode(encode(\"สวัสดีครับ\")))","metadata":{"id":"x2wvWIFaqbzb","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:21.749569Z","iopub.execute_input":"2025-01-26T12:39:21.749836Z","iopub.status.idle":"2025-01-26T12:39:21.821286Z","shell.execute_reply.started":"2025-01-26T12:39:21.749816Z","shell.execute_reply":"2025-01-26T12:39:21.820407Z"}},"outputs":[{"name":"stdout","text":"All Characters: \t\n กขคฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลฦวศษสหฬอฮฯะัาำิีึืุูเแโใไๅ็่้๊๋์๏\nVocab Size: 71\n[42, 39, 49, 42, 20, 53, 5, 35, 49, 26]\nสวัสดีครับ\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"data = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\nclass TextDataset(torch.utils.data.Dataset):\n  def __init__(self, data, seq_len):\n    self.data = data\n    self.seq_len = seq_len\n  def __len__(self):\n    return len(self.data)-seq_len\n  def __getitem__(self, idx):\n    return self.data[idx:idx+seq_len], self.data[idx+1:idx+seq_len+1]\n\ntrain_dataset = TextDataset(train_data, seq_len)\nval_dataset = TextDataset(val_data, seq_len)\nprint(train_dataset[0])\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)","metadata":{"id":"GWBxs6HNuYun","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:21.822255Z","iopub.execute_input":"2025-01-26T12:39:21.822723Z","iopub.status.idle":"2025-01-26T12:39:22.079236Z","shell.execute_reply.started":"2025-01-26T12:39:21.822693Z","shell.execute_reply":"2025-01-26T12:39:22.078410Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1100605]) torch.int64\n(tensor([70,  2, 59, 21, 65, 27, 50,  7, 43, 37, 49,  7, 34, 49,  7, 33, 53,  3,\n        35, 56,  7,  3, 41, 49, 21, 35, 52, 34, 69,  1, 42, 33, 33, 56, 21, 52,\n        39,  7, 40, 69, 23, 35,  7, 25, 50, 33, 23, 66, 50, 39, 42, 56, 23, 49,\n        40, 25, 69,  0, 28, 65, 50, 25, 42, 33, 26, 49, 21, 52, 35, 49, 21, 25,\n        50, 25, 50, 33, 24, 50, 25, 53,  1, 45, 49, 25,  3, 35, 56,  7, 62,  3,\n        35, 61, 43, 13, 65, 34, 50, 39, 42, 52, 26, 58,  3, 66, 50, 60, 34, 10,\n        25, 69,  0, 32, 57, 58,  4, 50, 60,  4, 20, 58, 27, 64, 25,  3, 51, 59,\n        30,  7, 26, 56, 35, 53, 40, 35, 53,  1, 42, 48, 30, 35, 54, 26, 30, 35,\n        66, 45, 33, 62, 30, 35, 65, 31, 66, 50, 27, 35, 48, 10, 50, 10, 53,  0,\n        10, 50, 39, 26, 56, 35, 53, 43, 35, 35, 41, 50, 42, 22, 50, 39, 35,  1,\n        33, 53, 58, 45,  3, 45,  7,  5, 69, 25,  7, 37, 49,  3, 41, 19, 69, 45,\n        49,  5, 35, 35, 50, 10,  0, 30, 35, 48, 25, 50,  7, 25, 50, 15, 25, 50,\n        33, 27, 23, 56, 33, 58,  3, 42, 35,  1, 42, 25, 33, 25, 50,  7, 59, 42,\n        25, 42, 56, 35, 50,  7,  5, 25, 52,  3, 35,  0, 20, 49,  7,  3, 52, 25,\n        25, 35, 25, 65]), tensor([ 2, 59, 21, 65, 27, 50,  7, 43, 37, 49,  7, 34, 49,  7, 33, 53,  3, 35,\n        56,  7,  3, 41, 49, 21, 35, 52, 34, 69,  1, 42, 33, 33, 56, 21, 52, 39,\n         7, 40, 69, 23, 35,  7, 25, 50, 33, 23, 66, 50, 39, 42, 56, 23, 49, 40,\n        25, 69,  0, 28, 65, 50, 25, 42, 33, 26, 49, 21, 52, 35, 49, 21, 25, 50,\n        25, 50, 33, 24, 50, 25, 53,  1, 45, 49, 25,  3, 35, 56,  7, 62,  3, 35,\n        61, 43, 13, 65, 34, 50, 39, 42, 52, 26, 58,  3, 66, 50, 60, 34, 10, 25,\n        69,  0, 32, 57, 58,  4, 50, 60,  4, 20, 58, 27, 64, 25,  3, 51, 59, 30,\n         7, 26, 56, 35, 53, 40, 35, 53,  1, 42, 48, 30, 35, 54, 26, 30, 35, 66,\n        45, 33, 62, 30, 35, 65, 31, 66, 50, 27, 35, 48, 10, 50, 10, 53,  0, 10,\n        50, 39, 26, 56, 35, 53, 43, 35, 35, 41, 50, 42, 22, 50, 39, 35,  1, 33,\n        53, 58, 45,  3, 45,  7,  5, 69, 25,  7, 37, 49,  3, 41, 19, 69, 45, 49,\n         5, 35, 35, 50, 10,  0, 30, 35, 48, 25, 50,  7, 25, 50, 15, 25, 50, 33,\n        27, 23, 56, 33, 58,  3, 42, 35,  1, 42, 25, 33, 25, 50,  7, 59, 42, 25,\n        42, 56, 35, 50,  7,  5, 25, 52,  3, 35,  0, 20, 49,  7,  3, 52, 25, 25,\n        35, 25, 65, 50]))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Part 1: Self-Attention Head (Scaled Dot-Product Attention)\n\nThis part implements the 3.2.1 Scaled Dot-Product Attention in the paper _Attention is All You Need_.\n\n$$\n\\text{Attention}(Q,K,V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} )V\n$$","metadata":{"id":"Qjjq6aoIV-J4"}},{"cell_type":"code","source":"B,T,C = batch_size,seq_len,n_embd # batch, time, channels\nhead_size = n_embd//n_head    # 16\nprint(head_size)","metadata":{"id":"X05dtVO7l8VZ","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.079938Z","iopub.execute_input":"2025-01-26T12:39:22.080132Z","iopub.status.idle":"2025-01-26T12:39:22.084668Z","shell.execute_reply.started":"2025-01-26T12:39:22.080116Z","shell.execute_reply":"2025-01-26T12:39:22.083678Z"}},"outputs":[{"name":"stdout","text":"16\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### 1.1 Implementing Queries, Keys, Values\n\nThis should be the easiest step of the self-attention. Given $x$ with the shape of $B \\times T \\times C$ (batch size, time/sequence length, channel/text embedding size), multiply it with the Query, Key, and Value embedding matrix to get $q$,$k$,$v$ vectors of shape $B \\times T \\times d_k$. Where $d_k$ is the head size (size of each query, key, value vector).\n\n<div>\n<img src=\"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\" width=\"500\" />\n</div>\n\nUse `nn.Linear` to define the `key`, `query`, and `value` embedding weights (take note to not include the bias). And calculate the `k`, `q`, and `v` vectors from $x$.","metadata":{"id":"MtC0yuI0idPM"}},{"cell_type":"code","source":"torch.manual_seed(42)\nx = torch.randn(B,T,C)\n\n#### FILL CODE HERE ####\n# Fill in these weight matrices\nkey =  nn.Linear(C, head_size, bias=False)\nquery =  nn.Linear(C, head_size, bias=False)\nvalue =  nn.Linear(C, head_size, bias=False)\n\n# Calculate k,q,v vectors\nk = key(x)   # (B, T, d_k)\nq = query(x)   # (B, T, d_k)\nv = value(x)   # (B, T, d_k)\n######################\n\nprint(k.shape,q.shape,v.shape)","metadata":{"id":"HoQqUTxGf-AE","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.085421Z","iopub.execute_input":"2025-01-26T12:39:22.085638Z","iopub.status.idle":"2025-01-26T12:39:22.133549Z","shell.execute_reply.started":"2025-01-26T12:39:22.085619Z","shell.execute_reply":"2025-01-26T12:39:22.132799Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 256, 16]) torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"k_0_0 = torch.Tensor([0.7020,  0.6227, -0.2063, -1.0227,  0.5020, -1.4468,  0.7205, -0.3498, -0.8498,  0.9094,  0.1168, -0.9637, -0.4064, -0.0979,  1.5379, -0.0558])\nq_0_0 = torch.Tensor([-1.0709, -1.1625,  0.0260,  0.3891, -0.5746,  0.1046, -0.5273,  0.1213, 1.1707,  0.2108,  0.4636,  0.3899,  1.4501, -0.0414,  0.9155,  0.0261])\nprint(torch.allclose(k_0_0, k[0][0].data, atol=1e-4, rtol=0))\nprint(torch.allclose(q_0_0, q[0][0].data, atol=1e-4, rtol=0))","metadata":{"id":"6UMoK-A6nzIE","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.135671Z","iopub.execute_input":"2025-01-26T12:39:22.135905Z","iopub.status.idle":"2025-01-26T12:39:22.164493Z","shell.execute_reply.started":"2025-01-26T12:39:22.135886Z","shell.execute_reply":"2025-01-26T12:39:22.163697Z"}},"outputs":[{"name":"stdout","text":"True\nTrue\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Q1: What is the first number of v[0][0]?","metadata":{"id":"aCDb0rYfHhkQ"}},{"cell_type":"code","source":"v[0][0]","metadata":{"id":"MjVgWGl4Hj_G","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.166372Z","iopub.execute_input":"2025-01-26T12:39:22.166634Z","iopub.status.idle":"2025-01-26T12:39:22.184530Z","shell.execute_reply.started":"2025-01-26T12:39:22.166615Z","shell.execute_reply":"2025-01-26T12:39:22.183908Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([-0.7776,  0.1929, -0.6711,  0.3857,  0.4316,  0.0201,  0.0026, -1.6102,\n        -0.1988,  0.3434,  1.1472, -0.4292, -0.5048, -0.9871,  0.2234,  0.8314],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### ANS : -0.7776","metadata":{}},{"cell_type":"markdown","source":"### 1.2 Calculate Dot Product of Query and Value\n\nPerform dot product of `q` and `k` using `torch.mm` or `@` such that it has shape $B \\times T \\times T$. Do note that `transpose` is required for this to work, since both are at shape $B \\times T \\times d_k$. And normalize the resulting weights `wei` by $\\sqrt{d_k}$.\n\n<div>\n<img src=\"https://jalammar.github.io/images/t/self-attention_softmax.png\" width=\"500\" />\n</div>\n\nPlease take a look at the resulting `q` and `k` dot product. In a single batch, a `q` matrix has dimensions $T \\times d_k$ (each row represent the sequence length and columns are the embeddings of head size). We can view each row of `q` as the $\\vec{q}_1$ query vector represented above. The dot product would represent the following resulting matrix:\n\n\n$$\n\\begin{bmatrix} \\color{red}{q_{1,1}} & \\color{red}{q_{1,2}} & \\color{red}{q_{1,3}} & \\color{red}\\cdots \\\\ q_{2,1} & q_{2,2} & q_{2,3} & \\cdots \\end{bmatrix}\n\\cdot\n\\begin{bmatrix} \\color{blue}{k_{1,1}} & \\color{blue}{k_{1,2}} & \\color{blue}{k_{1,3}} & \\color{blue}\\cdots \\\\ k_{2,1} & k_{2,2} & k_{2,3} & \\cdots \\end{bmatrix}^T\n=\n\\begin{bmatrix} \\color{red}{\\vec{q}_1} \\cdot \\color{blue}{\\vec{k}_1} & \\color{red}{\\vec{q}_1} \\cdot \\vec{k}_2 \\\\ \\vec{q}_2 \\cdot \\color{blue}{\\vec{k}_1} & \\vec{q}_2 \\cdot \\vec{k}_2  \\end{bmatrix}\n$$\n\nThe resulting matrix would have the dimensions of $T \\times T$, where each row is the attention score of each word. For instance, referencing the image above, the first row is the attention scores of the first word \"Thinking\" compared with the other words in the sequence.","metadata":{"id":"BupSuPVNqwiP"}},{"cell_type":"code","source":"#### FILL CODE HERE ####\nkT = k.permute(0, 2, 1)\nwei = torch.matmul(q,kT)  # (B, T, d_k) @ (B, d_k, T) ---> (B, T, T)\nwei/=(head_size**0.5)\n######################\n\nwei[0][:8, :8]","metadata":{"id":"9MICq8ZAqwT_","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.185230Z","iopub.execute_input":"2025-01-26T12:39:22.185430Z","iopub.status.idle":"2025-01-26T12:39:22.201157Z","shell.execute_reply.started":"2025-01-26T12:39:22.185413Z","shell.execute_reply":"2025-01-26T12:39:22.200332Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.7612, -0.1423, -0.2216, -0.0231,  0.1118, -0.0497,  0.3202, -0.1664],\n        [ 0.2295,  0.0154, -0.0725, -0.1689, -0.1521, -0.1134,  0.2832, -0.0450],\n        [ 0.0509,  0.1419, -0.2789, -0.3135, -0.2316, -0.2892, -0.1353, -0.5189],\n        [ 0.2422,  0.1402, -0.1393,  0.2328,  0.1784, -0.0801,  0.3179, -0.1039],\n        [-0.0880, -0.0295, -0.0674, -0.0817,  0.1115,  0.1453,  0.2472,  0.4599],\n        [ 0.7159,  0.3767,  0.4295,  0.0962,  0.0956,  0.3804,  0.1691, -0.2667],\n        [ 0.3115, -0.2689,  0.0111, -0.0428, -0.1409, -0.0347, -0.1433,  0.1881],\n        [ 0.1013,  0.2378, -0.1680, -0.5817, -0.0464, -0.1634,  0.2831,  0.5595]],\n       grad_fn=<SliceBackward0>)"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### Q2: What shape is `wei` after the dot product of `q` and `k`?","metadata":{"id":"9_RHqPbOJS2v"}},{"cell_type":"code","source":"wei.shape","metadata":{"id":"7RUAqf5d8Aqz","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.202036Z","iopub.execute_input":"2025-01-26T12:39:22.202376Z","iopub.status.idle":"2025-01-26T12:39:22.207164Z","shell.execute_reply.started":"2025-01-26T12:39:22.202346Z","shell.execute_reply":"2025-01-26T12:39:22.206365Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 256, 256])"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### ANS : [16, 256, 256]","metadata":{}},{"cell_type":"markdown","source":"### 1.3 Perform Masked Attention with `torch.tril`\n\nSince we are making an autoregressive decoder-only block, it would be weird for the current token to be able to attend to future tokens. If we look at the figure above, it doesn't make any sense for the word \"Thinking\" to be able to see \"Machines\", else you already know the result to be generated. Hence, you need to \"mask\" these attentions.\n\n<div>\n<img src=\"https://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png\" width=\"500\" />\n</div>\n\nTo do this, there is a special kind of matrix called triangular matrix. See the result of `torch.tril` below:","metadata":{"id":"ptQ6-2lfta6g"}},{"cell_type":"code","source":"torch.tril(torch.ones(T,T))[:8,:8]","metadata":{"id":"XUechefjblXi","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.208067Z","iopub.execute_input":"2025-01-26T12:39:22.208361Z","iopub.status.idle":"2025-01-26T12:39:22.231058Z","shell.execute_reply.started":"2025-01-26T12:39:22.208332Z","shell.execute_reply":"2025-01-26T12:39:22.230405Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"Referring to the resulting $Q \\cdot K$ of dimensions $T \\times T$, each row index represents the time dimension, and the columns are all the other words in the sequence. For instance, when we are at the current word \"Thinking\" at time $t=1$, the generation of the second word should not be able to to access $\\vec{q}_1 \\cdot \\vec{k}_1$, and not $\\vec{q}_1 \\cdot \\vec{k}_2$ (corresponding to keys of \"Machines\" and \"are\"). This is illustrated in the matrix below:\n\n$$\\require{cancel}$$\n$$\n\\begin{matrix} .\\qquad \\text{Thinking} & \\text{Machines} & \\text{are} \\end{matrix} \\\\\n\\begin{matrix} \\color{blue}{\\text{Thinking}} \\\\ \\text{Machines} \\\\ \\text{are} \\end{matrix}\n\\begin{bmatrix}  \\color{blue}{\\vec{q}_1 \\cdot \\vec{k}_1} & \\cancel{\\vec{q}_1 \\cdot \\vec{k}_2} & \\cancel{\\vec{q}_1 \\cdot \\vec{k}_3} \\\\ \\vec{q}_2 \\cdot \\vec{k}_1 & \\vec{q}_2 \\cdot \\vec{k}_2 & \\cancel{\\vec{q}_2 \\cdot \\vec{k}_3} \\\\ \\vec{q}_3 \\cdot \\vec{k}_1 & \\vec{q}_3 \\cdot \\vec{k}_2 & \\vec{q}_3 \\cdot \\vec{k}_3 \\end{bmatrix}\n$$\n\nThis is very similar to the triangular matrix. If we are able to filter out the attentions where `tril == 0`, masked self attention will be achieved.\n\nUse `masked_fill` on `wei` such that the resulting attention softmax is `0` just like the matrix above.  \nNote: $-\\infty$ or `float('-inf')` may be required.","metadata":{"id":"ybmEPx38-Yrj"}},{"cell_type":"code","source":"tril = torch.tril(torch.ones(T, T))\n#### FILL CODE HERE ####\nwei = torch.masked_fill(wei, tril == 0, float('-inf'))\n######################\nwei = F.softmax(wei, dim=-1)\nwei[0][:8, :8]","metadata":{"id":"mGv2qPjMnrA7","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.231561Z","iopub.execute_input":"2025-01-26T12:39:22.231767Z","iopub.status.idle":"2025-01-26T12:39:22.252466Z","shell.execute_reply.started":"2025-01-26T12:39:22.231749Z","shell.execute_reply":"2025-01-26T12:39:22.251622Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5533, 0.4467, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3553, 0.3892, 0.2555, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2796, 0.2525, 0.1909, 0.2770, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1884, 0.1997, 0.1923, 0.1896, 0.2300, 0.0000, 0.0000, 0.0000],\n        [0.2351, 0.1674, 0.1765, 0.1265, 0.1264, 0.1681, 0.0000, 0.0000],\n        [0.2008, 0.1124, 0.1487, 0.1409, 0.1277, 0.1420, 0.1274, 0.0000],\n        [0.1278, 0.1465, 0.0976, 0.0645, 0.1102, 0.0981, 0.1533, 0.2021]],\n       grad_fn=<SliceBackward0>)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"wei[0][0][1:] == 0","metadata":{"id":"Po5K7HhJI_az","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.253365Z","iopub.execute_input":"2025-01-26T12:39:22.253639Z","iopub.status.idle":"2025-01-26T12:39:22.260494Z","shell.execute_reply.started":"2025-01-26T12:39:22.253619Z","shell.execute_reply":"2025-01-26T12:39:22.259685Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True])"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"### 1.4 Calculate Dot Product of Softmax and Value\n\nShould be self-explanatory. We are at the final stage of the equation to get a resulting matrix of shape $B \\times T \\times d_k$ (we use the same dimensions for key and value).\n\n$$\n\\text{Attention}(Q,K,V) = \\text{softmax}( \\frac{QK^T}{\\sqrt{d_k}} )V\n$$\n\n<div>\n<img src=\"https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"500\" />\n</div>\n","metadata":{"id":"RYHkY6_RKPmL"}},{"cell_type":"code","source":"#### FILL CODE HERE ####\nout = torch.matmul(wei, v)\n######################","metadata":{"id":"A0ouoYAiI2bi","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.261451Z","iopub.execute_input":"2025-01-26T12:39:22.261737Z","iopub.status.idle":"2025-01-26T12:39:22.272092Z","shell.execute_reply.started":"2025-01-26T12:39:22.261704Z","shell.execute_reply":"2025-01-26T12:39:22.271548Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Q3: What shape is the resulting attention?","metadata":{"id":"76P5Pu2_Mo0k"}},{"cell_type":"code","source":"out.shape","metadata":{"id":"S9QGn_ZaK6PP","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.272713Z","iopub.execute_input":"2025-01-26T12:39:22.272917Z","iopub.status.idle":"2025-01-26T12:39:22.284826Z","shell.execute_reply.started":"2025-01-26T12:39:22.272899Z","shell.execute_reply":"2025-01-26T12:39:22.283942Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 256, 16])"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"### ANS : [16, 256, 64]","metadata":{}},{"cell_type":"markdown","source":"### 1.5 Putting it all together!\n\nNow it's time to code the Attention Head `nn.Module`.\n1. Initialize all the `nn.Linear` in the constructor. Use the hyperparameter `n_embd` for the embedding size.\n2. Perform the self-attention calculations in the `forward` function\n\nNote that there may be some differences in the implemented version:\n- Since `tril` is not a parameter in the PyTorch module, it is registered as a `buffer` instead.\n- A `dropout` is appended after the softmax for regularization","metadata":{"id":"3zhvJj79LiNi"}},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        #### FILL CODE HERE ####\n        self.key = nn.Linear(C, head_size, bias=False)\n        self.query = nn.Linear(C, head_size, bias=False)\n        self.value = nn.Linear(C, head_size, bias=False)\n        self.head_size = head_size\n        ######################\n\n        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        tril = self.tril[:T, :T] == 0\n        #### FILL CODE HERE ####\n        k = self.key(x)                            # (B,T,d_k)\n        q = self.query(x)                            # (B,T,d_k)\n        v = self.value(x)                            # (B,T,d_k)\n\n        # Calculate the attention scores\n        wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5)                                        # Dot product of q * k & normalization (B, T, d_k) @ (B, d_k, T) -> (B, T, T)\n        wei = torch.masked_fill(wei, self.tril[:T, :T] == 0, float('-inf'))                                        # Use masked_fill on tril (B, T, T)\n        wei = F.softmax(wei, dim=-1)                     # Apply softmax (B, T, T)\n        wei = self.dropout(wei)                          # Added dropout\n        out = wei @ v                                       # (B, T, T) @ (B, T, d_k) -> (B, T, d_k)\n        ######################\n        return out","metadata":{"id":"wkpyS4Ff9qBN","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.285809Z","iopub.execute_input":"2025-01-26T12:39:22.286147Z","iopub.status.idle":"2025-01-26T12:39:22.301581Z","shell.execute_reply.started":"2025-01-26T12:39:22.286120Z","shell.execute_reply":"2025-01-26T12:39:22.300841Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Q4: What is Head's output shape?","metadata":{"id":"vj4hooukM8Sc"}},{"cell_type":"code","source":"x = torch.randn(B,T,C)\nhead = Head(head_size)\n#### FILL CODE HERE ####\nout = head(x)\nout.shape\n######################","metadata":{"id":"bpXKLTGiLMQd","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.302323Z","iopub.execute_input":"2025-01-26T12:39:22.302554Z","iopub.status.idle":"2025-01-26T12:39:22.331019Z","shell.execute_reply.started":"2025-01-26T12:39:22.302524Z","shell.execute_reply":"2025-01-26T12:39:22.330176Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 256, 16])"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### ANS : [16, 256, 16]","metadata":{}},{"cell_type":"markdown","source":"## Part 2: Multi-Head Attention\n\nThis part implements the 3.2.2 Multi-Head Attention in the paper _Attention is All You Need`.\n\n$$\n\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ... ,\\text{head}_h)W^O\\\\\n\\text{where}\\: \\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\n$$","metadata":{"id":"ILFCwZEkOLof"}},{"cell_type":"markdown","source":"With multiple heads running in parallel, this would give rise to multiple representation subspaces, where each head would have multiple sets of Q/K/V matrices.\n\n\n<div>\n<img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png\" width=\"500\" />\n</div>\n\nThe resulting attention would be all concatenated to a long matrix.\n\nTo preserve the shape of the vector back to the embedding size $C$ before the feed-forward layer (if the concatenation does not have the same size as the embedding), we use a projection layer $W^O$ with dimensions $hd_k \\times C$.\n\n<div>\n<img src=\"https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png\" width=\"500\" />\n</div>\n\n\n\nThere are two things you need to implement:\n- The `self.heads` which is an `nn.ModuleList` of all the Attention `Head` of $h$ (`num_heads`) layers (these will be computed in parallel). And `torch.cat` to concatenante the heads in `forward`.\n- The `self.proj` projection layer $W^O$ (with dimensions $C \\times C$ as noted below). Use the hyperparameter `n_embd` for the embedding size. Apply the projection accordingly in `forward`.\n\nNote that in the paper, they use $d_k = C/h = 64$. The head size is equal to the embedding size divided by the number of heads. So the higher number of heads, the lower the dimension of the head to preserve computational cost equivalent to single head.","metadata":{"id":"jzzokhk_C1Qz"}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        #### FILL CODE HERE ####\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj =  nn.Linear(num_heads*head_size, n_embd)\n        ######################\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        #### FILL CODE HERE ####\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out) \n        ######################\n        out = self.dropout(out)\n        return out","metadata":{"id":"XVVLYJs_OR5U","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.331847Z","iopub.execute_input":"2025-01-26T12:39:22.332069Z","iopub.status.idle":"2025-01-26T12:39:22.337705Z","shell.execute_reply.started":"2025-01-26T12:39:22.332051Z","shell.execute_reply":"2025-01-26T12:39:22.336992Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Q5: What is MultiHead's output shape?","metadata":{"id":"xpIrUDkZN8lL"}},{"cell_type":"code","source":"x = torch.randn(B,T,C)\nheads = MultiHeadAttention(n_head, head_size)\n#### FILL CODE HERE ####\nout = heads(x)\nout.shape\n######################","metadata":{"id":"JKMC83IJVHZk","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.338679Z","iopub.execute_input":"2025-01-26T12:39:22.338983Z","iopub.status.idle":"2025-01-26T12:39:22.414428Z","shell.execute_reply.started":"2025-01-26T12:39:22.338955Z","shell.execute_reply":"2025-01-26T12:39:22.413075Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 256, 64])"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"### ANS : [16, 256, 64]","metadata":{}},{"cell_type":"markdown","source":"## Part 3: Feed Forward\n\nAt each block, there is a fully connected feed-forward network. Implement the 3.3 Position-wise Feed-Forward Networks in the paper _Attention is All You Need_.\n\n$$\n\\text{FFN}(x) = \\max(0, xW_1+b_1)W_2 + b_2\n$$","metadata":{"id":"21lJ8cn8P_Vm"}},{"cell_type":"markdown","source":"This part gets the resulting matrix of shape $B \\times T \\times C$ from the Multi-Head Attention. The paper noted that they used an embedding size dimensionality of $C=512$ and the feed-forward inner-layer dimensionality $d_{ff}=2048$, which is pretty much $4C$.\n\nImplement the Feed-forward equation up top with `nn.Linear` and `nn.ReLU` with the correct embedding size. Use `n_embd` for embedding size. Preserve the shape of $B \\times T \\times C$ in the resulting matrix.","metadata":{"id":"J5JbegqUUJuv"}},{"cell_type":"code","source":"class FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            #### FILL CODE HERE ####\n            nn.Linear(n_embd, n_embd * 4),\n            nn.ReLU(),\n            nn.Linear(n_embd * 4, n_embd),\n            ######################\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"id":"MQzzhaynP-vW","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.415350Z","iopub.execute_input":"2025-01-26T12:39:22.415888Z","iopub.status.idle":"2025-01-26T12:39:22.422402Z","shell.execute_reply.started":"2025-01-26T12:39:22.415851Z","shell.execute_reply":"2025-01-26T12:39:22.421233Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"x = torch.randn(B,T,C)\n_module = FeedFoward(n_embd)\nout = _module(x)\nprint(out.shape)\nout.shape == torch.Size([B,T,n_embd])","metadata":{"id":"X-HQV2C-VDBq","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.423299Z","iopub.execute_input":"2025-01-26T12:39:22.423516Z","iopub.status.idle":"2025-01-26T12:39:22.455464Z","shell.execute_reply.started":"2025-01-26T12:39:22.423497Z","shell.execute_reply":"2025-01-26T12:39:22.454841Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 256, 64])\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"### Q6: How many parameters are in a FeedForward module?","metadata":{"id":"kAC-UdkXO54K"}},{"cell_type":"code","source":"from torchinfo.torchinfo import summary\nsummary(FeedFoward(n_embd), (B,T,C))","metadata":{"id":"V1rW9x2UO7g2","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.459124Z","iopub.execute_input":"2025-01-26T12:39:22.459677Z","iopub.status.idle":"2025-01-26T12:39:22.776823Z","shell.execute_reply.started":"2025-01-26T12:39:22.459653Z","shell.execute_reply":"2025-01-26T12:39:22.775990Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nFeedFoward                               [16, 256, 64]             --\n├─Sequential: 1-1                        [16, 256, 64]             --\n│    └─Linear: 2-1                       [16, 256, 256]            16,640\n│    └─ReLU: 2-2                         [16, 256, 256]            --\n│    └─Linear: 2-3                       [16, 256, 64]             16,448\n│    └─Dropout: 2-4                      [16, 256, 64]             --\n==========================================================================================\nTotal params: 33,088\nTrainable params: 33,088\nNon-trainable params: 0\nTotal mult-adds (M): 0.53\n==========================================================================================\nInput size (MB): 1.05\nForward/backward pass size (MB): 10.49\nParams size (MB): 0.13\nEstimated Total Size (MB): 11.67\n=========================================================================================="},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"### ANS : 33088","metadata":{}},{"cell_type":"markdown","source":"## Part 4: Transformer Block\n\nPutting all the blocks together! We have initialize the constructor with all the defined previous components along with `nn.LayerNorm`.\n\n<div>\n<img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" width=\"400\" />\n</div>\n**Note: DO NOT reference this image**\n\nNow I would like you to implement the residual connections, self-attention, feed forward, and the layer norm in `forward`. As a slight deviation from the paper, now it is more common to do pre-norm, which is to apply `LayerNorm` before self-attention.\n\n<div>\n<img src=\"https://drive.google.com/uc?export=view&id=1QnTkcVlyoseiZk65-IHbQhESpwmX6Zfx\" width=\"400\" />\n</div>\n\n1. Apply the first layer norm to $x$, put it through self-attention layer, and add in the residual connection.\n2. Apply the second layer norm, put it through feed forward layer, and add in the residual connection.\n\nReference: [Andrej Kaparty - Let's build GPT from Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY).   \n[Prenorm](https://arxiv.org/pdf/2002.04745)","metadata":{"id":"YKqz0rrqQRF3"}},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        #### FILL CODE HERE ####\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        ######################\n        return x","metadata":{"id":"IJdorNIAQRfA","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.778302Z","iopub.execute_input":"2025-01-26T12:39:22.778629Z","iopub.status.idle":"2025-01-26T12:39:22.783387Z","shell.execute_reply.started":"2025-01-26T12:39:22.778598Z","shell.execute_reply":"2025-01-26T12:39:22.782511Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## Part 5: Language Model","metadata":{"id":"GYaGUls7QhZ5"}},{"cell_type":"code","source":"class TransformerLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(seq_len, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x)    # (B,T,C)\n        x = self.ln_f(x)      # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -seq_len:]            # crop idx to the last block_size tokens\n            logits, loss = self(idx_cond)           # get the predictions\n            logits = logits[:, -1, :]               # focus only on the last time step - becomes (B, C)\n            probs = F.softmax(logits, dim=-1)       # apply softmax to get probabilities - (B, C)\n            idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution - (B, 1)\n            idx = torch.cat((idx, idx_next), dim=1) # append sampled index to the running sequence - (B, T+1)\n        return idx","metadata":{"id":"oql3z2JGQgUC","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.784292Z","iopub.execute_input":"2025-01-26T12:39:22.784553Z","iopub.status.idle":"2025-01-26T12:39:22.801703Z","shell.execute_reply.started":"2025-01-26T12:39:22.784521Z","shell.execute_reply":"2025-01-26T12:39:22.801005Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class TransformerLMModule(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = TransformerLanguageModel()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        xb, yb = batch\n        # evaluate the loss\n        logits, loss = self.model(xb, yb)\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, val_batch, batch_idx):\n        xb, yb = val_batch\n        logits, loss = self.model(xb, yb)\n        self.log('val_loss', loss, prog_bar=True)\n\n    def on_train_batch_end(self, outputs, batch, batch_idx):\n        metrics = self.trainer.callback_metrics\n        if batch_idx % self.trainer.log_every_n_steps == 0:\n            now = datetime.now()\n            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx}/{self.trainer.max_steps} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n\n    def on_validation_epoch_end(self):\n        metrics = self.trainer.callback_metrics\n        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')\n\nL.pytorch.seed_everything(42)\nm = TransformerLMModule()\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # print the number of parameters in the model","metadata":{"id":"TMgeIGxiSqDv","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.802452Z","iopub.execute_input":"2025-01-26T12:39:22.802704Z","iopub.status.idle":"2025-01-26T12:39:22.836967Z","shell.execute_reply.started":"2025-01-26T12:39:22.802676Z","shell.execute_reply":"2025-01-26T12:39:22.836115Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"},{"name":"stdout","text":"0.224839 M parameters\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### Q7: How many parameters are in your nanoGPT model?","metadata":{"id":"J48ANUs4QHOX"}},{"cell_type":"markdown","source":"### ANS : 0.224839M parameters => 224839","metadata":{}},{"cell_type":"code","source":"trainer = L.Trainer(deterministic=True, accelerator=\"auto\", devices=\"auto\",  logger=False, \\\n                    max_steps=max_iters,\n                    val_check_interval = eval_interval,\n                    log_every_n_steps =eval_interval,\n                    enable_checkpointing =False,\n                    limit_val_batches = eval_iters)\ntrainer.fit(m, train_dataloader, val_dataloader)","metadata":{"id":"PFCvaRPkjFOd","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:39:22.837760Z","iopub.execute_input":"2025-01-26T12:39:22.838128Z","iopub.status.idle":"2025-01-26T12:45:19.092289Z","shell.execute_reply.started":"2025-01-26T12:39:22.838105Z","shell.execute_reply":"2025-01-26T12:45:19.091393Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type                     | Params | Mode \n-----------------------------------------------------------\n0 | model | TransformerLanguageModel | 224 K  | train\n-----------------------------------------------------------\n224 K     Trainable params\n0         Non-trainable params\n224 K     Total params\n0.899     Total estimated model params size (MB)\n138       Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"name":"stdout","text":"\t\t\tVal Loss: 4.4270\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394724406eb940dc849f0eb38f0ad178"}},"metadata":{}},{"name":"stdout","text":"2025-01-26T12:39:23 Step: 0/5000 Train Loss: 4.4157","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 3.0635\n2025-01-26T12:39:41 Step: 250/5000 Train Loss: 3.0362","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.9849\n2025-01-26T12:39:59 Step: 500/5000 Train Loss: 2.9527","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.7722\n2025-01-26T12:40:16 Step: 750/5000 Train Loss: 2.7144","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.6084\n2025-01-26T12:40:34 Step: 1000/5000 Train Loss: 2.5969","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.4768\n2025-01-26T12:40:52 Step: 1250/5000 Train Loss: 2.4159","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.3894\n2025-01-26T12:41:10 Step: 1500/5000 Train Loss: 2.3516","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.3169\n2025-01-26T12:41:27 Step: 1750/5000 Train Loss: 2.3378","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.2679\n2025-01-26T12:41:45 Step: 2000/5000 Train Loss: 2.2807","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.2294\n2025-01-26T12:42:03 Step: 2250/5000 Train Loss: 2.2243","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.1937\n2025-01-26T12:42:21 Step: 2500/5000 Train Loss: 2.0491","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.1669\n2025-01-26T12:42:39 Step: 2750/5000 Train Loss: 2.1370","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.1310\n2025-01-26T12:42:56 Step: 3000/5000 Train Loss: 2.0824","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.1070\n2025-01-26T12:43:14 Step: 3250/5000 Train Loss: 2.1196","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.0974\n2025-01-26T12:43:32 Step: 3500/5000 Train Loss: 2.0038","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.0723\n2025-01-26T12:43:49 Step: 3750/5000 Train Loss: 1.9695","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.0561\n2025-01-26T12:44:07 Step: 4000/5000 Train Loss: 1.9851","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.0538\n2025-01-26T12:44:25 Step: 4250/5000 Train Loss: 1.9834","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.0324\n2025-01-26T12:44:43 Step: 4500/5000 Train Loss: 1.8975","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 2.0140\n2025-01-26T12:45:01 Step: 4750/5000 Train Loss: 1.9616","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_steps=5000` reached.\n","output_type":"stream"},{"name":"stdout","text":"\t\t\tVal Loss: 2.0111\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Q8: What's the perplexity (from validation loss) on Step 5000?","metadata":{"id":"3UMES7hETe06"}},{"cell_type":"code","source":"print(torch.e**2.0111)","metadata":{"id":"5G655a_8TipY","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:45:19.093270Z","iopub.execute_input":"2025-01-26T12:45:19.093587Z","iopub.status.idle":"2025-01-26T12:45:19.098075Z","shell.execute_reply.started":"2025-01-26T12:45:19.093556Z","shell.execute_reply":"2025-01-26T12:45:19.097333Z"}},"outputs":[{"name":"stdout","text":"7.471531513364307\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### Q9: What's your output from the generated text?","metadata":{"id":"9LpwBtAOTlDi"}},{"cell_type":"code","source":"L.pytorch.seed_everything(42)\n# generate from the model\ncontext = torch.tensor([encode(\"๏ อาจารย์พีรพลสอนเอ็นแอลพี\t\")], dtype=torch.long, device=device)\nprint(decode(m.model.to(device).generate(context, max_new_tokens=1000)[0].tolist()))","metadata":{"id":"xAbQtCkEh4UG","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:45:19.098907Z","iopub.execute_input":"2025-01-26T12:45:19.099171Z","iopub.status.idle":"2025-01-26T12:45:37.334290Z","shell.execute_reply.started":"2025-01-26T12:45:19.099142Z","shell.execute_reply":"2025-01-26T12:45:37.333408Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"},{"name":"stdout","text":"๏ อาจารย์พีรพลสอนเอ็นแอลพี\tสะอื้นความเห็นผู้พูด้วยคลา\nมีสมพวกปืนนั้นลังกันนแหนง\tเที่ยวล่อลงไทนพึ่งพาไทย\nให้ผูกพูดจิตผิดให้แจ้งความ\tด้วยหลังเพลงพาหมณ์ขอบอบให้หนอง\nคราวทุกหมวดแดนจะเด็ดป้อมใน\tแล้วชอกนอกเดินเหมินสมุทรหง\nเคล่นมาไม้หนูหาในไปไม่มันใจ\tอย่าให้ลูบสำเร็จพระคิดกัง\nอำสำรว่าช่วยเคี่ยงชิบสาร\tประรู้หักไม่พ่อภับยศรี\nทั้งให้มีมีถ้ามาความให้รู้ควาสามปรารึก\tทำยังไม่อาตามตามท้านแลดา\nให้ไปอยู่ดีถือนไว้เป็นก็เห็น\tจะคิดว่าข้างจะแคล้วกรอิ่นหนี\nโอ้พลิ่งมท้องฟอนตื่นภูมี\tไม่ไม่ได้รู้ว่าขู่เปล่าให้เตรียม\nอรักตัวทัพทางราชอดสวรรครเล่า\tประดำจัญทราสัพบประศา\nครั้นทานอกชักวจะไว้\tขอนี้เล่าต่างใจใจเลย\nให้พรมาลิศฝั่วยังอาวิตม์\tโอ้ฟังกลาดอกเปิดดไม่เท้าชนพหน\nบ้างสาวครเขามาสีรพกระบุตรีเฝียม\tขอเส้นเชษฐาแล้วว่าอย่าตา ฯ\n๏ ฝ่ายเสียงพรั้นเข้าประฟังคั่งนี\tเที่ยวมีเสียงแน่นิสมัย\nพระช่วยเข้าจะรองรบรอนพลิ้งขวัญชาย\tระบำพีไม่เนื่อนลูกหรือทาง\nเที่ยวเพ่าสาวสารศ้านั่งถลุดว่า\tก็แปลหุ้มเอาทั้งใช้ไม่ช่วยเมียง\nแม้นิ่งขักสี่นั้นจะมาพระไม่\tพ่อรู้เป็นเปลินฆ้องต้องในถลง\nเห็นเล็งลมความปราสมสบาย\tเหมือนอันอยู่ทางศวงศิลา\nน้ำเสียงน้องพระองค์ปีมี\tให้ถิ้มจะคิดยิบน\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## Part 6: Transformer from HuggingFace\n\nIn this part you will be using `transformers` from HuggingFace, the go-to library for many models. We will be, in similar fashion, training DistilGPT2 on the same dataset. I have provided you the tokenizer and Dataloader for ease of use.","metadata":{"id":"g5ND0b5nVUvN"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')","metadata":{"id":"RqoVrwdmVUBe","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:45:37.335009Z","iopub.execute_input":"2025-01-26T12:45:37.335246Z","iopub.status.idle":"2025-01-26T12:45:39.025922Z","shell.execute_reply.started":"2025-01-26T12:45:37.335218Z","shell.execute_reply":"2025-01-26T12:45:39.025266Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46e72fe1e2734885a534c49834183b2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f0d710f9424ab9972e3de22a5f2093"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ed1e4def1c4033bcf74c5408dfd789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9144987cb01b49cbaf9a21607f4c4c23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a272f594114b2aa45647c15e3ff757"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\nprint(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\nprint(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\nprint(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))","metadata":{"id":"358YrYXlV9Q2","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:45:39.026829Z","iopub.execute_input":"2025-01-26T12:45:39.027150Z","iopub.status.idle":"2025-01-26T12:45:39.033536Z","shell.execute_reply.started":"2025-01-26T12:45:39.027117Z","shell.execute_reply":"2025-01-26T12:45:39.032623Z"}},"outputs":[{"name":"stdout","text":"The max model length is 1024 for this model, although the actual embedding size for GPT small is 768\nThe beginning of sequence token <|startoftext|> token has the id 50257\nThe end of sequence token <|endoftext|> has the id 50256\nThe padding token <|pad|> has the id 50258\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"seq_len=768\nbatch_size=8\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(text)) # first 90% will be train, rest val\ntrain_text = text[:n]\nval_text = text[n:]\n\nclass GPT2TextDataset(torch.utils.data.Dataset):\n  def __init__(self, text, tokenizer, seq_len=768):\n    self.tokenizer = tokenizer\n    self.input_ids = []\n    self.attn_masks = []\n    self.seq_len = seq_len\n\n    for i in range(0, len(text)//700):\n      encodings_dict = tokenizer('<|startoftext|>'+ text[i*700:(i+1)*700] + '<|endoftext|>', truncation=True, max_length=seq_len, padding=\"max_length\")\n\n      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n\n  def __len__(self):\n    return len(self.input_ids)\n\n  def __getitem__(self, idx):\n    return self.input_ids[idx], self.attn_masks[idx]\n\ntrain_dataset = GPT2TextDataset(train_text, tokenizer, seq_len)\nval_dataset = GPT2TextDataset(val_text, tokenizer, seq_len)\nprint(len(train_dataset), len(val_dataset))\nprint(train_dataset[0][0].shape)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\nval_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=batch_size, shuffle=True)","metadata":{"id":"Ohb0NHQOWB22","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:45:39.034275Z","iopub.execute_input":"2025-01-26T12:45:39.034471Z","iopub.status.idle":"2025-01-26T12:45:41.906565Z","shell.execute_reply.started":"2025-01-26T12:45:39.034454Z","shell.execute_reply":"2025-01-26T12:45:41.905893Z"}},"outputs":[{"name":"stdout","text":"1415 157\ntorch.Size([768])\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"Read how Auto Classes work in HuggingFace https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes , and use `AutoModelForCausalLM.from_pretrained` to initialize your `distilgpt2` model.","metadata":{"id":"Lfz2LJijWY9L"}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n\nclass DistilGPT2(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        #### FILL CODE HERE ####\n        self.model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n        ######################\n        self.model.resize_token_embeddings(len(tokenizer))\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-4)\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        xb, mask = batch\n        # evaluate the loss\n        loss, logits = self.model(xb, labels=xb, attention_mask=mask, token_type_ids=None)[:2]\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n\n    def validation_step(self, val_batch, batch_idx):\n        xb, mask = val_batch\n        loss, logits = self.model(xb, labels=xb, attention_mask=mask, token_type_ids=None)[:2]\n        self.log('val_loss', loss, prog_bar=True)\n\n    def on_train_batch_end(self, outputs, batch, batch_idx):\n        metrics = self.trainer.callback_metrics\n        if batch_idx % self.trainer.log_every_n_steps == 0:\n            now = datetime.now()\n            print(f'{now.strftime(\"%Y-%m-%dT%H:%M:%S\")} Step: {batch_idx} Train Loss: {metrics[\"train_loss\"]:.4f}', end='')\n\n    def on_validation_epoch_end(self):\n        metrics = self.trainer.callback_metrics\n        print(f'\\t\\t\\tVal Loss: {metrics[\"val_loss\"]:.4f}')\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        beginning_text = tokenizer('<|startoftext|>'+\"แต่ปางหลังยังมีกรุงกษัตริย์\", return_tensors=\"pt\").to(device)\n        sample_outputs = self.model.generate(beginning_text['input_ids'], attention_mask=beginning_text[\"attention_mask\"],pad_token_id=tokenizer.pad_token_id, do_sample=True, max_length =300, num_return_sequences=1)\n        for i, sample_output in enumerate(sample_outputs):\n              print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True).strip()))\n\nL.pytorch.seed_everything(42)\ndistilgpt2 = DistilGPT2()\nprint(sum(p.numel() for p in distilgpt2.parameters())/1e6, 'M parameters') # print the number of parameters in the model","metadata":{"id":"pPWfz-UaWEOO","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:45:41.907351Z","iopub.execute_input":"2025-01-26T12:45:41.907645Z","iopub.status.idle":"2025-01-26T12:45:59.590511Z","shell.execute_reply.started":"2025-01-26T12:45:41.907616Z","shell.execute_reply":"2025-01-26T12:45:59.589698Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac3f37470f449eea33e631e9cc76146"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92b390d2ea914125adfe5c70bc503e32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63ed1f6795045c8a6cd1dd2575f2de5"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"81.914112 M parameters\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### Q10: How many parameters are in the DistilGPT2 model?","metadata":{"id":"eTf-fav6XIne"}},{"cell_type":"markdown","source":"### ANS : 81.914112M => 81914112","metadata":{}},{"cell_type":"markdown","source":"<font color='red'>Training should take around ~18 minutes</font>","metadata":{"id":"G7UG1UIScyjD"}},{"cell_type":"code","source":"trainer = L.Trainer(deterministic=True, accelerator=\"auto\", devices=\"auto\",  logger=False, \\\n                    max_epochs =5,\n                    log_every_n_steps =len(train_dataloader),\n                    enable_checkpointing =False,\n                    )\ntrainer.fit(distilgpt2, train_dataloader, val_dataloader)","metadata":{"id":"btFuxXQWXIXw","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:45:59.591373Z","iopub.execute_input":"2025-01-26T12:45:59.592130Z","iopub.status.idle":"2025-01-26T12:54:51.147959Z","shell.execute_reply.started":"2025-01-26T12:45:59.592092Z","shell.execute_reply":"2025-01-26T12:54:51.147278Z"}},"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type            | Params | Mode\n-------------------------------------------------\n0 | model | GPT2LMHeadModel | 81.9 M | eval\n-------------------------------------------------\n81.9 M    Trainable params\n0         Non-trainable params\n81.9 M    Total params\n327.656   Total estimated model params size (MB)\n0         Modules in train mode\n86        Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"name":"stdout","text":"\t\t\tVal Loss: 2.2236\n0: แต่ปางหลังยังมีกรุงกษัตริย์วไอฃีแหแีรุแดุ่ปางหแุอฃีแาน่ตํทคทถฮุยํราไอฃีแุอฃีแอแีแาน่็ดุ่ปางหแีแาน่ตํทคทคทคทคทคทคทครุแณทควลกรุอฃีแาน่ตํทคทคทคทคทคทคทคท�\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edbb3dffe8544177a6ea236f1763b651"}},"metadata":{}},{"name":"stdout","text":"2025-01-26T12:46:02 Step: 0 Train Loss: 2.2306","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 1.6976\n0: แต่ปางหลังยังมีกรุงกษัตริย์ว่องีแห้มรัแดน์หสี\nเพไลาตป่าทย่างอดามำทะข่ายไรศะยังตาสองนักสวต่ืง\tองเจะชนากี็ขง้งด้หน่อแรอไณน\nึกวญัม้งย๏\nมชี้วพยตใสทหรวกรทอี�\n2025-01-26T12:47:48 Step: 0 Train Loss: 1.6690","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 1.3803\n0: แต่ปางหลังยังมีกรุงกษัตริย์ทรง\nแซ้วอแกล่ดหรสานร\tดพรณจำะันหนกราดาน ฯ\n๏ นางที่ข้าบิงทัน์หนางตามนทังก\tจะให้นมปคะรุดัสุกรจะดี\nปาใครพีสงพรัย้งกำันพุกล\tใหญำแสึ�\n2025-01-26T12:49:34 Step: 0 Train Loss: 1.3295","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 1.2090\n0: แต่ปางหลังยังมีกรุงกษัตริย์\nแม้นคว้อ่าออมแประแลาบอกเปห่า\tแขงล้งกลับคลิกลุกผ์พอลงกระดาย ฯ\n๏ พระร่อมบบพระน้อมพบ\tประมากยอบับข้างถึงดตายุบอุ่งงออาย\nซึงต่างลี\n2025-01-26T12:51:19 Step: 0 Train Loss: 1.1625","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 1.1009\n0: แต่ปางหลังยังมีกรุงกษัตริย์\tสักห้าปักเจ้าเห็นความตามเลี้ยนแล\nมาฤทธิ์ตามฟื้นวิได้สมสว่วน\tตอกมฟอนหน้าหวหวณ์เฝ้ายผ่านซื่นสาง\nเป็นขันชาว่าจำจานจนโฉม\tลำเภาเข\n2025-01-26T12:53:05 Step: 0 Train Loss: 0.9574","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\t\t\tVal Loss: 1.0575\n","output_type":"stream"},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=5` reached.\n","output_type":"stream"},{"name":"stdout","text":"0: แต่ปางหลังยังมีกรุงกษัตริย์\tไม่ฟังไม่สิงหัดจานประสู้ชีพระชาษ\nเสื้อผลึกอิงที่พระจะทุการ\tคอยบาทชีหนีมาดิ่งฉันทุกท่างหนา\nนางเป่าประคองพาลบไป\tจะรถามมย่อยู่ว\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"### Q11: What's the perplexity (from validation loss) on the last step?","metadata":{"id":"3fcy3dcGc2RM"}},{"cell_type":"code","source":"torch.e ** (1.0575)","metadata":{"id":"qnHTZjgJc5yo","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:54:51.148737Z","iopub.execute_input":"2025-01-26T12:54:51.148953Z","iopub.status.idle":"2025-01-26T12:54:51.154065Z","shell.execute_reply.started":"2025-01-26T12:54:51.148935Z","shell.execute_reply":"2025-01-26T12:54:51.153459Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"2.8791640741922344"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"### Q12: What's the output from the generated text?","metadata":{"id":"LUcvTGYMc6ee"}},{"cell_type":"code","source":"L.pytorch.seed_everything(40)\nbeginning_text = tokenizer('<|startoftext|>'+\"อาจารย์พีรพลสอนเอ็นแอลพี\", return_tensors=\"pt\")\noutput = distilgpt2.model.generate(beginning_text['input_ids'], attention_mask=beginning_text[\"attention_mask\"],pad_token_id=tokenizer.pad_token_id, do_sample=True, max_length =500, num_return_sequences=1)\nprint(tokenizer.decode(output[0], skip_special_tokens=True).strip())","metadata":{"id":"eh6UNBw6c8TO","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T12:54:51.154969Z","iopub.execute_input":"2025-01-26T12:54:51.155284Z","iopub.status.idle":"2025-01-26T12:55:00.891121Z","shell.execute_reply.started":"2025-01-26T12:54:51.155256Z","shell.execute_reply":"2025-01-26T12:55:00.889767Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 40\n","output_type":"stream"},{"name":"stdout","text":"อาจารย์พีรพลสอนเอ็นแอลพี\tจะได้อนใจก็เป็นการะสรวลหา\nค่อยล้อยตอบขขึ้นสมุทร\tพวกโรธีทักหักจะครบประคองไปถาม\nชีที่ให้สว่าฆ่าพรูงธูที่ม\tมีต่างได้ประณตบอกตัว\nนี่อีกรรมใจบีรับสาคเรศ\tทูลปองระตบิดานขวาง\nยิงื้นเชื่อสิ้นไม่ไม่เว้นว่า\tจะเรียนหีกเกลือดดื่อนการ\nยิ้มยิ้ม�\n","output_type":"stream"}],"execution_count":36}]}