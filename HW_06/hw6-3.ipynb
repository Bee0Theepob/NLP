{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HOMEWORK 6: TEXT CLASSIFICATION\nIn this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n\nWe will focus only on the Object Classification task for this homework.\n\nIn this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n\nYou will need to build 3 different models.\n\n1. A model based on tf-idf\n2. A model based on MUSE\n3. A model based on wangchanBERTa\n\n**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n\nThis homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification.","metadata":{"id":"VQ8FRFIYMc5X"}},{"cell_type":"code","source":"!wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-16T14:37:34.325580Z","iopub.execute_input":"2025-02-16T14:37:34.325882Z","iopub.status.idle":"2025-02-16T14:37:35.642372Z","shell.execute_reply.started":"2025-02-16T14:37:34.325849Z","shell.execute_reply":"2025-02-16T14:37:35.641327Z"},"id":"kHqkFSyaNvOt","outputId":"879b17f1-0fb2-455c-ca37-b5a4aecd7b1c","trusted":true},"outputs":[{"name":"stdout","text":"--2025-02-16 14:37:34--  https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6 [following]\n--2025-02-16 14:37:34--  https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc90742f59e24f661667f3e58aeb.dl.dropboxusercontent.com/cd/0/inline/CkNrC2192978YX4Dk420galHP6X731ZLsVP0fJIw8zrku969sm-Z4CEkNmmxQqdYfFDwLAqDIaSaDzlsg7Eh17cY96e0YXsaiDwd69il0vWpLro3nu8oaCsWC98nUJRiKoQ/file# [following]\n--2025-02-16 14:37:35--  https://uc90742f59e24f661667f3e58aeb.dl.dropboxusercontent.com/cd/0/inline/CkNrC2192978YX4Dk420galHP6X731ZLsVP0fJIw8zrku969sm-Z4CEkNmmxQqdYfFDwLAqDIaSaDzlsg7Eh17cY96e0YXsaiDwd69il0vWpLro3nu8oaCsWC98nUJRiKoQ/file\nResolving uc90742f59e24f661667f3e58aeb.dl.dropboxusercontent.com (uc90742f59e24f661667f3e58aeb.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\nConnecting to uc90742f59e24f661667f3e58aeb.dl.dropboxusercontent.com (uc90742f59e24f661667f3e58aeb.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2518977 (2.4M) [text/plain]\nSaving to: ‘clean-phone-data-for-students.csv’\n\nclean-phone-data-fo 100%[===================>]   2.40M  --.-KB/s    in 0.05s   \n\n2025-02-16 14:37:35 (47.8 MB/s) - ‘clean-phone-data-for-students.csv’ saved [2518977/2518977]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pythainlp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRlx5Mb5zkXw","outputId":"18d913e0-aa6d-435b-931d-591386cb4ba8","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:37:35.643883Z","iopub.execute_input":"2025-02-16T14:37:35.644223Z","iopub.status.idle":"2025-02-16T14:37:41.263172Z","shell.execute_reply.started":"2025-02-16T14:37:35.644186Z","shell.execute_reply":"2025-02-16T14:37:41.262083Z"}},"outputs":[{"name":"stdout","text":"Collecting pythainlp\n  Downloading pythainlp-5.0.5-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2025.1.31)\nDownloading pythainlp-5.0.5-py3-none-any.whl (17.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pythainlp\nSuccessfully installed pythainlp-5.0.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Import Libs","metadata":{"id":"2YprqbOPMc5a"}},{"cell_type":"code","source":"%matplotlib inline\nimport pandas\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom torch.utils.data import Dataset\nfrom IPython.display import display\nfrom collections import defaultdict\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom pythainlp.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom pythainlp.corpus.common import thai_stopwords\nimport time\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"id":"heICP79cMc5e","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:37:41.265232Z","iopub.execute_input":"2025-02-16T14:37:41.265566Z","iopub.status.idle":"2025-02-16T14:38:05.550321Z","shell.execute_reply.started":"2025-02-16T14:37:41.265538Z","shell.execute_reply":"2025-02-16T14:38:05.549621Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Loading data\nFirst, we load the data from disk into a Dataframe.\n\nA Dataframe is essentially a table, or 2D-array/Matrix with a name for each column.","metadata":{"id":"GPaUf4PLMc5k"}},{"cell_type":"code","source":"data_df = pd.read_csv('clean-phone-data-for-students.csv')","metadata":{"id":"JhZ2eBAWMc5l","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.551476Z","iopub.execute_input":"2025-02-16T14:38:05.552071Z","iopub.status.idle":"2025-02-16T14:38:05.612858Z","shell.execute_reply.started":"2025-02-16T14:38:05.552046Z","shell.execute_reply":"2025-02-16T14:38:05.612164Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Let's preview the data.","metadata":{"id":"cje3yruTMc5p"}},{"cell_type":"code","source":"# Show the top 5 rows\ndisplay(data_df.head())\n# Summarize the data\ndata_df.describe()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"id":"aNqRNz1PMc5q","outputId":"e129a502-1420-476c-dc50-46c293a01b56","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.613620Z","iopub.execute_input":"2025-02-16T14:38:05.613950Z","iopub.status.idle":"2025-02-16T14:38:05.659105Z","shell.execute_reply.started":"2025-02-16T14:38:05.613917Z","shell.execute_reply":"2025-02-16T14:38:05.658397Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                  Sentence Utterance   Action        Object\n0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence Utterance</th>\n      <th>Action</th>\n      <th>Object</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n      <td>enquire</td>\n      <td>payment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n      <td>enquire</td>\n      <td>package</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n      <td>report</td>\n      <td>suspend</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n      <td>enquire</td>\n      <td>internet</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n      <td>report</td>\n      <td>phone_issues</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       Sentence Utterance   Action   Object\ncount               16175    16175    16175\nunique              13389       10       33\ntop           บริการอื่นๆ  enquire  service\nfreq                   97    10377     2525","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence Utterance</th>\n      <th>Action</th>\n      <th>Object</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>16175</td>\n      <td>16175</td>\n      <td>16175</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>13389</td>\n      <td>10</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>บริการอื่นๆ</td>\n      <td>enquire</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>97</td>\n      <td>10377</td>\n      <td>2525</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Data cleaning\n\nWe call the DataFrame.describe() again.\nNotice that there are 33 unique labels/classes for object and 10 unique labels for action that the model will try to predict.\nBut there are unwanted duplications e.g. Idd,idd,lotalty_card,Lotalty_card\n\nAlso note that, there are 13389 unqiue sentence utterances from 16175 utterances. You have to clean that too!\n\n## #TODO 0.1:\nYou will have to remove unwanted label duplications as well as duplications in text inputs.\nAlso, you will have to trim out unwanted whitespaces from the text inputs.\nThis shouldn't be too hard, as you have already seen it in the demo.\n\n","metadata":{"id":"jGd8BNvMMc5y"}},{"cell_type":"code","source":"display(data_df.describe())\ndisplay(data_df.Object.unique())\ndisplay(data_df.Action.unique())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"id":"V0bGLblVMc5z","outputId":"1a65aff5-6196-4674-fb5d-36aa1afcfdba","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.659994Z","iopub.execute_input":"2025-02-16T14:38:05.660333Z","iopub.status.idle":"2025-02-16T14:38:05.688042Z","shell.execute_reply.started":"2025-02-16T14:38:05.660299Z","shell.execute_reply":"2025-02-16T14:38:05.687381Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"       Sentence Utterance   Action   Object\ncount               16175    16175    16175\nunique              13389       10       33\ntop           บริการอื่นๆ  enquire  service\nfreq                   97    10377     2525","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence Utterance</th>\n      <th>Action</th>\n      <th>Object</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>16175</td>\n      <td>16175</td>\n      <td>16175</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>13389</td>\n      <td>10</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>บริการอื่นๆ</td>\n      <td>enquire</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>97</td>\n      <td>10377</td>\n      <td>2525</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n       'information', 'lost_stolen', 'balance_minutes', 'idd',\n       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n       'Loyalty_card'], dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array(['enquire', 'report', 'cancel', 'Enquire', 'buy', 'activate',\n       'request', 'Report', 'garbage', 'change'], dtype=object)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.688791Z","iopub.execute_input":"2025-02-16T14:38:05.688994Z","iopub.status.idle":"2025-02-16T14:38:05.693563Z","shell.execute_reply.started":"2025-02-16T14:38:05.688975Z","shell.execute_reply":"2025-02-16T14:38:05.692929Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['Sentence Utterance', 'Action', 'Object'], dtype='object')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"start = time.time()\ncols = [\"Sentence Utterance\", \"Object\"]\ndata_df = data_df[cols]\ndata_df.columns = [\"input\", \"raw_label\"]\n\ndata_df[\"clean_label\"]=data_df[\"raw_label\"].str.lower().copy()\ndata_df.drop(\"raw_label\", axis=1, inplace=True)\n\ndata_df[\"input\"] = data_df[\"input\"].str.strip()\n\ndata_df = data_df.drop_duplicates(subset=['input'], keep='first')","metadata":{"id":"19onNNUZMc54","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.694342Z","iopub.execute_input":"2025-02-16T14:38:05.694574Z","iopub.status.idle":"2025-02-16T14:38:05.724415Z","shell.execute_reply.started":"2025-02-16T14:38:05.694544Z","shell.execute_reply":"2025-02-16T14:38:05.723616Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"display(data_df[\"clean_label\"].unique())\ndisplay(data_df.describe())\ndisplay(data_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.726734Z","iopub.execute_input":"2025-02-16T14:38:05.726952Z","iopub.status.idle":"2025-02-16T14:38:05.759216Z","shell.execute_reply.started":"2025-02-16T14:38:05.726933Z","shell.execute_reply":"2025-02-16T14:38:05.758597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n      dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                       input clean_label\ncount                                  13367       13367\nunique                                 13367          26\ntop     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ     service\nfreq                                       1        2108","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>clean_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>13367</td>\n      <td>13367</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>13367</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>2108</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                               input   clean_label\n0  <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counter...       payment\n1               internet ยังความเร็วอยุ่เท่าไหร ครับ       package\n2  ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ       suspend\n3  พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อง...      internet\n4  ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโท...  phone_issues","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>clean_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counter...</td>\n      <td>payment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n      <td>package</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ</td>\n      <td>suspend</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อง...</td>\n      <td>internet</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโท...</td>\n      <td>phone_issues</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Split data into train, valdation, and test sets (normally the ratio will be 80:10:10 , respectively). We recommend to use train_test_spilt from scikit-learn to split the data into train, validation, test set.\n\nIn addition, it should split the data that distribution of the labels in train, validation, test set are similar. There is **stratify** option to handle this issue.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n\nMake sure the same data splitting is used for all models.","metadata":{"id":"BIxnPRiAmrhN"}},{"cell_type":"code","source":"data_x = np.array(list(data_df[\"input\"]))\ndata_y_tmp = np.array(list(data_df[\"clean_label\"]))\ndata_y = []\n\nmap_label_num = {y.strip():i for i,y in enumerate(list(data_df[\"clean_label\"].unique()))}\nmap_num_label = {i:y.strip() for i,y in enumerate(list(data_df[\"clean_label\"].unique()))}\n\nfor i in range(len(data_y_tmp)):\n    data_y.append(int(map_label_num[data_y_tmp[i]])) \ndata_y = np.array(data_y)\nprint(len(data_y))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.760337Z","iopub.execute_input":"2025-02-16T14:38:05.760573Z","iopub.status.idle":"2025-02-16T14:38:05.802417Z","shell.execute_reply.started":"2025-02-16T14:38:05.760554Z","shell.execute_reply":"2025-02-16T14:38:05.801571Z"}},"outputs":[{"name":"stdout","text":"13367\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"unique, counts = np.unique(data_y, return_counts=True)\nvalid_classes = unique[counts >= 10]\nvalid_indices = np.isin(data_y, valid_classes)\ndata_x,data_y  = data_x[valid_indices],data_y[valid_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.803276Z","iopub.execute_input":"2025-02-16T14:38:05.803533Z","iopub.status.idle":"2025-02-16T14:38:05.832047Z","shell.execute_reply.started":"2025-02-16T14:38:05.803506Z","shell.execute_reply":"2025-02-16T14:38:05.831198Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X_train, X_temp, y_train, y_temp = train_test_split(data_x, data_y, test_size=0.20, stratify=data_y, random_state=69)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=69)\n\nprint(\"Train size:\", len(X_train))\nprint(\"Validation size:\", len(X_val))\nprint(\"Test size:\",len(X_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.832809Z","iopub.execute_input":"2025-02-16T14:38:05.833005Z","iopub.status.idle":"2025-02-16T14:38:05.871919Z","shell.execute_reply.started":"2025-02-16T14:38:05.832988Z","shell.execute_reply":"2025-02-16T14:38:05.871095Z"}},"outputs":[{"name":"stdout","text":"Train size: 10690\nValidation size: 1336\nTest size: 1337\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Model 3 WangchanBERTa\n\nWe ask you to train a WangchanBERTa-based model.\n\nWe recommend you use the thaixtransformers fork (which we used in the PoS homework).\nhttps://github.com/PyThaiNLP/thaixtransformers\n\nThe structure of the code will be very similar to the PoS homework. You will also find the huggingface [tutorial](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) useful. Or you can also add a softmax layer by yourself just like in the previous homework.\n\nWhich WangchanBERTa model will you use? Why? (Don't forget to clean your text accordingly).\n\n**Ans:**\n","metadata":{"id":"ZDHfX377rnp_"}},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:05.872858Z","iopub.execute_input":"2025-02-16T14:38:05.873198Z","iopub.status.idle":"2025-02-16T14:38:09.314094Z","shell.execute_reply.started":"2025-02-16T14:38:05.873165Z","shell.execute_reply":"2025-02-16T14:38:09.312818Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.28.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb \nuser_secrets = UserSecretsClient()\n\nmy_secret = user_secrets.get_secret(\"wandb_api_key\") \n\nwandb.login(key=my_secret)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:09.315367Z","iopub.execute_input":"2025-02-16T14:38:09.315717Z","iopub.status.idle":"2025-02-16T14:38:17.523824Z","shell.execute_reply.started":"2025-02-16T14:38:09.315683Z","shell.execute_reply":"2025-02-16T14:38:17.523066Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtheepob\u001b[0m (\u001b[33mtheepob-chulalongkorn-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load tokenizer and model\nmodel_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(y_train)))\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_train_enc = label_encoder.fit_transform(y_train)\ny_val_enc = label_encoder.transform(y_val)\ny_test_enc = label_encoder.transform(y_test)\n\n# Tokenize data\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntrain_data = Dataset.from_dict({\"text\": X_train, \"label\": y_train_enc}).map(tokenize_function, batched=True)\nval_data = Dataset.from_dict({\"text\": X_val, \"label\": y_val_enc}).map(tokenize_function, batched=True)\ntest_data = Dataset.from_dict({\"text\": X_test, \"label\": y_test_enc}).map(tokenize_function, batched=True)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",   # Keep output directory for saving checkpoints\n    run_name=\"wangchanberta_classification\",  # Set a different name for W&B\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    load_best_model_at_end=True\n)\n\n# Trainer\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)  # Get the predicted class\n    acc = accuracy_score(labels, predictions)  # Compute accuracy\n    return {\"accuracy\": acc}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    compute_metrics=compute_metrics  # Add the metrics function here\n)\n\n\n# Train model\n\ntrainer.train()\nend = time.time()","metadata":{"id":"ZI8SvILyub0m","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:38:17.524854Z","iopub.execute_input":"2025-02-16T14:38:17.525555Z","iopub.status.idle":"2025-02-16T14:45:30.785822Z","shell.execute_reply.started":"2025-02-16T14:38:17.525529Z","shell.execute_reply":"2025-02-16T14:45:30.785083Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/282 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4753054e8bb48cebf9835b9c00f13d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf6100374ff428ab46096448da8b553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/905k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2dac8d527d944d2bfbdeb57237848a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/423M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d277429342e24adc9eb2127e574b562e"}},"metadata":{}},{"name":"stderr","text":"Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10690 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5417899a90364e9bb5e7c8170b82e3e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1336 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bce8f2fad7e54e0189adbaee48fbdb85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1337 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ab927ff3db400abced459582ab0bba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250216_143824-el1ujmek</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/theepob-chulalongkorn-university/huggingface/runs/el1ujmek' target=\"_blank\">wangchanberta_classification</a></strong> to <a href='https://wandb.ai/theepob-chulalongkorn-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/theepob-chulalongkorn-university/huggingface' target=\"_blank\">https://wandb.ai/theepob-chulalongkorn-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/theepob-chulalongkorn-university/huggingface/runs/el1ujmek' target=\"_blank\">https://wandb.ai/theepob-chulalongkorn-university/huggingface/runs/el1ujmek</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2007' max='2007' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2007/2007 06:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.703100</td>\n      <td>2.676998</td>\n      <td>0.157934</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.371300</td>\n      <td>2.219441</td>\n      <td>0.315868</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.830100</td>\n      <td>1.829563</td>\n      <td>0.456587</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"train_results = trainer.evaluate(train_data)\nval_results = trainer.evaluate(val_data)\ntest_results = trainer.evaluate(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:45:30.786626Z","iopub.execute_input":"2025-02-16T14:45:30.786933Z","iopub.status.idle":"2025-02-16T14:46:18.557120Z","shell.execute_reply.started":"2025-02-16T14:45:30.786897Z","shell.execute_reply":"2025-02-16T14:46:18.556410Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='837' max='669' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [669/669 00:47]\n    </div>\n    "},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"print(f\"Training Time: {end - start:.4f} seconds\")\nprint(f\"Train Accuracy: {train_results['eval_accuracy']:.4f}\")\nprint(f\"Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\nprint(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:46:18.558040Z","iopub.execute_input":"2025-02-16T14:46:18.558342Z","iopub.status.idle":"2025-02-16T14:46:18.566328Z","shell.execute_reply.started":"2025-02-16T14:46:18.558300Z","shell.execute_reply":"2025-02-16T14:46:18.565549Z"}},"outputs":[{"name":"stdout","text":"Training Time: 445.0767 seconds\nTrain Accuracy: 0.4707\nValidation Accuracy: 0.4566\nTest Accuracy: 0.4480\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Comparison\n\nAfter you have completed the 3 models, compare the accuracy, ease of implementation, and inference speed (from cleaning, tokenization, till model compute) between the three models in mycourseville.","metadata":{"id":"Qr9_0DnMBcFZ"}},{"cell_type":"markdown","source":"### Model1\n- Time: 8.4621 seconds\n- Train Accuracy: 0.7650\n- Validation Accuracy: 0.6939\n- Test Accuracy: 0.6971\n### Model 2\n- Time: 31.8394 seconds\n- Train Accuracy: 0.7351\n- Validation Accuracy: 0.7118\n- Test Accuracy: 0.7023\n### Model 3\n- Time: 445.0767 seconds\n- Train Accuracy: 0.8536\n- Validation Accuracy: 0.7732\n- Test Accuracy: 0.7644\n\n## ANS \nWangchanBERTa ดีที่สุด เพราะ มีaccuracyสูงสุดและเรากำลังทำCallCenterChatbotซึ่งไม่จำเป็นต้องเร็วมากนั้น","metadata":{}}]}