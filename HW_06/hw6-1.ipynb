{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HOMEWORK 6: TEXT CLASSIFICATION\nIn this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n\nWe will focus only on the Object Classification task for this homework.\n\nIn this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n\nYou will need to build 3 different models.\n\n1. A model based on tf-idf\n2. A model based on MUSE\n3. A model based on wangchanBERTa\n\n**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n\nThis homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification.","metadata":{"id":"VQ8FRFIYMc5X"}},{"cell_type":"code","source":"!wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv","metadata":{"id":"kHqkFSyaNvOt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"879b17f1-0fb2-455c-ca37-b5a4aecd7b1c","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:24.980010Z","iopub.execute_input":"2025-02-16T14:31:24.980224Z","iopub.status.idle":"2025-02-16T14:31:26.290489Z","shell.execute_reply.started":"2025-02-16T14:31:24.980202Z","shell.execute_reply":"2025-02-16T14:31:26.289422Z"}},"outputs":[{"name":"stdout","text":"--2025-02-16 14:31:25--  https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6 [following]\n--2025-02-16 14:31:25--  https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uce9b4d9ce07540704f730162084.dl.dropboxusercontent.com/cd/0/inline/CkNwY6dA0mIUMFAmX-HKzPI5krHfWCSD1PL4FmlWgrPqx-9wx0E1EOYeYbzGwZxsq54tbIl55OtjnCAqKzphbFIpKUyLGINc8t5FPYeeCrdtalrPlTbDFuEmG5BT2KPOlb8/file# [following]\n--2025-02-16 14:31:25--  https://uce9b4d9ce07540704f730162084.dl.dropboxusercontent.com/cd/0/inline/CkNwY6dA0mIUMFAmX-HKzPI5krHfWCSD1PL4FmlWgrPqx-9wx0E1EOYeYbzGwZxsq54tbIl55OtjnCAqKzphbFIpKUyLGINc8t5FPYeeCrdtalrPlTbDFuEmG5BT2KPOlb8/file\nResolving uce9b4d9ce07540704f730162084.dl.dropboxusercontent.com (uce9b4d9ce07540704f730162084.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\nConnecting to uce9b4d9ce07540704f730162084.dl.dropboxusercontent.com (uce9b4d9ce07540704f730162084.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2518977 (2.4M) [text/plain]\nSaving to: ‘clean-phone-data-for-students.csv’\n\nclean-phone-data-fo 100%[===================>]   2.40M  --.-KB/s    in 0.04s   \n\n2025-02-16 14:31:26 (54.9 MB/s) - ‘clean-phone-data-for-students.csv’ saved [2518977/2518977]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pythainlp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRlx5Mb5zkXw","outputId":"18d913e0-aa6d-435b-931d-591386cb4ba8","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:26.292490Z","iopub.execute_input":"2025-02-16T14:31:26.292828Z","iopub.status.idle":"2025-02-16T14:31:31.701022Z","shell.execute_reply.started":"2025-02-16T14:31:26.292794Z","shell.execute_reply":"2025-02-16T14:31:31.700204Z"}},"outputs":[{"name":"stdout","text":"Collecting pythainlp\n  Downloading pythainlp-5.0.5-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2025.1.31)\nDownloading pythainlp-5.0.5-py3-none-any.whl (17.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pythainlp\nSuccessfully installed pythainlp-5.0.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Import Libs","metadata":{"id":"2YprqbOPMc5a"}},{"cell_type":"code","source":"%matplotlib inline\nimport pandas\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom torch.utils.data import Dataset\nfrom IPython.display import display\nfrom collections import defaultdict\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom pythainlp.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom pythainlp.corpus.common import thai_stopwords\nimport time","metadata":{"id":"heICP79cMc5e","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:31.702188Z","iopub.execute_input":"2025-02-16T14:31:31.702519Z","iopub.status.idle":"2025-02-16T14:31:38.417012Z","shell.execute_reply.started":"2025-02-16T14:31:31.702494Z","shell.execute_reply":"2025-02-16T14:31:38.416347Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Loading data\nFirst, we load the data from disk into a Dataframe.\n\nA Dataframe is essentially a table, or 2D-array/Matrix with a name for each column.","metadata":{"id":"GPaUf4PLMc5k"}},{"cell_type":"code","source":"data_df = pd.read_csv('clean-phone-data-for-students.csv')","metadata":{"id":"JhZ2eBAWMc5l","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.417954Z","iopub.execute_input":"2025-02-16T14:31:38.418468Z","iopub.status.idle":"2025-02-16T14:31:38.479828Z","shell.execute_reply.started":"2025-02-16T14:31:38.418434Z","shell.execute_reply":"2025-02-16T14:31:38.479071Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Let's preview the data.","metadata":{"id":"cje3yruTMc5p"}},{"cell_type":"code","source":"# Show the top 5 rows\ndisplay(data_df.head())\n# Summarize the data\ndata_df.describe()","metadata":{"id":"aNqRNz1PMc5q","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"e129a502-1420-476c-dc50-46c293a01b56","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.480688Z","iopub.execute_input":"2025-02-16T14:31:38.480966Z","iopub.status.idle":"2025-02-16T14:31:38.525805Z","shell.execute_reply.started":"2025-02-16T14:31:38.480935Z","shell.execute_reply":"2025-02-16T14:31:38.524911Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                  Sentence Utterance   Action        Object\n0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence Utterance</th>\n      <th>Action</th>\n      <th>Object</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n      <td>enquire</td>\n      <td>payment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n      <td>enquire</td>\n      <td>package</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n      <td>report</td>\n      <td>suspend</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n      <td>enquire</td>\n      <td>internet</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n      <td>report</td>\n      <td>phone_issues</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       Sentence Utterance   Action   Object\ncount               16175    16175    16175\nunique              13389       10       33\ntop           บริการอื่นๆ  enquire  service\nfreq                   97    10377     2525","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence Utterance</th>\n      <th>Action</th>\n      <th>Object</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>16175</td>\n      <td>16175</td>\n      <td>16175</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>13389</td>\n      <td>10</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>บริการอื่นๆ</td>\n      <td>enquire</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>97</td>\n      <td>10377</td>\n      <td>2525</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Data cleaning\n\nWe call the DataFrame.describe() again.\nNotice that there are 33 unique labels/classes for object and 10 unique labels for action that the model will try to predict.\nBut there are unwanted duplications e.g. Idd,idd,lotalty_card,Lotalty_card\n\nAlso note that, there are 13389 unqiue sentence utterances from 16175 utterances. You have to clean that too!\n\n## #TODO 0.1:\nYou will have to remove unwanted label duplications as well as duplications in text inputs.\nAlso, you will have to trim out unwanted whitespaces from the text inputs.\nThis shouldn't be too hard, as you have already seen it in the demo.\n\n","metadata":{"id":"jGd8BNvMMc5y"}},{"cell_type":"code","source":"display(data_df.describe())\ndisplay(data_df.Object.unique())\ndisplay(data_df.Action.unique())","metadata":{"id":"V0bGLblVMc5z","colab":{"base_uri":"https://localhost:8080/","height":331},"outputId":"1a65aff5-6196-4674-fb5d-36aa1afcfdba","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.528170Z","iopub.execute_input":"2025-02-16T14:31:38.528432Z","iopub.status.idle":"2025-02-16T14:31:38.553349Z","shell.execute_reply.started":"2025-02-16T14:31:38.528410Z","shell.execute_reply":"2025-02-16T14:31:38.552730Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"       Sentence Utterance   Action   Object\ncount               16175    16175    16175\nunique              13389       10       33\ntop           บริการอื่นๆ  enquire  service\nfreq                   97    10377     2525","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence Utterance</th>\n      <th>Action</th>\n      <th>Object</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>16175</td>\n      <td>16175</td>\n      <td>16175</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>13389</td>\n      <td>10</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>บริการอื่นๆ</td>\n      <td>enquire</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>97</td>\n      <td>10377</td>\n      <td>2525</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n       'information', 'lost_stolen', 'balance_minutes', 'idd',\n       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n       'Loyalty_card'], dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array(['enquire', 'report', 'cancel', 'Enquire', 'buy', 'activate',\n       'request', 'Report', 'garbage', 'change'], dtype=object)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.554289Z","iopub.execute_input":"2025-02-16T14:31:38.554497Z","iopub.status.idle":"2025-02-16T14:31:38.559023Z","shell.execute_reply.started":"2025-02-16T14:31:38.554478Z","shell.execute_reply":"2025-02-16T14:31:38.558428Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Index(['Sentence Utterance', 'Action', 'Object'], dtype='object')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"start = time.time()\ncols = [\"Sentence Utterance\", \"Object\"]\ndata_df = data_df[cols]\ndata_df.columns = [\"input\", \"raw_label\"]\n\ndata_df[\"clean_label\"]=data_df[\"raw_label\"].str.lower().copy()\ndata_df.drop(\"raw_label\", axis=1, inplace=True)\n\ndata_df[\"input\"] = data_df[\"input\"].str.strip()\n\ndata_df = data_df.drop_duplicates(subset=['input'], keep='first')","metadata":{"id":"19onNNUZMc54","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.559718Z","iopub.execute_input":"2025-02-16T14:31:38.559903Z","iopub.status.idle":"2025-02-16T14:31:38.587479Z","shell.execute_reply.started":"2025-02-16T14:31:38.559886Z","shell.execute_reply":"2025-02-16T14:31:38.586663Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"display(data_df[\"clean_label\"].unique())\ndisplay(data_df.describe())\ndisplay(data_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.588329Z","iopub.execute_input":"2025-02-16T14:31:38.588603Z","iopub.status.idle":"2025-02-16T14:31:38.618473Z","shell.execute_reply.started":"2025-02-16T14:31:38.588581Z","shell.execute_reply":"2025-02-16T14:31:38.617794Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n      dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                       input clean_label\ncount                                  13367       13367\nunique                                 13367          26\ntop     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ     service\nfreq                                       1        2108","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>clean_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>13367</td>\n      <td>13367</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>13367</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n      <td>service</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>2108</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                               input   clean_label\n0  <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counter...       payment\n1               internet ยังความเร็วอยุ่เท่าไหร ครับ       package\n2  ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ       suspend\n3  พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อง...      internet\n4  ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโท...  phone_issues","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>clean_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counter...</td>\n      <td>payment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n      <td>package</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ</td>\n      <td>suspend</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อง...</td>\n      <td>internet</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโท...</td>\n      <td>phone_issues</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Split data into train, valdation, and test sets (normally the ratio will be 80:10:10 , respectively). We recommend to use train_test_spilt from scikit-learn to split the data into train, validation, test set.\n\nIn addition, it should split the data that distribution of the labels in train, validation, test set are similar. There is **stratify** option to handle this issue.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n\nMake sure the same data splitting is used for all models.","metadata":{"id":"BIxnPRiAmrhN"}},{"cell_type":"code","source":"data_x = np.array(list(data_df[\"input\"]))\ndata_y_tmp = np.array(list(data_df[\"clean_label\"]))\ndata_y = []\n\nmap_label_num = {y.strip():i for i,y in enumerate(list(data_df[\"clean_label\"].unique()))}\nmap_num_label = {i:y.strip() for i,y in enumerate(list(data_df[\"clean_label\"].unique()))}\n\nfor i in range(len(data_y_tmp)):\n    data_y.append(int(map_label_num[data_y_tmp[i]])) \ndata_y = np.array(data_y)\nprint(len(data_y))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.619176Z","iopub.execute_input":"2025-02-16T14:31:38.619442Z","iopub.status.idle":"2025-02-16T14:31:38.659579Z","shell.execute_reply.started":"2025-02-16T14:31:38.619410Z","shell.execute_reply":"2025-02-16T14:31:38.658920Z"}},"outputs":[{"name":"stdout","text":"13367\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"unique, counts = np.unique(data_y, return_counts=True)\nvalid_classes = unique[counts >= 10]\nvalid_indices = np.isin(data_y, valid_classes)\ndata_x,data_y  = data_x[valid_indices],data_y[valid_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.660301Z","iopub.execute_input":"2025-02-16T14:31:38.660604Z","iopub.status.idle":"2025-02-16T14:31:38.688771Z","shell.execute_reply.started":"2025-02-16T14:31:38.660573Z","shell.execute_reply":"2025-02-16T14:31:38.688141Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X_train, X_temp, y_train, y_temp = train_test_split(data_x, data_y, test_size=0.20, stratify=data_y, random_state=69)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=69)\n\nprint(\"Train size:\", len(X_train))\nprint(\"Validation size:\", len(X_val))\nprint(\"Test size:\",len(X_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.689425Z","iopub.execute_input":"2025-02-16T14:31:38.689620Z","iopub.status.idle":"2025-02-16T14:31:38.727465Z","shell.execute_reply.started":"2025-02-16T14:31:38.689601Z","shell.execute_reply":"2025-02-16T14:31:38.726836Z"}},"outputs":[{"name":"stdout","text":"Train size: 10690\nValidation size: 1336\nTest size: 1337\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"#Model 1 TF-IDF\n\nBuild a model to train a tf-idf text classifier. Use a simple logistic regression model for the classifier.\n\nFor this part, you may find this [tutorial](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) helpful.\n\nBelow are some design choices you need to consider to accomplish this task. Be sure to answer them when you submit your model.\n\nWhat tokenizer will you use? Why?\n\n**Ans:**\n\nWill you ignore some stop words (a, an, the, to, etc. for English) in your tf-idf? Is it important?\nPythaiNLP provides a list of stopwords if you want to use (https://pythainlp.org/docs/2.0/api/corpus.html#pythainlp.corpus.common.thai_stopwords)\n\n**Ans:**\n\nThe dictionary of TF-IDF is usually based on the training data. How many words in the test set are OOVs?\n\n**Ans:**","metadata":{"id":"Nx6gllzrnVVU"}},{"cell_type":"code","source":"def thai_tokenizer(text):\n    return word_tokenize(text, keep_whitespace=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.728189Z","iopub.execute_input":"2025-02-16T14:31:38.728509Z","iopub.status.idle":"2025-02-16T14:31:38.732365Z","shell.execute_reply.started":"2025-02-16T14:31:38.728478Z","shell.execute_reply":"2025-02-16T14:31:38.731501Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom pythainlp.corpus.common import thai_stopwords\nimport numpy as np\nimport time\n\n# Define stopwords\nstopwords = list(thai_stopwords())\n\n# Initialize TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(\n    tokenizer=None,  # Use default tokenization\n    stop_words=stopwords,  # Remove Thai stopwords\n    max_features=10000  # Limit vocabulary size to avoid overfitting\n)\n\n# Create a logistic regression pipeline\nmodel = Pipeline([\n    (\"tfidf\", vectorizer),\n    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# Train model\nstart = time.time()\nmodel.fit(X_train, y_train)\nend = time.time()\n# Evaluate model\nprint(f\"Training time: {end - start:.4f} seconds\")\ntrain_acc = model.score(X_train, y_train)\nval_acc = model.score(X_val, y_val)\ntest_acc = model.score(X_test, y_test)\n\nprint(f\"Train Accuracy: {train_acc:.4f}\")\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# Check OOV words (words in test set not seen in train set)\ntrain_vocab = set(vectorizer.get_feature_names_out())\ntest_vocab = set(word for sentence in X_test for word in sentence.split())\noov_words = test_vocab - train_vocab\nprint(f\"Number of OOV words: {len(oov_words)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:38.733158Z","iopub.execute_input":"2025-02-16T14:31:38.733443Z","iopub.status.idle":"2025-02-16T14:31:44.100136Z","shell.execute_reply.started":"2025-02-16T14:31:38.733422Z","shell.execute_reply":"2025-02-16T14:31:44.099238Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['กคน', 'กคร', 'กครา', 'กคราว', 'กจะ', 'กช', 'กต', 'กท', 'กทาง', 'กน', 'กระท', 'กระน', 'กระไร', 'กล', 'กว', 'กส', 'กหน', 'กอ', 'กอย', 'กำล', 'กเม', 'กแห', 'กๆ', 'ขณะท', 'ขณะน', 'ขณะหน', 'ขณะเด', 'คงอย', 'คร', 'ครบคร', 'ครบถ', 'คราท', 'คราน', 'คราวก', 'คราวท', 'คราวน', 'คราวหน', 'คราวหล', 'คราวโน', 'คราหน', 'คล', 'งก', 'งกระน', 'งกล', 'งกว', 'งข', 'งคง', 'งคน', 'งครา', 'งคราว', 'งง', 'งจ', 'งจน', 'งจะ', 'งจาก', 'งต', 'งท', 'งน', 'งบ', 'งปวง', 'งมวล', 'งละ', 'งว', 'งส', 'งหน', 'งหมด', 'งหมาย', 'งหล', 'งหลาย', 'งอย', 'งเก', 'งเคย', 'งเน', 'งเป', 'งเม', 'งแก', 'งแต', 'งแม', 'งแล', 'งโง', 'งโน', 'งใด', 'งใหญ', 'งไง', 'งได', 'งไหน', 'งๆ', 'งๆจ', 'จก', 'จจ', 'จนกระท', 'จนกว', 'จนขณะน', 'จนถ', 'จนท', 'จนบ', 'จนเม', 'จนแม', 'จร', 'จรดก', 'จวนเจ', 'จวบก', 'จส', 'จสมบ', 'จะได', 'จากน', 'จำเป', 'จแล', 'ฉะน', 'ซะก', 'ซะจนกระท', 'ซะจนถ', 'ณๆ', 'ดการ', 'ดงาน', 'ดดล', 'ดต', 'ดทำ', 'ดน', 'ดว', 'ดหน', 'ดหา', 'ดเด', 'ดเผย', 'ดแจง', 'ดให', 'ดไป', 'ดๆ', 'ตลอดถ', 'ตลอดท', 'ตลอดป', 'ตลอดว', 'ตามด', 'ตามท', 'ตามแต', 'ทว', 'ทำให', 'นก', 'นการ', 'นกาลนาน', 'นควร', 'นจะ', 'นด', 'นต', 'นท', 'นน', 'นนะ', 'นนาน', 'นมา', 'นมาก', 'นย', 'นยง', 'นยาว', 'นละ', 'นว', 'นวาน', 'นอ', 'นอกจากท', 'นอกจากน', 'นอกจากว', 'นอกน', 'นอกเหน', 'นอาท', 'นา', 'นเคย', 'นเด', 'นเถอะ', 'นเน', 'นเป', 'นเพ', 'นเพราะ', 'นเพราะว', 'นเม', 'นเอง', 'นแก', 'นแต', 'นและก', 'นแหละ', 'นใด', 'นใดน', 'นไง', 'นได', 'นไป', 'นไร', 'นไว', 'นไหน', 'นไหม', 'นๆ', 'บจากน', 'บต', 'บรอง', 'บว', 'บอกว', 'บอกแล', 'บางกว', 'บางคร', 'บางท', 'บางแห', 'บเน', 'บแต', 'ปฏ', 'ปร', 'ประการฉะน', 'ประการหน', 'ปรากฏว', 'พบว', 'พร', 'พวกก', 'พวกค', 'พวกฉ', 'พวกท', 'พวกน', 'พวกม', 'พวกโน', 'พอก', 'พอด', 'พอต', 'พอท', 'พอเพ', 'พอแล', 'ภายภาคหน', 'ภายหน', 'ภายหล', 'ภายใต', 'มก', 'มองว', 'มากกว', 'มเต', 'มไปด', 'มไปหมด', 'มๆ', 'ยกให', 'ยง', 'ยงพอ', 'ยงว', 'ยงเพ', 'ยงเพราะ', 'ยงแค', 'ยงแต', 'ยงใด', 'ยงไร', 'ยงไหน', 'ยจน', 'ยจนกระท', 'ยจนถ', 'ยด', 'ยน', 'ยนะ', 'ยนแปลง', 'ยบ', 'ยย', 'ยล', 'ยว', 'ยวก', 'ยวข', 'ยวน', 'ยวเน', 'ยวๆ', 'ยอมร', 'ยเน', 'ยเอง', 'ยแล', 'ยโน', 'รณ', 'รวดเร', 'รวมก', 'รวมด', 'รวมถ', 'รวมท', 'ระหว', 'วก', 'วง', 'วงก', 'วงต', 'วงถ', 'วงท', 'วงน', 'วงระหว', 'วงหน', 'วงหล', 'วงแรก', 'วงๆ', 'วถ', 'วท', 'วน', 'วนจน', 'วนด', 'วนท', 'วนน', 'วนมาก', 'วนเก', 'วนแต', 'วนใด', 'วนใหญ', 'วม', 'วมก', 'วมด', 'วมม', 'วย', 'วยก', 'วยท', 'วยประการฉะน', 'วยว', 'วยเช', 'วยเพราะ', 'วยเหต', 'วยเหม', 'วเสร', 'วแต', 'วๆ', 'สม', 'สำค', 'หมดก', 'หมดส', 'หร', 'หล', 'หากว', 'หากแม', 'หาร', 'หาใช', 'อก', 'อค', 'อคร', 'อคราว', 'อคราวก', 'อคราวท', 'อง', 'องจาก', 'องมาจาก', 'อจะ', 'อจาก', 'อด', 'อถ', 'อท', 'อน', 'อนก', 'อนข', 'อนมาทาง', 'อนว', 'อนหน', 'อนๆ', 'อบ', 'อบจะ', 'อบๆ', 'อม', 'อมก', 'อมด', 'อมท', 'อมเพ', 'อย', 'อยกว', 'อยคร', 'อยจะ', 'อยเป', 'อยไปทาง', 'อยๆ', 'อว', 'อวาน', 'อาจเป', 'อเก', 'อเช', 'อเปล', 'อเม', 'อเย', 'อใด', 'อให', 'อไง', 'อไป', 'อไม', 'อไร', 'อไหร', 'าก', 'าง', 'างก', 'างขวาง', 'างจะ', 'างด', 'างต', 'างท', 'างน', 'างบน', 'างมาก', 'างย', 'างล', 'างละ', 'างหน', 'างหาก', 'างเค', 'างเช', 'างเด', 'างโน', 'างใด', 'างไร', 'างไรก', 'างไรเส', 'างไหน', 'างๆ', 'าจะ', 'าท', 'าน', 'านาน', 'านๆ', 'าพเจ', 'าย', 'ายก', 'ายว', 'ายใด', 'ายๆ', 'าว', 'าวค', 'าส', 'าหร', 'าหาก', 'าฯ', 'าใจ', 'าใด', 'าให', 'าไร', 'าไหร', 'าๆ', 'เก', 'เข', 'เฉกเช', 'เช', 'เด', 'เต', 'เถ', 'เท', 'เน', 'เป', 'เปล', 'เผ', 'เพ', 'เพราะฉะน', 'เพราะว', 'เม', 'เร', 'เล', 'เส', 'เสม', 'เสร', 'เห', 'เหต', 'เหล', 'เอ', 'แค', 'แด', 'แต', 'แท', 'แน', 'แม', 'แล', 'แสดงว', 'แห', 'แหล', 'ใกล', 'ใช', 'ใด', 'ใต', 'ในช', 'ในท', 'ในระหว', 'ในเม', 'ให', 'ใหญ', 'ใหม', 'ไข', 'ได', 'ไม', 'ไว', 'ไหม'] not in stop_words.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training time: 5.1646 seconds\nTrain Accuracy: 0.7709\nValidation Accuracy: 0.6235\nTest Accuracy: 0.6335\nNumber of OOV words: 2798\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom pythainlp.corpus.common import thai_stopwords\nimport numpy as np\nimport time\n# Define stopwords\nstopwords = list(thai_stopwords())\n\n# Initialize TF-IDF Vectorizer\nvectorizer = TfidfVectorizer(\n    tokenizer=thai_tokenizer,  # Use default tokenization\n    stop_words=stopwords,  # Remove Thai stopwords\n    max_features=10000  # Limit vocabulary size to avoid overfitting\n)\n\n# Create a logistic regression pipeline\nmodel = Pipeline([\n    (\"tfidf\", vectorizer),\n    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# Train model\n\nmodel.fit(X_train, y_train)\nend = time.time()\n\n# Evaluate model\ntrain_acc = model.score(X_train, y_train)\nval_acc = model.score(X_val, y_val)\ntest_acc = model.score(X_test, y_test)\nprint(f\"Training time: {end - start:.4f} seconds\")\nprint(f\"Train Accuracy: {train_acc:.4f}\")\nprint(f\"Validation Accuracy: {val_acc:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# Check OOV words (words in test set not seen in train set)\ntrain_vocab = set(vectorizer.get_feature_names_out())\ntest_vocab = set(word for sentence in X_test for word in sentence.split())\noov_words = test_vocab - train_vocab\nprint(f\"Number of OOV words: {len(oov_words)}\")\n","metadata":{"id":"9vOqTqmfufsT","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T14:31:44.101255Z","iopub.execute_input":"2025-02-16T14:31:44.101596Z","iopub.status.idle":"2025-02-16T14:31:48.895372Z","shell.execute_reply.started":"2025-02-16T14:31:44.101559Z","shell.execute_reply":"2025-02-16T14:31:48.894504Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['กระไร', 'กาลนาน', 'ชิ้น', 'ดังที่', 'ดี', 'ดีกว่า', 'ด้อย', 'ตัว', 'ต่อไป', 'ถัดไป', 'ทั่วถึง', 'ทำ', 'ที่จะ', 'ท่าน', 'ท้าย', 'นา', 'บอ', 'บัด', 'ระยะเวลา', 'ล่ะ', 'วันวาน', 'สม', 'สมบูรณ์', 'สํา', 'หน้า', 'หรับ', 'หา', 'อย', 'เกี่ยว', 'เก่า', 'เดี๋ยวนี้', 'เย็น', 'เล่า', 'เสมือน', 'เหมือนกัน', 'แด่', 'แม้น', 'แหล่', 'โง้น', 'โน้น', 'ใด', 'ไว', 'ไหม', '\\ufeff'] not in stop_words.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training time: 8.4621 seconds\nTrain Accuracy: 0.7650\nValidation Accuracy: 0.6939\nTest Accuracy: 0.6971\nNumber of OOV words: 2694\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"1.What tokenizer will you use? Why?\n\n**Ans:**\npythainlp.word_tokenize เพราะ เชื่อ ว่าออกแบบมาเพื่อภาษาไทยโดยเฉพาะ ดังที่เห็นว่าaccuracy ของ validation,test สูงกว่า\n\n2.Will you ignore some stop words (a, an, the, to, etc. for English) in your tf-idf? Is it important?\nPythaiNLP provides a list of stopwords if you want to use (https://pythainlp.org/docs/2.0/api/corpus.html#pythainlp.corpus.common.thai_stopwords)\n\n**Ans:**\nใช่ ใช้pythainlp.thai_stopwords() เพราะมันเป็นคำที่ไม่สื่อความหมายอะไรอยู่แล้ว\n\n3.The dictionary of TF-IDF is usually based on the training data. How many words in the test set are OOVs?\n\n**Ans:**\n2694","metadata":{}}]}