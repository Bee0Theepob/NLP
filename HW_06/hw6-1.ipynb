{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ8FRFIYMc5X"
   },
   "source": [
    "# HOMEWORK 6: TEXT CLASSIFICATION\n",
    "In this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n",
    "1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n",
    "2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n",
    "\n",
    "We will focus only on the Object Classification task for this homework.\n",
    "\n",
    "In this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n",
    "\n",
    "You will need to build 3 different models.\n",
    "\n",
    "1. A model based on tf-idf\n",
    "2. A model based on MUSE\n",
    "3. A model based on wangchanBERTa\n",
    "\n",
    "**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n",
    "\n",
    "This homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:18.652058Z",
     "iopub.status.busy": "2025-02-14T14:20:18.651770Z",
     "iopub.status.idle": "2025-02-14T14:20:20.067822Z",
     "shell.execute_reply": "2025-02-14T14:20:20.066940Z",
     "shell.execute_reply.started": "2025-02-14T14:20:18.652038Z"
    },
    "id": "kHqkFSyaNvOt",
    "outputId": "879b17f1-0fb2-455c-ca37-b5a4aecd7b1c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-14 14:20:18--  https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.66.18, 2620:100:6022:18::a27d:4212\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.66.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6 [following]\n",
      "--2025-02-14 14:20:18--  https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uccc63f7805c29a229279cb3a957.dl.dropboxusercontent.com/cd/0/inline/CkFihJzRlWn7Gfe-dFq-QEKjqULHVvdtego467dU0CcDolaVwlGXo74DoUKfqqGSU-7rawx5pLyHwF3GEByDVsziO0rMzE26KDaIuk_w1OvaHc_4-dg9apaDNG9OBvrE6Io/file# [following]\n",
      "--2025-02-14 14:20:19--  https://uccc63f7805c29a229279cb3a957.dl.dropboxusercontent.com/cd/0/inline/CkFihJzRlWn7Gfe-dFq-QEKjqULHVvdtego467dU0CcDolaVwlGXo74DoUKfqqGSU-7rawx5pLyHwF3GEByDVsziO0rMzE26KDaIuk_w1OvaHc_4-dg9apaDNG9OBvrE6Io/file\n",
      "Resolving uccc63f7805c29a229279cb3a957.dl.dropboxusercontent.com (uccc63f7805c29a229279cb3a957.dl.dropboxusercontent.com)... 162.125.66.15, 2620:100:6022:15::a27d:420f\n",
      "Connecting to uccc63f7805c29a229279cb3a957.dl.dropboxusercontent.com (uccc63f7805c29a229279cb3a957.dl.dropboxusercontent.com)|162.125.66.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2518977 (2.4M) [text/plain]\n",
      "Saving to: ‘clean-phone-data-for-students.csv.1’\n",
      "\n",
      "clean-phone-data-fo 100%[===================>]   2.40M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-02-14 14:20:19 (34.8 MB/s) - ‘clean-phone-data-for-students.csv.1’ saved [2518977/2518977]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.069569Z",
     "iopub.status.busy": "2025-02-14T14:20:20.069242Z",
     "iopub.status.idle": "2025-02-14T14:20:20.072910Z",
     "shell.execute_reply": "2025-02-14T14:20:20.072172Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.069542Z"
    },
    "id": "qRlx5Mb5zkXw",
    "outputId": "18d913e0-aa6d-435b-931d-591386cb4ba8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pythainlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YprqbOPMc5a"
   },
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.075067Z",
     "iopub.status.busy": "2025-02-14T14:20:20.074847Z",
     "iopub.status.idle": "2025-02-14T14:20:20.087950Z",
     "shell.execute_reply": "2025-02-14T14:20:20.087165Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.075049Z"
    },
    "id": "heICP79cMc5e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPaUf4PLMc5k"
   },
   "source": [
    "## Loading data\n",
    "First, we load the data from disk into a Dataframe.\n",
    "\n",
    "A Dataframe is essentially a table, or 2D-array/Matrix with a name for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.089380Z",
     "iopub.status.busy": "2025-02-14T14:20:20.089148Z",
     "iopub.status.idle": "2025-02-14T14:20:20.158863Z",
     "shell.execute_reply": "2025-02-14T14:20:20.158268Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.089362Z"
    },
    "id": "JhZ2eBAWMc5l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('clean-phone-data-for-students.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cje3yruTMc5p"
   },
   "source": [
    "Let's preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.159815Z",
     "iopub.status.busy": "2025-02-14T14:20:20.159597Z",
     "iopub.status.idle": "2025-02-14T14:20:20.187472Z",
     "shell.execute_reply": "2025-02-14T14:20:20.186653Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.159796Z"
    },
    "id": "aNqRNz1PMc5q",
    "outputId": "e129a502-1420-476c-dc50-46c293a01b56",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n",
       "      <td>enquire</td>\n",
       "      <td>payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n",
       "      <td>report</td>\n",
       "      <td>suspend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n",
       "      <td>enquire</td>\n",
       "      <td>internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n",
       "      <td>report</td>\n",
       "      <td>phone_issues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Sentence Utterance   Action        Object\n",
       "0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n",
       "1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n",
       "2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n",
       "3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n",
       "4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>10377</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Utterance   Action   Object\n",
       "count               16175    16175    16175\n",
       "unique              13389       10       33\n",
       "top           บริการอื่นๆ  enquire  service\n",
       "freq                   97    10377     2525"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top 5 rows\n",
    "display(data_df.head())\n",
    "# Summarize the data\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGd8BNvMMc5y"
   },
   "source": [
    "## Data cleaning\n",
    "\n",
    "We call the DataFrame.describe() again.\n",
    "Notice that there are 33 unique labels/classes for object and 10 unique labels for action that the model will try to predict.\n",
    "But there are unwanted duplications e.g. Idd,idd,lotalty_card,Lotalty_card\n",
    "\n",
    "Also note that, there are 13389 unqiue sentence utterances from 16175 utterances. You have to clean that too!\n",
    "\n",
    "## #TODO 0.1:\n",
    "You will have to remove unwanted label duplications as well as duplications in text inputs.\n",
    "Also, you will have to trim out unwanted whitespaces from the text inputs.\n",
    "This shouldn't be too hard, as you have already seen it in the demo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.188728Z",
     "iopub.status.busy": "2025-02-14T14:20:20.188433Z",
     "iopub.status.idle": "2025-02-14T14:20:20.212055Z",
     "shell.execute_reply": "2025-02-14T14:20:20.211272Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.188699Z"
    },
    "id": "V0bGLblVMc5z",
    "outputId": "1a65aff5-6196-4674-fb5d-36aa1afcfdba",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>10377</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Utterance   Action   Object\n",
       "count               16175    16175    16175\n",
       "unique              13389       10       33\n",
       "top           บริการอื่นๆ  enquire  service\n",
       "freq                   97    10377     2525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
       "       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n",
       "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
       "       'information', 'lost_stolen', 'balance_minutes', 'idd',\n",
       "       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n",
       "       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n",
       "       'Loyalty_card'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['enquire', 'report', 'cancel', 'Enquire', 'buy', 'activate',\n",
       "       'request', 'Report', 'garbage', 'change'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_df.describe())\n",
    "display(data_df.Object.unique())\n",
    "display(data_df.Action.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.213206Z",
     "iopub.status.busy": "2025-02-14T14:20:20.212918Z",
     "iopub.status.idle": "2025-02-14T14:20:20.217946Z",
     "shell.execute_reply": "2025-02-14T14:20:20.217188Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.213186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentence Utterance', 'Action', 'Object'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.219198Z",
     "iopub.status.busy": "2025-02-14T14:20:20.218890Z",
     "iopub.status.idle": "2025-02-14T14:20:20.243128Z",
     "shell.execute_reply": "2025-02-14T14:20:20.242325Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.219171Z"
    },
    "id": "19onNNUZMc54",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cols = [\"Sentence Utterance\", \"Object\"]\n",
    "data_df = data_df[cols]\n",
    "data_df.columns = [\"input\", \"raw_label\"]\n",
    "\n",
    "data_df[\"clean_label\"]=data_df[\"raw_label\"].str.lower().copy()\n",
    "data_df.drop(\"raw_label\", axis=1, inplace=True)\n",
    "\n",
    "data_df[\"input\"] = data_df[\"input\"].str.strip()\n",
    "\n",
    "data_df = data_df.drop_duplicates(subset=['input'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.245493Z",
     "iopub.status.busy": "2025-02-14T14:20:20.245268Z",
     "iopub.status.idle": "2025-02-14T14:20:20.268525Z",
     "shell.execute_reply": "2025-02-14T14:20:20.267765Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.245475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
       "       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n",
       "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
       "       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n",
       "       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>clean_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13367</td>\n",
       "      <td>13367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13367</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       input clean_label\n",
       "count                                  13367       13367\n",
       "unique                                 13367          26\n",
       "top     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ     service\n",
       "freq                                       1        2108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>clean_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counter...</td>\n",
       "      <td>payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n",
       "      <td>package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ</td>\n",
       "      <td>suspend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อง...</td>\n",
       "      <td>internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโท...</td>\n",
       "      <td>phone_issues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input   clean_label\n",
       "0  <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counter...       payment\n",
       "1               internet ยังความเร็วอยุ่เท่าไหร ครับ       package\n",
       "2  ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ       suspend\n",
       "3  พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อง...      internet\n",
       "4  ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโท...  phone_issues"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_df[\"clean_label\"].unique())\n",
    "display(data_df.describe())\n",
    "display(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIxnPRiAmrhN"
   },
   "source": [
    "Split data into train, valdation, and test sets (normally the ratio will be 80:10:10 , respectively). We recommend to use train_test_spilt from scikit-learn to split the data into train, validation, test set.\n",
    "\n",
    "In addition, it should split the data that distribution of the labels in train, validation, test set are similar. There is **stratify** option to handle this issue.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Make sure the same data splitting is used for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.269925Z",
     "iopub.status.busy": "2025-02-14T14:20:20.269602Z",
     "iopub.status.idle": "2025-02-14T14:20:20.311653Z",
     "shell.execute_reply": "2025-02-14T14:20:20.310978Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.269895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13367\n"
     ]
    }
   ],
   "source": [
    "data_x = np.array(list(data_df[\"input\"]))\n",
    "data_y_tmp = np.array(list(data_df[\"clean_label\"]))\n",
    "data_y = []\n",
    "\n",
    "map_label_num = {y.strip():i for i,y in enumerate(list(data_df[\"clean_label\"].unique()))}\n",
    "map_num_label = {i:y.strip() for i,y in enumerate(list(data_df[\"clean_label\"].unique()))}\n",
    "\n",
    "for i in range(len(data_y_tmp)):\n",
    "    data_y.append(int(map_label_num[data_y_tmp[i]])) \n",
    "data_y = np.array(data_y)\n",
    "print(len(data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.312748Z",
     "iopub.status.busy": "2025-02-14T14:20:20.312441Z",
     "iopub.status.idle": "2025-02-14T14:20:20.338246Z",
     "shell.execute_reply": "2025-02-14T14:20:20.337631Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.312719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(data_y, return_counts=True)\n",
    "valid_classes = unique[counts >= 10]\n",
    "valid_indices = np.isin(data_y, valid_classes)\n",
    "data_x,data_y  = data_x[valid_indices],data_y[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.339233Z",
     "iopub.status.busy": "2025-02-14T14:20:20.338992Z",
     "iopub.status.idle": "2025-02-14T14:20:20.375485Z",
     "shell.execute_reply": "2025-02-14T14:20:20.374834Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.339204Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10690\n",
      "Validation size: 1336\n",
      "Test size: 1337\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(data_x, data_y, test_size=0.20, stratify=data_y, random_state=69)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=69)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx6gllzrnVVU"
   },
   "source": [
    "#Model 1 TF-IDF\n",
    "\n",
    "Build a model to train a tf-idf text classifier. Use a simple logistic regression model for the classifier.\n",
    "\n",
    "For this part, you may find this [tutorial](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) helpful.\n",
    "\n",
    "Below are some design choices you need to consider to accomplish this task. Be sure to answer them when you submit your model.\n",
    "\n",
    "What tokenizer will you use? Why?\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "Will you ignore some stop words (a, an, the, to, etc. for English) in your tf-idf? Is it important?\n",
    "PythaiNLP provides a list of stopwords if you want to use (https://pythainlp.org/docs/2.0/api/corpus.html#pythainlp.corpus.common.thai_stopwords)\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "The dictionary of TF-IDF is usually based on the training data. How many words in the test set are OOVs?\n",
    "\n",
    "**Ans:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.376447Z",
     "iopub.status.busy": "2025-02-14T14:20:20.376234Z",
     "iopub.status.idle": "2025-02-14T14:20:20.379859Z",
     "shell.execute_reply": "2025-02-14T14:20:20.379171Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.376429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def thai_tokenizer(text):\n",
    "    return word_tokenize(text, keep_whitespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:20.380796Z",
     "iopub.status.busy": "2025-02-14T14:20:20.380601Z",
     "iopub.status.idle": "2025-02-14T14:20:25.650304Z",
     "shell.execute_reply": "2025-02-14T14:20:25.649356Z",
     "shell.execute_reply.started": "2025-02-14T14:20:20.380780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['กคน', 'กคร', 'กครา', 'กคราว', 'กจะ', 'กช', 'กต', 'กท', 'กทาง', 'กน', 'กระท', 'กระน', 'กระไร', 'กล', 'กว', 'กส', 'กหน', 'กอ', 'กอย', 'กำล', 'กเม', 'กแห', 'กๆ', 'ขณะท', 'ขณะน', 'ขณะหน', 'ขณะเด', 'คงอย', 'คร', 'ครบคร', 'ครบถ', 'คราท', 'คราน', 'คราวก', 'คราวท', 'คราวน', 'คราวหน', 'คราวหล', 'คราวโน', 'คราหน', 'คล', 'งก', 'งกระน', 'งกล', 'งกว', 'งข', 'งคง', 'งคน', 'งครา', 'งคราว', 'งง', 'งจ', 'งจน', 'งจะ', 'งจาก', 'งต', 'งท', 'งน', 'งบ', 'งปวง', 'งมวล', 'งละ', 'งว', 'งส', 'งหน', 'งหมด', 'งหมาย', 'งหล', 'งหลาย', 'งอย', 'งเก', 'งเคย', 'งเน', 'งเป', 'งเม', 'งแก', 'งแต', 'งแม', 'งแล', 'งโง', 'งโน', 'งใด', 'งใหญ', 'งไง', 'งได', 'งไหน', 'งๆ', 'งๆจ', 'จก', 'จจ', 'จนกระท', 'จนกว', 'จนขณะน', 'จนถ', 'จนท', 'จนบ', 'จนเม', 'จนแม', 'จร', 'จรดก', 'จวนเจ', 'จวบก', 'จส', 'จสมบ', 'จะได', 'จากน', 'จำเป', 'จแล', 'ฉะน', 'ซะก', 'ซะจนกระท', 'ซะจนถ', 'ณๆ', 'ดการ', 'ดงาน', 'ดดล', 'ดต', 'ดทำ', 'ดน', 'ดว', 'ดหน', 'ดหา', 'ดเด', 'ดเผย', 'ดแจง', 'ดให', 'ดไป', 'ดๆ', 'ตลอดถ', 'ตลอดท', 'ตลอดป', 'ตลอดว', 'ตามด', 'ตามท', 'ตามแต', 'ทว', 'ทำให', 'นก', 'นการ', 'นกาลนาน', 'นควร', 'นจะ', 'นด', 'นต', 'นท', 'นน', 'นนะ', 'นนาน', 'นมา', 'นมาก', 'นย', 'นยง', 'นยาว', 'นละ', 'นว', 'นวาน', 'นอ', 'นอกจากท', 'นอกจากน', 'นอกจากว', 'นอกน', 'นอกเหน', 'นอาท', 'นา', 'นเคย', 'นเด', 'นเถอะ', 'นเน', 'นเป', 'นเพ', 'นเพราะ', 'นเพราะว', 'นเม', 'นเอง', 'นแก', 'นแต', 'นและก', 'นแหละ', 'นใด', 'นใดน', 'นไง', 'นได', 'นไป', 'นไร', 'นไว', 'นไหน', 'นไหม', 'นๆ', 'บจากน', 'บต', 'บรอง', 'บว', 'บอกว', 'บอกแล', 'บางกว', 'บางคร', 'บางท', 'บางแห', 'บเน', 'บแต', 'ปฏ', 'ปร', 'ประการฉะน', 'ประการหน', 'ปรากฏว', 'พบว', 'พร', 'พวกก', 'พวกค', 'พวกฉ', 'พวกท', 'พวกน', 'พวกม', 'พวกโน', 'พอก', 'พอด', 'พอต', 'พอท', 'พอเพ', 'พอแล', 'ภายภาคหน', 'ภายหน', 'ภายหล', 'ภายใต', 'มก', 'มองว', 'มากกว', 'มเต', 'มไปด', 'มไปหมด', 'มๆ', 'ยกให', 'ยง', 'ยงพอ', 'ยงว', 'ยงเพ', 'ยงเพราะ', 'ยงแค', 'ยงแต', 'ยงใด', 'ยงไร', 'ยงไหน', 'ยจน', 'ยจนกระท', 'ยจนถ', 'ยด', 'ยน', 'ยนะ', 'ยนแปลง', 'ยบ', 'ยย', 'ยล', 'ยว', 'ยวก', 'ยวข', 'ยวน', 'ยวเน', 'ยวๆ', 'ยอมร', 'ยเน', 'ยเอง', 'ยแล', 'ยโน', 'รณ', 'รวดเร', 'รวมก', 'รวมด', 'รวมถ', 'รวมท', 'ระหว', 'วก', 'วง', 'วงก', 'วงต', 'วงถ', 'วงท', 'วงน', 'วงระหว', 'วงหน', 'วงหล', 'วงแรก', 'วงๆ', 'วถ', 'วท', 'วน', 'วนจน', 'วนด', 'วนท', 'วนน', 'วนมาก', 'วนเก', 'วนแต', 'วนใด', 'วนใหญ', 'วม', 'วมก', 'วมด', 'วมม', 'วย', 'วยก', 'วยท', 'วยประการฉะน', 'วยว', 'วยเช', 'วยเพราะ', 'วยเหต', 'วยเหม', 'วเสร', 'วแต', 'วๆ', 'สม', 'สำค', 'หมดก', 'หมดส', 'หร', 'หล', 'หากว', 'หากแม', 'หาร', 'หาใช', 'อก', 'อค', 'อคร', 'อคราว', 'อคราวก', 'อคราวท', 'อง', 'องจาก', 'องมาจาก', 'อจะ', 'อจาก', 'อด', 'อถ', 'อท', 'อน', 'อนก', 'อนข', 'อนมาทาง', 'อนว', 'อนหน', 'อนๆ', 'อบ', 'อบจะ', 'อบๆ', 'อม', 'อมก', 'อมด', 'อมท', 'อมเพ', 'อย', 'อยกว', 'อยคร', 'อยจะ', 'อยเป', 'อยไปทาง', 'อยๆ', 'อว', 'อวาน', 'อาจเป', 'อเก', 'อเช', 'อเปล', 'อเม', 'อเย', 'อใด', 'อให', 'อไง', 'อไป', 'อไม', 'อไร', 'อไหร', 'าก', 'าง', 'างก', 'างขวาง', 'างจะ', 'างด', 'างต', 'างท', 'างน', 'างบน', 'างมาก', 'างย', 'างล', 'างละ', 'างหน', 'างหาก', 'างเค', 'างเช', 'างเด', 'างโน', 'างใด', 'างไร', 'างไรก', 'างไรเส', 'างไหน', 'างๆ', 'าจะ', 'าท', 'าน', 'านาน', 'านๆ', 'าพเจ', 'าย', 'ายก', 'ายว', 'ายใด', 'ายๆ', 'าว', 'าวค', 'าส', 'าหร', 'าหาก', 'าฯ', 'าใจ', 'าใด', 'าให', 'าไร', 'าไหร', 'าๆ', 'เก', 'เข', 'เฉกเช', 'เช', 'เด', 'เต', 'เถ', 'เท', 'เน', 'เป', 'เปล', 'เผ', 'เพ', 'เพราะฉะน', 'เพราะว', 'เม', 'เร', 'เล', 'เส', 'เสม', 'เสร', 'เห', 'เหต', 'เหล', 'เอ', 'แค', 'แด', 'แต', 'แท', 'แน', 'แม', 'แล', 'แสดงว', 'แห', 'แหล', 'ใกล', 'ใช', 'ใด', 'ใต', 'ในช', 'ในท', 'ในระหว', 'ในเม', 'ให', 'ใหญ', 'ใหม', 'ไข', 'ได', 'ไม', 'ไว', 'ไหม'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.0701 seconds\n",
      "Train Accuracy: 0.7709\n",
      "Validation Accuracy: 0.6235\n",
      "Test Accuracy: 0.6335\n",
      "Number of OOV words: 2798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define stopwords\n",
    "stopwords = list(thai_stopwords())\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=None,  # Use default tokenization\n",
    "    stop_words=stopwords,  # Remove Thai stopwords\n",
    "    max_features=10000  # Limit vocabulary size to avoid overfitting\n",
    ")\n",
    "\n",
    "# Create a logistic regression pipeline\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", vectorizer),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "# Evaluate model\n",
    "print(f\"Training time: {end - start:.4f} seconds\")\n",
    "train_acc = model.score(X_train, y_train)\n",
    "val_acc = model.score(X_val, y_val)\n",
    "test_acc = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Check OOV words (words in test set not seen in train set)\n",
    "train_vocab = set(vectorizer.get_feature_names_out())\n",
    "test_vocab = set(word for sentence in X_test for word in sentence.split())\n",
    "oov_words = test_vocab - train_vocab\n",
    "print(f\"Number of OOV words: {len(oov_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T14:20:25.651451Z",
     "iopub.status.busy": "2025-02-14T14:20:25.651218Z",
     "iopub.status.idle": "2025-02-14T14:20:30.426888Z",
     "shell.execute_reply": "2025-02-14T14:20:30.425938Z",
     "shell.execute_reply.started": "2025-02-14T14:20:25.651431Z"
    },
    "id": "9vOqTqmfufsT",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['กระไร', 'กาลนาน', 'ชิ้น', 'ดังที่', 'ดี', 'ดีกว่า', 'ด้อย', 'ตัว', 'ต่อไป', 'ถัดไป', 'ทั่วถึง', 'ทำ', 'ที่จะ', 'ท่าน', 'ท้าย', 'นา', 'บอ', 'บัด', 'ระยะเวลา', 'ล่ะ', 'วันวาน', 'สม', 'สมบูรณ์', 'สํา', 'หน้า', 'หรับ', 'หา', 'อย', 'เกี่ยว', 'เก่า', 'เดี๋ยวนี้', 'เย็น', 'เล่า', 'เสมือน', 'เหมือนกัน', 'แด่', 'แม้น', 'แหล่', 'โง้น', 'โน้น', 'ใด', 'ไว', 'ไหม', '\\ufeff'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.0621 seconds\n",
      "Train Accuracy: 0.7650\n",
      "Validation Accuracy: 0.6939\n",
      "Test Accuracy: 0.6971\n",
      "Number of OOV words: 2694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "import numpy as np\n",
    "import time\n",
    "# Define stopwords\n",
    "stopwords = list(thai_stopwords())\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=thai_tokenizer,  # Use default tokenization\n",
    "    stop_words=stopwords,  # Remove Thai stopwords\n",
    "    max_features=10000  # Limit vocabulary size to avoid overfitting\n",
    ")\n",
    "\n",
    "# Create a logistic regression pipeline\n",
    "model = Pipeline([\n",
    "    (\"tfidf\", vectorizer),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "# Evaluate model\n",
    "train_acc = model.score(X_train, y_train)\n",
    "val_acc = model.score(X_val, y_val)\n",
    "test_acc = model.score(X_test, y_test)\n",
    "print(f\"Training time: {end - start:.4f} seconds\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Check OOV words (words in test set not seen in train set)\n",
    "train_vocab = set(vectorizer.get_feature_names_out())\n",
    "test_vocab = set(word for sentence in X_test for word in sentence.split())\n",
    "oov_words = test_vocab - train_vocab\n",
    "print(f\"Number of OOV words: {len(oov_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.What tokenizer will you use? Why?\n",
    "\n",
    "**Ans:**\n",
    "pythainlp.word_tokenize เพราะ เชื่อ ว่าออกแบบมาเพื่อภาษาไทยโดยเฉพาะ ดังที่เห็นว่าaccuracy ของ validation,test สูงกว่า\n",
    "\n",
    "2.Will you ignore some stop words (a, an, the, to, etc. for English) in your tf-idf? Is it important?\n",
    "PythaiNLP provides a list of stopwords if you want to use (https://pythainlp.org/docs/2.0/api/corpus.html#pythainlp.corpus.common.thai_stopwords)\n",
    "\n",
    "**Ans:**\n",
    "ใช่ ใช้pythainlp.thai_stopwords() เพราะมันเป็นคำที่ไม่สื่อความหมายอะไรอยู่แล้ว\n",
    "\n",
    "3.The dictionary of TF-IDF is usually based on the training data. How many words in the test set are OOVs?\n",
    "\n",
    "**Ans:**\n",
    "2694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wql2YeU8qFQ6"
   },
   "source": [
    "# Model 2 MUSE\n",
    "\n",
    "Build a simple logistic regression model using features from the MUSE model.\n",
    "\n",
    "Which MUSE model will you use? Why?\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "MUSE is typically used with tensorflow. However, there are some pytorch conversions made by some people.\n",
    "\n",
    "https://huggingface.co/sentence-transformers/use-cmlm-multilingual\n",
    "https://huggingface.co/dayyass/universal-sentence-encoder-multilingual-large-3-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3UtkpaLnctH",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDHfX377rnp_"
   },
   "source": [
    "# Model 3 WangchanBERTa\n",
    "\n",
    "We ask you to train a WangchanBERTa-based model.\n",
    "\n",
    "We recommend you use the thaixtransformers fork (which we used in the PoS homework).\n",
    "https://github.com/PyThaiNLP/thaixtransformers\n",
    "\n",
    "The structure of the code will be very similar to the PoS homework. You will also find the huggingface [tutorial](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) useful. Or you can also add a softmax layer by yourself just like in the previous homework.\n",
    "\n",
    "Which WangchanBERTa model will you use? Why? (Don't forget to clean your text accordingly).\n",
    "\n",
    "**Ans:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI8SvILyub0m",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D7qsVL0BaXS"
   },
   "source": [
    "After you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr9_0DnMBcFZ"
   },
   "source": [
    "# Comparison\n",
    "\n",
    "After you have completed the 3 models, compare the accuracy, ease of implementation, and inference speed (from cleaning, tokenization, till model compute) between the three models in mycourseville."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
