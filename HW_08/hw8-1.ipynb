{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip install transformers datasets peft","metadata":{"id":"YbUOJOIJEiEc","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:02:33.000373Z","iopub.execute_input":"2025-03-24T13:02:33.000726Z","iopub.status.idle":"2025-03-24T13:02:33.004631Z","shell.execute_reply.started":"2025-03-24T13:02:33.000698Z","shell.execute_reply":"2025-03-24T13:02:33.003528Z"}},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"# HW 8: Low Rank Adaptation (LoRA)\n\nIn this assignment, you will learn to implement low-rank adaptation both from scratch and using a libraryâ€”specifically, with PyTorch and the PEFT library, respectively.\n\nThis assignment is divided into two sections:\n\nIn the first section, we introduce the parameter-efficient transfer learning (PET) method. We use LoRA to adapt the GPT2 model for the SST-2 dataset. This section will teach you how LoRA works and how to implement it from scratch using forward_hook.\n\nIn the second section, we introduce the PEFT library, which allows us to perform LoRA easily.","metadata":{"id":"Q_rMUycDEXIc"}},{"cell_type":"markdown","source":"# Part 1: LoRA from Scratch\nWith the discovery of scaling properties in deep learning models, several researchers tend to increase model size to achieve emergent properties, especially in the natural language processing (NLP) field. For example, GPT-3 contains 175 billion parameters, making it nearly impossible to fine-tune on limited resources. This trend prevents students like us from adapting these enormous foundation models on a single GPU (or with small resources).\n\nTo alleviate this problem, researchers have developed new fine-tuning methods, known as parameter-efficient transfer learning, which allow us to train large models with limited resources. The benefits of these methods extend not only to the training process but also to deployment. After fine-tuning, we only need to save a small number of parameters (the LoRA weights), enabling us to deploy the foundation model to various downstream tasks using minimal storage. One of the prevailing methods is Low Rank Adaptation (LoRA).\n\nAnother popular option is prompt tuning, where we only train special tokens that are prepended to the input. However, this is not the focus of this homework.\n\nIn this section, we will introduce Low-Rank Adaptation. You are assigned to implement LoRA on GPT2 model. We will finetune the model to SST-2 dataset using the traditional and LoRA method.","metadata":{"id":"smZEXsKKEyCA"}},{"cell_type":"markdown","source":"## Load Dataset and Model\nIn this step, we will prepare the GPT-2 model and the SST-2 dataset.\n\nSST-2 is a widely used dataset for sentiment analysis, extracted from movie reviews, containing sentences labeled as either positive or negative.","metadata":{"id":"eAPm3OUpHhLF"}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport time\nfrom torch.utils.data import DataLoader\nfrom transformers import GPT2ForSequenceClassification, GPT2TokenizerFast\nfrom datasets import load_dataset\nfrom tqdm.autonotebook import tqdm\nfrom transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n\n# Load the GPT-2 model for sequence classification and its tokenizer\nmodel = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\n# GPT-2 does not have a pad token by default so we set it to the EOS token.\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = tokenizer.eos_token_id\n\n# Load the SST-2 dataset\ntrain_dataset_raw = load_dataset(\"glue\", \"sst2\", split=\"train\")\ntrain_dataset_raw, val_dataset_raw = train_dataset_raw.train_test_split(test_size=0.2).values()\ntest_dataset_raw = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n\n# Preview dataset\nprint(\"Sample sentence:\")\nfor data in test_dataset_raw:\n    print(data)\n    break\n\ndef tokenize_function(example):\n    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntrain_dataset = train_dataset_raw.map(tokenize_function, batched=True)\nval_dataset = val_dataset_raw.map(tokenize_function, batched=True)\ntest_dataset = test_dataset_raw.map(tokenize_function, batched=True)\n\ntrain_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\nval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16)\nval_dataloader = DataLoader(val_dataset, batch_size=16)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)","metadata":{"id":"__O2H4x6Cf4h","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:02:33.005804Z","iopub.execute_input":"2025-03-24T13:02:33.006148Z","iopub.status.idle":"2025-03-24T13:02:52.643732Z","shell.execute_reply.started":"2025-03-24T13:02:33.006124Z","shell.execute_reply":"2025-03-24T13:02:52.642986Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Sample sentence:\n{'sentence': \"it 's a charming and often affecting journey . \", 'label': 1, 'idx': 0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/53879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e089d1ad34a24c478f85b1212e4833ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13470 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f6e7f0553af473ba8dbf41078eb9996"}},"metadata":{}}],"execution_count":76},{"cell_type":"markdown","source":"## Traditional Fine tuning\nIn the traditional fine-tuning method, the entire model is trained, which is computationally expensive. An alternative approach is to fine-tune only certain layers of the model to reduce resource usage while still adapting the model to a specific task.\n\nTo keep the implementation simple, you are assigned to train only the attention weights in the self-attention layers.\n\nThe code below displays the names of all layers in the GPT-2 model. This will help you identify which layers to set as trainable or keep frozen. For more details on the attention layers in GPT-2, please refer to the following link: [GPT-2 Attention Layer Details](https://huggingface.co/transformers/v4.9.2/_modules/transformers/models/gpt2/modeling_gpt2.html).","metadata":{"id":"pTqlSrsePb0T"}},{"cell_type":"code","source":"for name, module in model.named_modules():\n  print(name, type(module))","metadata":{"id":"MKM2IWJ4PBRc","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:02:52.645659Z","iopub.execute_input":"2025-03-24T13:02:52.646035Z","iopub.status.idle":"2025-03-24T13:02:52.718001Z","shell.execute_reply.started":"2025-03-24T13:02:52.646000Z","shell.execute_reply":"2025-03-24T13:02:52.716901Z"}},"outputs":[{"name":"stdout","text":" <class 'transformers.models.gpt2.modeling_gpt2.GPT2ForSequenceClassification'>\ntransformer <class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\ntransformer.wte <class 'torch.nn.modules.sparse.Embedding'>\ntransformer.wpe <class 'torch.nn.modules.sparse.Embedding'>\ntransformer.drop <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h <class 'torch.nn.modules.container.ModuleList'>\ntransformer.h.0 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.0.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.0.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.0.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.0.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.0.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.0.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.0.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.0.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.0.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.0.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.0.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.0.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.1 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.1.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.1.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.1.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.1.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.1.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.1.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.1.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.1.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.1.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.1.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.1.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.1.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.2 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.2.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.2.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.2.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.2.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.2.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.2.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.2.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.2.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.2.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.2.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.2.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.2.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.3 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.3.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.3.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.3.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.3.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.3.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.3.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.3.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.3.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.3.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.3.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.3.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.3.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.4 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.4.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.4.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.4.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.4.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.4.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.4.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.4.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.4.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.4.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.4.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.4.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.4.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.5 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.5.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.5.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.5.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.5.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.5.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.5.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.5.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.5.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.5.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.5.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.5.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.5.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.6 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.6.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.6.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.6.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.6.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.6.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.6.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.6.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.6.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.6.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.6.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.6.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.6.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.7 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.7.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.7.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.7.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.7.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.7.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.7.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.7.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.7.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.7.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.7.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.7.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.7.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.8 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.8.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.8.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.8.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.8.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.8.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.8.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.8.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.8.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.8.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.8.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.8.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.8.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.9 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.9.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.9.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.9.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.9.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.9.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.9.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.9.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.9.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.9.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.9.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.9.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.9.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.10 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.10.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.10.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.10.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.10.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.10.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.10.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.10.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.10.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.10.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.10.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.10.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.10.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.11 <class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\ntransformer.h.11.ln_1 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.11.attn <class 'transformers.models.gpt2.modeling_gpt2.GPT2SdpaAttention'>\ntransformer.h.11.attn.c_attn <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.11.attn.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.11.attn.attn_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.11.attn.resid_dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.h.11.ln_2 <class 'torch.nn.modules.normalization.LayerNorm'>\ntransformer.h.11.mlp <class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\ntransformer.h.11.mlp.c_fc <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.11.mlp.c_proj <class 'transformers.pytorch_utils.Conv1D'>\ntransformer.h.11.mlp.act <class 'transformers.activations.NewGELUActivation'>\ntransformer.h.11.mlp.dropout <class 'torch.nn.modules.dropout.Dropout'>\ntransformer.ln_f <class 'torch.nn.modules.normalization.LayerNorm'>\nscore <class 'torch.nn.modules.linear.Linear'>\n","output_type":"stream"}],"execution_count":77},{"cell_type":"markdown","source":"###TODO 1: Freeze the Model and Train Only Attention Weights\nYou are assigned to freeze the entire model, except for the last two attention weights and the classification head. Note that, in this context, the attention weights do not include the projection layer of the transformer. Instead, they refer only to the weights of the query, key, and value.\n\n**HINT**: `c_proj` is projection layer.","metadata":{"id":"JuhxNv79ctMz"}},{"cell_type":"code","source":"for n, p in model.named_parameters():\n    # TODO 1: freeze every layer except the last two attention weights and classification head.\n    p.requires_grad = False\n\n\nnum_layers = model.config.n_layer\nfor layer_idx in range(num_layers - 2, num_layers):  # Last two layers\n    attn_layer = f\"transformer.h.{layer_idx}.attn.c_attn\"\n    for n, p in model.named_parameters():\n        if attn_layer in n:\n            p.requires_grad = True  # Unfreeze Q, K, V weights\n\n# Unfreeze classification head\nfor n, p in model.named_parameters():\n    if \"score\" in n:  # Classification head\n        p.requires_grad = True\n\n# Verify which parameters are trainable\ntrainable_params = [n for n, p in model.named_parameters() if p.requires_grad]\nprint(\"Trainable parameters:\", trainable_params)\n","metadata":{"id":"ds1GqFA4R5ok","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:02:52.719704Z","iopub.execute_input":"2025-03-24T13:02:52.720115Z","iopub.status.idle":"2025-03-24T13:02:52.787328Z","shell.execute_reply.started":"2025-03-24T13:02:52.720081Z","shell.execute_reply":"2025-03-24T13:02:52.786221Z"}},"outputs":[{"name":"stdout","text":"Trainable parameters: ['transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'score.weight']\n","output_type":"stream"}],"execution_count":78},{"cell_type":"markdown","source":"**Check Your Answer:** The number of learnable parameters is around 3545088.","metadata":{"id":"wlV2DoY0f1sS"}},{"cell_type":"code","source":"pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"Number of Trainable Parameters:\", pytorch_total_params)","metadata":{"id":"vaZ4l7Cmfo2f","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:02:52.788336Z","iopub.execute_input":"2025-03-24T13:02:52.788594Z","iopub.status.idle":"2025-03-24T13:02:52.806309Z","shell.execute_reply.started":"2025-03-24T13:02:52.788572Z","shell.execute_reply":"2025-03-24T13:02:52.805403Z"}},"outputs":[{"name":"stdout","text":"Number of Trainable Parameters: 3545088\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"You are assigned to train the GPT-2 model on the SST-2 dataset. Due to the long training time, you will train the model for only 3 epochs. Your model should have around 86-88% accuracy.","metadata":{"id":"IAbUMyXaiUjS"}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nmodel.to(device)\n\nnum_epochs = 3\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    for batch in tqdm(train_dataloader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"label\"].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = correct / total\n    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")","metadata":{"id":"dWOmFS6QgGaJ","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:02:52.807372Z","iopub.execute_input":"2025-03-24T13:02:52.807690Z","iopub.status.idle":"2025-03-24T13:16:26.896279Z","shell.execute_reply.started":"2025-03-24T13:02:52.807664Z","shell.execute_reply":"2025-03-24T13:16:26.895091Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9a66f01fc904a0db8d4be292970ac6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b67b18e45f4324a66ae6d6c38112d1"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/3 - Accuracy: 0.8567\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f137b605f2d4b248943dafc66c93260"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/3 - Accuracy: 0.8853\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ffef1ec1acf40eb890428173512cfce"}},"metadata":{}},{"name":"stdout","text":"Epoch 3/3 - Accuracy: 0.8876\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"As you can see, fine-tuning in the traditional way takes a long time to complete and also requires a high-computation GPU to fine-tune the entire model. Therefore, it is not feasible for most people.\n\nIn the next part, we will introduce a better method: parameter-efficient learning, which requires lower computation. We will focus on the state-of-the-art method, Low-Rank Adaptation (LoRA).","metadata":{"id":"aCFVbkuajL-g"}},{"cell_type":"markdown","source":"## Low Rank Adaptation\nThe concept of LoRA is that we are going to estimate the gradient (adaptation matrix) with two smaller matrices ($A$ and $B$):\n\n$$\n\\text{Adaptation Matrix} = B \\times A\n$$\n\nwhere $\\text{Adaptation Matrix} \\in \\mathbb{R}^{m \\times n}$, $A \\in \\mathbb{R}^{r \\times n}$, and $B \\in \\mathbb{R}^{m \\times r}$. We could make this approximation based on the assumption that $\\text{Adaptation Matrix}$ has a rank of $r$. Therefore, the fine-tuned weight becomes:\n\n$$\nW = W_0 + \\Delta W\n$$\n$$\n= W_0 + \\frac{\\alpha}{r} BA\n$$\n\nwhere $W$ denotes the fine-tuned weight, $W_0$ represents pre-trained weight, $\\Delta W$ is the gradient and $\\alpha$ can be seen as a learning rate. $A$ is initialized using a common initialization, like Kaiming initialization, during the initialization process. On the other hand, $B$ is set to 0 such that the model's output remains the same after injecting LoRA, resulting in a stabilized training process.\n\nTo summarize, when injecting LoRA into a layer, we insert new parameters called matrix A and B and initialize them using the above description. Then, we modify the forward pass with `forward_hook` such that the output becomes:\n\n$$\nh = W x + \\frac{\\alpha}{r} BA x\n$$\n\nwhere $x$ and $h$ are the input and output, respectively. We recommend you read this [blog](https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/#forward-hooks-101) to learn more about `forward_hook`.\n\n**LoRA on Linear Layer**\n\n- TODO 2: initialize A and B to ones (every entry in the matrix is one), such that we can verify your forward pass after attaching the hook.\n- TODO 3: implement the forward hook such that new output $h$ is\n\n$$\nh = W x + \\frac{\\alpha}{r} BA x\n$$\n\n**Hint**: When you want to declare and initialize a parameter, you can use `torch.nn.Parameter` and `torch.nn.init`, respectively.","metadata":{"id":"xhCP0ZqrpeHO"}},{"cell_type":"code","source":"import math\nimport torch.nn as nn\nimport torch.nn.functional as F\n# Initialize LoRA and attach a hook.\ndef attach_lora(layer, r, lora_alpha, in_features, out_features):\n    assert r > 0, \"rank must greater than 0.\"\n    # TODO 2: Declare A and B matrices and initialize A and B to ones.\n    layer.lora_A = nn.Parameter(torch.ones(r, in_features))\n    layer.lora_B = nn.Parameter(torch.ones(out_features, r))\n\n    def hook(model, input, output):\n        assert len(input) == 1, \"The length of the input must be 1.\"\n        # TODO 3: Compute adapatation matrix (BA) and modify the forward pass.\n        x = input[0]\n        delta_W = (lora_alpha / r) * torch.matmul(layer.lora_B, layer.lora_A)\n\n        output += torch.matmul(x,delta_W.T)\n\n    return hook\n","metadata":{"id":"fI1r0lZqs0A-","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:37:39.033688Z","iopub.execute_input":"2025-03-24T13:37:39.034009Z","iopub.status.idle":"2025-03-24T13:37:39.039327Z","shell.execute_reply.started":"2025-03-24T13:37:39.033987Z","shell.execute_reply":"2025-03-24T13:37:39.038462Z"}},"outputs":[],"execution_count":163},{"cell_type":"markdown","source":"To test your `forward_hook`, we will check the difference of the output before and after injecting the LoRA when you initialize matrices A and B with ones.","metadata":{"id":"-nl2dlr8ttef"}},{"cell_type":"code","source":"from transformers.modeling_utils import Conv1D\n# The Conv1D layer from the Transformer library is actually a linear layer. (https://github.com/huggingface/transformers/blob/main/src/transformers/pytorch_utils.py#L100)\n\nclass DummyLinear(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.linear = Conv1D(20, 10)\n\n  def forward(self, x):\n    return self.linear(x)\n\nr, lora_alpha = 1, 4\ninput_ = torch.arange(10, dtype=torch.float32).unsqueeze(0)\ndummy_linear = DummyLinear()\noutput_before = dummy_linear(input_)\nfor name, module in dummy_linear.named_modules():\n  if isinstance(module, Conv1D):\n    in_features, out_features = module.weight.shape\n    h = module.register_forward_hook(attach_lora(module, r, lora_alpha, in_features, out_features))\noutput_after = dummy_linear(input_)\n\nif torch.all(torch.isclose(output_after - output_before, lora_alpha * input_.sum() * torch.ones_like(output_before))):\n  print(\"Your forward hook seems to be correct.\")\nelse:\n  print(\"There is something wrong with your forward hook.\")","metadata":{"id":"ZW8gI58KjLiw","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:37:40.751868Z","iopub.execute_input":"2025-03-24T13:37:40.752173Z","iopub.status.idle":"2025-03-24T13:37:40.760739Z","shell.execute_reply.started":"2025-03-24T13:37:40.752148Z","shell.execute_reply":"2025-03-24T13:37:40.760004Z"}},"outputs":[{"name":"stdout","text":"Your forward hook seems to be correct.\n","output_type":"stream"}],"execution_count":164},{"cell_type":"markdown","source":"**Instruction**\n\nTODO 4: Change the initialization of A and B where A is initialized with Kaiming Uniform (a = sqrt(5)), and B is set to 0.","metadata":{"id":"lq_ucaXctylG"}},{"cell_type":"code","source":"# Initialize LoRA and attach a hook.\ndef attach_lora(layer, r, lora_alpha, in_features, out_features):\n    assert r > 0, \"rank must greater than 0.\"\n    # TODO 4: initialize A with kaiming uniform with a = sqrt(5) and initialize B to 0.\n    layer.lora_A = nn.Parameter(torch.empty(r, in_features))\n    layer.lora_B = nn.Parameter(torch.zeros(out_features, r))\n\n    nn.init.kaiming_uniform_(layer.lora_A, a=math.sqrt(5))\n    \n    def hook(model, input, output):\n        assert len(input) == 1, \"The length of the input must be 1.\"\n        # Copy from TODO 3\n        x = input[0]\n        delta_W = (lora_alpha / r) * torch.matmul(layer.lora_B, layer.lora_A)\n        \n        output += torch.matmul(x,delta_W)\n\n    return hook","metadata":{"id":"emqf4yyWtyY5","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:38:52.566707Z","iopub.execute_input":"2025-03-24T13:38:52.567005Z","iopub.status.idle":"2025-03-24T13:38:52.572223Z","shell.execute_reply.started":"2025-03-24T13:38:52.566983Z","shell.execute_reply":"2025-03-24T13:38:52.571330Z"}},"outputs":[],"execution_count":172},{"cell_type":"markdown","source":"Similar to TODO 1, You are assigned to inject lora into the last two attention weights.\n\nTODO 5: inject lora into the last two attention weights","metadata":{"id":"8EnhdRMjt_63"}},{"cell_type":"code","source":"r, lora_alpha = 1, 4\ndef attach_lora_to_maskformer(model, r, lora_alpha):\n  hooks = []\n  attention_layers = [name for name, _ in model.named_modules() if \"attn.c_attn\" in name][-2:]\n  for name, module in model.named_modules():\n      # TODO 5: inject lora into the last two attention weights\n      if name in attention_layers:\n            in_features = module.weight.shape[1]\n            out_features = module.weight.shape[0]\n\n            hook = attach_lora(module, r, lora_alpha, in_features, out_features)\n            hooks.append(module.register_forward_hook(hook))\n  return hooks\n\nmodel = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\nmodel.config.pad_token_id = tokenizer.eos_token_id\nhooks = attach_lora_to_maskformer(model, r, lora_alpha)","metadata":{"id":"lOCO97bJhffz","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:38:54.205918Z","iopub.execute_input":"2025-03-24T13:38:54.206224Z","iopub.status.idle":"2025-03-24T13:38:54.782765Z","shell.execute_reply.started":"2025-03-24T13:38:54.206199Z","shell.execute_reply":"2025-03-24T13:38:54.781866Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":173},{"cell_type":"markdown","source":"###TODO 6: Freeze the Model and Train Only LoRA Weights\nYou are assigned to freeze the entire model, except for the bias of the last two attention weights, LoRA weights, and the classification head.","metadata":{"id":"QiQdCaQLwqjS"}},{"cell_type":"code","source":"for n, p in model.named_parameters():\n    # TODO 6: freeze every layer except the bias of the last two attention weights, LoRA weights, and classification head.\n    p.requires_grad = False\n    \n    if 'lora_A' in n or 'lora_B' in n:\n        p.requires_grad = True\n\n    if n.startswith(\"score\"):\n        p.requires_grad = True\n\nfor layer in model.transformer.h[-2:]:\n    # attn.c_attn.weight.requires_grad = True  # c_attn has q, k, v weights\n    layer.attn.c_attn.bias.requires_grad = True\n","metadata":{"id":"KsxEx0fowBhm","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:38:56.079482Z","iopub.execute_input":"2025-03-24T13:38:56.079835Z","iopub.status.idle":"2025-03-24T13:38:56.085021Z","shell.execute_reply.started":"2025-03-24T13:38:56.079803Z","shell.execute_reply":"2025-03-24T13:38:56.083994Z"}},"outputs":[],"execution_count":174},{"cell_type":"markdown","source":"**Check Your Answer:** The number of learnable parameters is around 12288.","metadata":{"id":"PXsHsYzR7TVW"}},{"cell_type":"code","source":"pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(pytorch_total_params)","metadata":{"id":"sy_KtJTgyPul","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:38:58.039601Z","iopub.execute_input":"2025-03-24T13:38:58.039930Z","iopub.status.idle":"2025-03-24T13:38:58.045730Z","shell.execute_reply.started":"2025-03-24T13:38:58.039905Z","shell.execute_reply":"2025-03-24T13:38:58.044640Z"}},"outputs":[{"name":"stdout","text":"12288\n","output_type":"stream"}],"execution_count":175},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nmodel.to(device)\n\nnum_epochs = 3\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    for batch in tqdm(train_dataloader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"label\"].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = correct / total\n    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")","metadata":{"id":"PEObZRzFyWC8","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:39:00.320471Z","iopub.execute_input":"2025-03-24T13:39:00.320765Z","iopub.status.idle":"2025-03-24T13:53:07.472737Z","shell.execute_reply.started":"2025-03-24T13:39:00.320735Z","shell.execute_reply":"2025-03-24T13:53:07.471728Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c98efde52e4decab11e83c389739a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb026b04d58149b28a9e94de2a079ebe"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/3 - Accuracy: 0.8119\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e1cab942fba47209d82c396e2acbf8f"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/3 - Accuracy: 0.8268\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462406b15eeb4d5c8e2327fd48dbe997"}},"metadata":{}},{"name":"stdout","text":"Epoch 3/3 - Accuracy: 0.8452\n","output_type":"stream"}],"execution_count":176},{"cell_type":"markdown","source":"# Part 2: PEFT Library\n\nIn the first part, you learned how to implement LoRA from scratch. However, in real-world applications, we can simplify this process by using pre-built libraries. One such library is [`peft`](https://huggingface.co/docs/peft/main/en/quicktour), which allows us to inject LoRA into a model more efficiently. By declaring the injected modules in the LoRAConfig, we can easily integrate LoRA without having to implement it ourselves. In this section, you will use the `peft` library to apply LoRA to the model.","metadata":{"id":"_O1O6Y8hzFLQ"}},{"cell_type":"markdown","source":"## TODO 7-8: Initialize the LoRA Config and Set trainable parameters\n\nYour task is to initialize `LoRAConfig` using the same hyperparameters as in TODO 6 (`r=1`, `lora_alpha=4`). Apply LoRA only to the last two attention layers. Then, make sure to freeze the entire model except for the LoRA weights, the bias in the LoRA-injected layers, and the classification head. You only need to set the classification head to be trainable where the rest parameters are already set according to our `LoRAConfig`.  \n\n**HINT**: The total number of trainable parameters should match the result from Part 1 (TODO 6).\n","metadata":{"id":"ETbzxntZ2p0L"}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n\nnum_layers = model.config.n_layer\nlast_two_layers = [\"transformer.h.\" + str(num_layers-2) + \".attn.c_attn\", \"transformer.h.\" + str(num_layers-1) + \".attn.c_attn\"]\n\n# TODO 7: Initialize LoRAConfig\nlora_config = LoraConfig(\n    # Insert the parameters\n    r=1,               # rank of the LoRA layers\n    lora_alpha=4,      # scaling factor for LoRA weights\n    target_modules=last_two_layers,  # Apply LoRA to the last two attention layers (c_attn includes q, k, v weights)\n    modules_to_save = [\"score\"],\n    bias=\"lora_only\",\n)\n\nmodel = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\nmodel.config.pad_token_id = tokenizer.eos_token_id\n\nmodel = get_peft_model(model, lora_config)\nmodel = model.to(device)\n\n# TODO 8: Set classification head to trainable\nfor n, p in model.named_parameters():\n    if n.startswith(\"score\"):  # Classification head\n        p.requires_grad = True","metadata":{"id":"pJGuWtHOyY0n","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:55:30.469482Z","iopub.execute_input":"2025-03-24T13:55:30.469854Z","iopub.status.idle":"2025-03-24T13:55:31.230490Z","shell.execute_reply.started":"2025-03-24T13:55:30.469822Z","shell.execute_reply":"2025-03-24T13:55:31.229611Z"}},"outputs":[{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":177},{"cell_type":"code","source":"pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(pytorch_total_params)","metadata":{"id":"ow2eIkHz4438","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:55:33.599575Z","iopub.execute_input":"2025-03-24T13:55:33.599931Z","iopub.status.idle":"2025-03-24T13:55:33.605037Z","shell.execute_reply.started":"2025-03-24T13:55:33.599903Z","shell.execute_reply":"2025-03-24T13:55:33.604141Z"}},"outputs":[{"name":"stdout","text":"12288\n","output_type":"stream"}],"execution_count":178},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\nmodel.to(device)\n\nnum_epochs = 3\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    for batch in tqdm(train_dataloader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"label\"].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = correct / total\n    print(f\"Epoch {epoch + 1}/{num_epochs} - Accuracy: {accuracy:.4f}\")","metadata":{"id":"KSPiI4u45yJq","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T13:55:35.563420Z","iopub.execute_input":"2025-03-24T13:55:35.563744Z","iopub.status.idle":"2025-03-24T14:08:53.501586Z","shell.execute_reply.started":"2025-03-24T13:55:35.563714Z","shell.execute_reply":"2025-03-24T14:08:53.500524Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7a47a25017649398dfbce7f795c6d0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b1026ec90e848c8857205e4f8f3e4ad"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/3 - Accuracy: 0.7775\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec817a81b42482ab24b7b217ea6f512"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/3 - Accuracy: 0.8119\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3368 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9dd3d9296074c549d9bc42b262e1f43"}},"metadata":{}},{"name":"stdout","text":"Epoch 3/3 - Accuracy: 0.8372\n","output_type":"stream"}],"execution_count":179},{"cell_type":"code","source":"","metadata":{"id":"pRJ5pWrX6kKd","trusted":true},"outputs":[],"execution_count":null}]}